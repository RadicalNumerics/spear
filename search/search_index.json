{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>SPEAR is a collection of kernels for AI model architectures developed by Radical Numerics.</p>"},{"location":"#installation","title":"Installation","text":"<p>You may use PyPI to install SPEAR:</p> <pre><code>pip install spear-python\n</code></pre> <p>Note that it will take few minutes to compile kernels for your specific GPU architecture.</p> <p>You may also install it locally using the following method to install the package in development mode:</p> <pre><code>git clone https://github.com/radicalnumerics/spear.git &amp;&amp; cd spear # clone the repository\nuv venv &amp;&amp; source .venv/bin/activate # virtual env with uv (recommended)\nuv pip install -e '.[dev]' # install in development mode\n</code></pre>"},{"location":"#caching","title":"Caching","text":"<p>We use <code>ccache</code> by default. To use it and enable faster compilation (see explanation on the vLLM docs), run: <pre><code>CCACHE_NOHASHDIR=\"true\" uv pip install --no-build-isolation -e '.[dev]'\n</code></pre></p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import torch\nfrom spear.nn.phalanx import Phalanx\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.bfloat16\n\ndim = 512  # Must be divisible by 16 (head_dim is fixed at 16)\nlength = 128\nbatch_size = 1024\nlayer = Phalanx(dim=dim, length=length, dtype=dtype).to(device)\n\nx = torch.randn(batch_size, length, dim, dtype=dtype, device=device)\ny = layer(x)\nprint(f\"Input: {x.shape} -&gt; Output: {y.shape}\")\n</code></pre>"},{"location":"#development","title":"Development","text":"<p>We include pre-commit hooks for linting and formatting (Python, C++, CUDA). To install:</p> <pre><code>uv run pre-commit install\n</code></pre> <p>To run (note they will be run automatically on commit, so not necessary to run manually):</p> <pre><code>uv run pre-commit run --all-files\n</code></pre> <p>To run tests</p> <pre><code>uv run pytest\n</code></pre>"},{"location":"#structure","title":"Structure","text":"<pre><code>csrc/        # kernels: CUDA/C++ or other DSLs\nspear/\n\u251c\u2500 ops/      # low-level wrappers per op family\n\u2502  \u2514\u2500 &lt;op&gt;/\n\u2514\u2500 nn/       # layers built from ops (parametrized)\n   \u2514\u2500 &lt;layer&gt;/\n</code></pre>"},{"location":"#target-architectures","title":"Target Architectures","text":"<p>Currently supported hardware includes compute capabilities 9.0 (Hopper) and 10.0 (Blackwell).</p> Kernel Name (NVIDIA) sm9.0 (NVIDIA) sm10.0 (NVIDIA) sm10.3 <code>swr.btp.fwd.bf16.bdl.hd16-bl16.sm90</code> \u2714\ufe0e ~ \u26d4 <code>swr.btp.bwd.bf16.bdl.hd16-bl16.sm90</code> \u2714\ufe0e ~ \u26d4 <ul> <li>\u2714\ufe0e: optimized</li> <li>~: working but not fully optimized</li> <li>\u26d4: not available</li> </ul> <p> </p>"},{"location":"dist/spear/","title":"Index","text":"<p>SPEAR is a collection of kernels for AI model architectures developed by Radical Numerics.</p>"},{"location":"dist/spear/#installation","title":"Installation","text":"<p>You may use PyPI to install SPEAR:</p> <pre><code>pip install spear-python\n</code></pre> <p>Note that it will take few minutes to compile kernels for your specific GPU architecture.</p> <p>You may also install it locally using the following method to install the package in development mode:</p> <pre><code>git clone https://github.com/radicalnumerics/spear.git &amp;&amp; cd spear # clone the repository\nuv venv &amp;&amp; source .venv/bin/activate # virtual env with uv (recommended)\nuv pip install -e '.[dev]' # install in development mode\n</code></pre>"},{"location":"dist/spear/#caching","title":"Caching","text":"<p>We use <code>ccache</code> by default. To use it and enable faster compilation (see explanation on the vLLM docs), run: <pre><code>CCACHE_NOHASHDIR=\"true\" uv pip install --no-build-isolation -e '.[dev]'\n</code></pre></p>"},{"location":"dist/spear/#quick-start","title":"Quick Start","text":"<pre><code>import torch\nfrom spear.nn.phalanx import Phalanx\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.bfloat16\n\ndim = 512  # Must be divisible by 16 (head_dim is fixed at 16)\nlength = 128\nbatch_size = 1024\nlayer = Phalanx(dim=dim, length=length, dtype=dtype).to(device)\n\nx = torch.randn(batch_size, length, dim, dtype=dtype, device=device)\ny = layer(x)\nprint(f\"Input: {x.shape} -&gt; Output: {y.shape}\")\n</code></pre>"},{"location":"dist/spear/#development","title":"Development","text":"<p>We include pre-commit hooks for linting and formatting (Python, C++, CUDA). To install:</p> <pre><code>uv run pre-commit install\n</code></pre> <p>To run (note they will be run automatically on commit, so not necessary to run manually):</p> <pre><code>uv run pre-commit run --all-files\n</code></pre> <p>To run tests</p> <pre><code>uv run pytest\n</code></pre>"},{"location":"dist/spear/#structure","title":"Structure","text":"<pre><code>csrc/        # kernels: CUDA/C++ or other DSLs\nspear/\n\u251c\u2500 ops/      # low-level wrappers per op family\n\u2502  \u2514\u2500 &lt;op&gt;/\n\u2514\u2500 nn/       # layers built from ops (parametrized)\n   \u2514\u2500 &lt;layer&gt;/\n</code></pre>"},{"location":"dist/spear/#target-architectures","title":"Target Architectures","text":"<p>Currently supported hardware includes compute capabilities 9.0 (Hopper) and 10.0 (Blackwell).</p> Kernel Name (NVIDIA) sm9.0 (NVIDIA) sm10.0 (NVIDIA) sm10.3 <code>swr.btp.fwd.bf16.bdl.hd16-bl16.sm90</code> \u2714\ufe0e ~ \u26d4 <code>swr.btp.bwd.bf16.bdl.hd16-bl16.sm90</code> \u2714\ufe0e ~ \u26d4 <ul> <li>\u2714\ufe0e: optimized</li> <li>~: working but not fully optimized</li> <li>\u26d4: not available</li> </ul> <p> </p>"},{"location":"dist/spear/docs/","title":"Index","text":"<p>SPEAR is a collection of kernels for AI model architectures developed by Radical Numerics.</p>"},{"location":"dist/spear/docs/#installation","title":"Installation","text":"<pre><code>uv venv\nsource .venv/bin/activate\nuv pip install -e '.[dev]'\n</code></pre> <p>where <code>.[dev]</code> installs all dependencies for development mode; can be simplified to <code>uv pip install -e .</code></p>"},{"location":"dist/spear/docs/#caching","title":"Caching","text":"<p>We use <code>ccache</code> by default. To use it and enable faster compilation (see explanation on the vLLM docs), run: <pre><code>CCACHE_NOHASHDIR=\"true\" uv pip install --no-build-isolation -e '.[dev]'\n</code></pre></p>"},{"location":"dist/spear/docs/#quick-start","title":"Quick Start","text":"<pre><code>import torch\nfrom spear.nn.phalanx import Phalanx\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.bfloat16\n\ndim = 512  # Must be divisible by 16 (head_dim is fixed at 16)\nlength = 128\nbatch_size = 1024\nlayer = Phalanx(dim=dim, length=length, dtype=dtype).to(device)\n\nx = torch.randn(batch_size, length, dim, dtype=dtype, device=device)\ny = layer(x)\nprint(f\"Input: {x.shape} -&gt; Output: {y.shape}\")\n</code></pre>"},{"location":"dist/spear/docs/#development","title":"Development","text":"<p>We include pre-commit hooks for linting and formatting (Python, C++, CUDA). To install:</p> <pre><code>uv run pre-commit install\n</code></pre> <p>To run (note they will be run automatically on commit, so not necessary to run manually):</p> <pre><code>uv run pre-commit run --all-files\n</code></pre> <p>To run tests</p> <pre><code>uv run pytest\n</code></pre>"},{"location":"dist/spear/docs/#structure","title":"Structure","text":"<pre><code>csrc/        # kernels: CUDA/C++ or other DSLs\nspear/\n\u251c\u2500 ops/      # low-level wrappers per op family\n\u2502  \u2514\u2500 &lt;op&gt;/\n\u2514\u2500 nn/       # layers built from ops (parametrized)\n   \u2514\u2500 &lt;layer&gt;/\n</code></pre>"},{"location":"dist/spear/docs/#target-architectures","title":"Target Architectures","text":"<p>Currently supported hardware includes compute capabilities 9.0 (Hopper) and 10.0 (Blackwell).</p> Kernel Name (NVIDIA) sm9.0 (NVIDIA) sm10.0 (NVIDIA) sm10.3 <code>swr.btp.fwd.bf16.bdl.hd16-bl16.sm90</code> \u2714\ufe0e ~ \u26d4 <code>swr.btp.bwd.bf16.bdl.hd16-bl16.sm90</code> \u2714\ufe0e ~ \u26d4 <ul> <li>\u2714\ufe0e: optimized</li> <li>~: working but not fully optimized</li> <li>\u26d4: not available</li> </ul> <p> </p>"},{"location":"dist/spear/docs/api/","title":"API Reference","text":"<p>The API mirrors the three layers of the project. Conceptual overviews live in <code>docs/concepts</code>; this section links directly to the Python surface area so you can drop kernels and modules into your own models.</p>"},{"location":"dist/spear/docs/api/#spearnn","title":"<code>spear.nn</code>","text":"<p>High-level modules built on sliding window recurrences. The <code>Phalanx</code> layer wraps the BTP kernel with SigmoidA parametrisation, and <code>SigmoidA</code> is exposed separately when you only need the projections.</p> <p>See Phalanx for the design rationale.</p>"},{"location":"dist/spear/docs/api/#spear.nn.phalanx.Phalanx","title":"Phalanx","text":"<pre><code>Phalanx(\n    dim: int,\n    heads: int | None = None,\n    length: int = 2048,\n    method: str = \"default\",\n    dtype: dtype = float32,\n    k: int = 2,\n    wpb: int = 32,\n    output_dtype: dtype | None = None,\n    block_size: int = 16,\n    kv_heads: int | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Main forward pass - delegates to method-specific implementation.</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    heads: int | None = None,\n    length: int = 2048,\n    method: str = \"default\",\n    dtype: torch.dtype = torch.float32,\n    k: int = 2,\n    wpb: int = 32,\n    output_dtype: torch.dtype | None = None,\n    block_size: int = 16,\n    kv_heads: int | None = None,\n):\n    super().__init__()\n\n    # Phalanx requires head_dim = 16, so heads = dim // 16\n    if heads is None:\n        if dim % 16 != 0:\n            raise ValueError(f\"dim ({dim}) must be divisible by 16\")\n        heads = dim // 16\n    else:\n        if dim % heads != 0:\n            raise ValueError(f\"dim ({dim}) must be divisible by heads ({heads})\")\n        head_dim_check = dim // heads\n        if head_dim_check != 16:\n            raise ValueError(f\"head_dim must be 16, got {head_dim_check}. Use heads=dim//16 or leave heads=None\")\n\n    if method not in (\"default\", \"pytorch\", \"pytorch_linspace\"):\n        raise ValueError(f\"method must be one of 'default', 'pytorch', 'pytorch_linspace', got {method}\")\n\n    self.dim = dim\n    self.heads = heads\n    self.head_dim = 16  # Always 16 for Phalanx\n    self.length = (length + 15) // 16 * 16\n    self.method = method\n    self.dtype = dtype\n    self.compute_dtype = torch.bfloat16\n    self.k = k\n    self.wpb = wpb\n    self.output_dtype = output_dtype if output_dtype is not None else dtype\n    self.block_size = block_size\n    self.kv_heads = kv_heads if kv_heads is not None else heads\n    mixed_precision = True\n\n    self.param = SigmoidA(dim, heads, self.head_dim, dtype=dtype, kv_heads=kv_heads)\n\n    self.proj_out = nn.Linear(dim, dim, bias=False, dtype=dtype)\n\n    if method == \"default\":\n        from spear.ops.btp import btp\n\n        self.btp_module = btp\n        self._forward_fn = self._forward_default\n\n    elif method == \"pytorch\":\n        from spear.ops.btp.reference import block_two_pass_log\n\n        self.pytorch_block_two_pass = block_two_pass_log\n        self._forward_fn = self._forward_pytorch\n\n    elif method == \"pytorch_linspace\":\n        from spear.ops.btp.reference import block_two_pass_linspace\n\n        self.pytorch_block_two_pass = block_two_pass_linspace\n        self._forward_fn = self._forward_pytorch\n\n    else:\n        raise ValueError(f\"Invalid method: {method}. Supported methods: 'default', 'pytorch', 'pytorch_linspace'. \")\n</code></pre>"},{"location":"dist/spear/docs/api/#spear.nn.phalanx.Phalanx.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Main forward pass - delegates to method-specific implementation.</p> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Main forward pass - delegates to method-specific implementation.\"\"\"\n    return self._forward_fn(x)\n</code></pre>"},{"location":"dist/spear/docs/api/#spear.nn.phalanx.SigmoidA","title":"SigmoidA","text":"<pre><code>SigmoidA(\n    dim: int,\n    heads: int,\n    head_dim: int,\n    dtype: dtype = bfloat16,\n    mixed_precision: bool = True,\n    kv_heads: int | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Sigmoid-gated attention parametrization for SSM with optional KV groups.</p> <p>Methods:</p> <ul> <li> <code>forward_fused_gates</code>             \u2013              <p>Forward for fused gates kernel - returns packed tensor [B, 3HD + H, L].</p> </li> <li> <code>forward_axcv</code>             \u2013              <p>Forward for default/pytorch kernels - returns (A, X, C, V) tensors.</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    heads: int,\n    head_dim: int,\n    dtype: torch.dtype = torch.bfloat16,\n    mixed_precision: bool = True,\n    kv_heads: int | None = None,\n):\n    super().__init__()\n    self.heads = heads\n    self.head_dim = head_dim\n    self.dim = dim\n    self.dtype = dtype\n    self.compute_dtype = torch.bfloat16\n\n    self.kv_heads = kv_heads if kv_heads is not None else heads\n    if heads % self.kv_heads != 0:\n        raise ValueError(f\"heads ({heads}) must be divisible by kv_heads ({self.kv_heads})\")\n\n    self.kv_repeat = KVRepeat(self.kv_heads, heads)\n\n    self.dim_bvc = 3 * heads * head_dim\n    self.dim_a = heads\n\n    self.proj_a = nn.Linear(dim, heads, bias=True, dtype=dtype)\n\n    kv_gate_dim = self.kv_heads * head_dim\n    self.proj_b = nn.Linear(dim, kv_gate_dim, bias=True, dtype=dtype)\n    self.proj_c = nn.Linear(dim, kv_gate_dim, bias=True, dtype=dtype)\n\n    gate_dim = heads * head_dim\n    self.proj_v = nn.Linear(dim, gate_dim, bias=False, dtype=dtype)\n</code></pre>"},{"location":"dist/spear/docs/api/#spear.nn.phalanx.SigmoidA.forward_fused_gates","title":"forward_fused_gates","text":"<pre><code>forward_fused_gates(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward for fused gates kernel - returns packed tensor [B, 3HD + H, L].</p> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward_fused_gates(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward for fused gates kernel - returns packed tensor [B, 3*H*D + H, L].\"\"\"\n    B, L, D = x.shape\n\n    A, B_gate_kv, C_kv, V = self._compute_base_projections(x)\n\n    if self.kv_repeat.enabled:\n        B_gate = self.kv_repeat.expand_and_reshape(B_gate_kv, B, self.head_dim, L)\n        B_gate = B_gate.reshape(B, self.heads * self.head_dim, L)\n        C = self.kv_repeat.expand_and_reshape(C_kv, B, self.head_dim, L)\n        C = C.reshape(B, self.heads * self.head_dim, L)\n    else:\n        B_gate = B_gate_kv\n        C = C_kv\n\n    out = torch.cat([B_gate, V, C, A], dim=1)  # [B, 3*H*D + H, L]\n\n    return out\n</code></pre>"},{"location":"dist/spear/docs/api/#spear.nn.phalanx.SigmoidA.forward_axcv","title":"forward_axcv","text":"<pre><code>forward_axcv(\n    x: Tensor,\n) -&gt; tuple[Tensor, Tensor, Tensor, Tensor]\n</code></pre> <p>Forward for default/pytorch kernels - returns (A, X, C, V) tensors.</p> <p>This method is used by both the default BTP kernel and pure PyTorch implementation since they both need the same tensor format: separate A, X, C, V tensors.</p> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward_axcv(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Forward for default/pytorch kernels - returns (A, X, C, V) tensors.\n\n    This method is used by both the default BTP kernel and pure PyTorch implementation\n    since they both need the same tensor format: separate A, X, C, V tensors.\n    \"\"\"\n    B, L, D = x.shape\n    H, DH = self.heads, self.head_dim\n\n    A_logits, B_gate_kv_logits, C_kv, V_flat = self._compute_base_projections(x)\n\n    V = V_flat.view(B, H, DH, L)  # [B, H, DH, L]\n\n    if self.kv_repeat.enabled:\n        B_gate_logits = self.kv_repeat.expand_and_reshape(B_gate_kv_logits, B, DH, L)\n        C = self.kv_repeat.expand_and_reshape(C_kv, B, DH, L)\n    else:\n        B_gate_logits = B_gate_kv_logits.view(B, H, DH, L)\n        C = C_kv.view(B, H, DH, L)\n\n    A = torch.sigmoid(A_logits)  # [B, H, L]\n    B_gate = torch.sigmoid(B_gate_logits)  # [B, H, DH, L]\n\n    X = B_gate * V  # [B, H, DH, L]\n\n    return A, X, C, V\n</code></pre>"},{"location":"dist/spear/docs/api/#spearops","title":"<code>spear.ops</code>","text":"<p>Low-level access to the BTP kernels and reference implementations. Use these when you need fine-grained control over tiling, dtype, or integration with custom autograd.</p> <p>Start with Block Two-Pass for an end-to-end explanation.</p>"},{"location":"dist/spear/docs/api/#spear.ops.btp.btp","title":"btp","text":"<pre><code>btp(\n    coeff: Tensor,\n    x: Tensor,\n    k: int = 2,\n    wpb: int | None = None,\n    output_dtype: dtype | None = None,\n) -&gt; Tensor\n</code></pre> <p>Apply BTP (Block Two-Pass) operation.</p> <p>Parameters:</p> <ul> <li> <code>coeff</code>               (<code>Tensor</code>)           \u2013            <p>Coefficient tensor of shape [B, H, L]</p> </li> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape [B, H, DH, L]</p> </li> <li> <code>k</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Order parameter (1 or 2)</p> </li> <li> <code>wpb</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Warps per block (default: 32 for k=2, 8 for k=1)</p> </li> <li> <code>output_dtype</code>               (<code>dtype | None</code>, default:                   <code>None</code> )           \u2013            <p>Output dtype (default: float32)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Output tensor of shape [B, H, DH, L]</p> </li> </ul> Source code in <code>spear/ops/btp/interface.py</code> <pre><code>def btp(\n    coeff: torch.Tensor,\n    x: torch.Tensor,\n    k: int = 2,\n    wpb: int | None = None,\n    output_dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply BTP (Block Two-Pass) operation.\n\n    Args:\n        coeff: Coefficient tensor of shape [B, H, L]\n        x: Input tensor of shape [B, H, DH, L]\n        k: Order parameter (1 or 2)\n        wpb: Warps per block (default: 32 for k=2, 8 for k=1)\n        output_dtype: Output dtype (default: float32)\n\n    Returns:\n        Output tensor of shape [B, H, DH, L]\n    \"\"\"\n    out_dtype = output_dtype or torch.float32\n    wpb_val = wpb if wpb is not None else (WPB_K2_DEFAULT if k == 2 else WPB_K1_DEFAULT)\n    coeff = coeff.contiguous()\n    x = x.contiguous()\n    return _BTPFunction.apply(coeff, x, k, out_dtype, wpb_val)\n</code></pre>"},{"location":"dist/spear/docs/api/nn/","title":"Neural Network Modules","text":"<p><code>spear.nn</code> provides the trainable layers that sit on top of the BTP kernel. Each entry here is documented in depth through mkdocstrings; read the Phalanx concept to understand the surrounding design decisions.</p>"},{"location":"dist/spear/docs/api/nn/#phalanx","title":"Phalanx","text":""},{"location":"dist/spear/docs/api/nn/#spear.nn.phalanx.Phalanx","title":"Phalanx","text":"<pre><code>Phalanx(\n    dim: int,\n    heads: int | None = None,\n    length: int = 2048,\n    method: str = \"default\",\n    dtype: dtype = float32,\n    k: int = 2,\n    wpb: int = 32,\n    output_dtype: dtype | None = None,\n    block_size: int = 16,\n    kv_heads: int | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Main forward pass - delegates to method-specific implementation.</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    heads: int | None = None,\n    length: int = 2048,\n    method: str = \"default\",\n    dtype: torch.dtype = torch.float32,\n    k: int = 2,\n    wpb: int = 32,\n    output_dtype: torch.dtype | None = None,\n    block_size: int = 16,\n    kv_heads: int | None = None,\n):\n    super().__init__()\n\n    # Phalanx requires head_dim = 16, so heads = dim // 16\n    if heads is None:\n        if dim % 16 != 0:\n            raise ValueError(f\"dim ({dim}) must be divisible by 16\")\n        heads = dim // 16\n    else:\n        if dim % heads != 0:\n            raise ValueError(f\"dim ({dim}) must be divisible by heads ({heads})\")\n        head_dim_check = dim // heads\n        if head_dim_check != 16:\n            raise ValueError(f\"head_dim must be 16, got {head_dim_check}. Use heads=dim//16 or leave heads=None\")\n\n    if method not in (\"default\", \"pytorch\", \"pytorch_linspace\"):\n        raise ValueError(f\"method must be one of 'default', 'pytorch', 'pytorch_linspace', got {method}\")\n\n    self.dim = dim\n    self.heads = heads\n    self.head_dim = 16  # Always 16 for Phalanx\n    self.length = (length + 15) // 16 * 16\n    self.method = method\n    self.dtype = dtype\n    self.compute_dtype = torch.bfloat16\n    self.k = k\n    self.wpb = wpb\n    self.output_dtype = output_dtype if output_dtype is not None else dtype\n    self.block_size = block_size\n    self.kv_heads = kv_heads if kv_heads is not None else heads\n    mixed_precision = True\n\n    self.param = SigmoidA(dim, heads, self.head_dim, dtype=dtype, kv_heads=kv_heads)\n\n    self.proj_out = nn.Linear(dim, dim, bias=False, dtype=dtype)\n\n    if method == \"default\":\n        from spear.ops.btp import btp\n\n        self.btp_module = btp\n        self._forward_fn = self._forward_default\n\n    elif method == \"pytorch\":\n        from spear.ops.btp.reference import block_two_pass_log\n\n        self.pytorch_block_two_pass = block_two_pass_log\n        self._forward_fn = self._forward_pytorch\n\n    elif method == \"pytorch_linspace\":\n        from spear.ops.btp.reference import block_two_pass_linspace\n\n        self.pytorch_block_two_pass = block_two_pass_linspace\n        self._forward_fn = self._forward_pytorch\n\n    else:\n        raise ValueError(f\"Invalid method: {method}. Supported methods: 'default', 'pytorch', 'pytorch_linspace'. \")\n</code></pre>"},{"location":"dist/spear/docs/api/nn/#spear.nn.phalanx.Phalanx.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Main forward pass - delegates to method-specific implementation.</p> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Main forward pass - delegates to method-specific implementation.\"\"\"\n    return self._forward_fn(x)\n</code></pre>"},{"location":"dist/spear/docs/api/nn/#sigmoida","title":"SigmoidA","text":""},{"location":"dist/spear/docs/api/nn/#spear.nn.phalanx.SigmoidA","title":"SigmoidA","text":"<pre><code>SigmoidA(\n    dim: int,\n    heads: int,\n    head_dim: int,\n    dtype: dtype = bfloat16,\n    mixed_precision: bool = True,\n    kv_heads: int | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Sigmoid-gated attention parametrization for SSM with optional KV groups.</p> <p>Methods:</p> <ul> <li> <code>forward_fused_gates</code>             \u2013              <p>Forward for fused gates kernel - returns packed tensor [B, 3HD + H, L].</p> </li> <li> <code>forward_axcv</code>             \u2013              <p>Forward for default/pytorch kernels - returns (A, X, C, V) tensors.</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    heads: int,\n    head_dim: int,\n    dtype: torch.dtype = torch.bfloat16,\n    mixed_precision: bool = True,\n    kv_heads: int | None = None,\n):\n    super().__init__()\n    self.heads = heads\n    self.head_dim = head_dim\n    self.dim = dim\n    self.dtype = dtype\n    self.compute_dtype = torch.bfloat16\n\n    self.kv_heads = kv_heads if kv_heads is not None else heads\n    if heads % self.kv_heads != 0:\n        raise ValueError(f\"heads ({heads}) must be divisible by kv_heads ({self.kv_heads})\")\n\n    self.kv_repeat = KVRepeat(self.kv_heads, heads)\n\n    self.dim_bvc = 3 * heads * head_dim\n    self.dim_a = heads\n\n    self.proj_a = nn.Linear(dim, heads, bias=True, dtype=dtype)\n\n    kv_gate_dim = self.kv_heads * head_dim\n    self.proj_b = nn.Linear(dim, kv_gate_dim, bias=True, dtype=dtype)\n    self.proj_c = nn.Linear(dim, kv_gate_dim, bias=True, dtype=dtype)\n\n    gate_dim = heads * head_dim\n    self.proj_v = nn.Linear(dim, gate_dim, bias=False, dtype=dtype)\n</code></pre>"},{"location":"dist/spear/docs/api/nn/#spear.nn.phalanx.SigmoidA.forward_fused_gates","title":"forward_fused_gates","text":"<pre><code>forward_fused_gates(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward for fused gates kernel - returns packed tensor [B, 3HD + H, L].</p> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward_fused_gates(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward for fused gates kernel - returns packed tensor [B, 3*H*D + H, L].\"\"\"\n    B, L, D = x.shape\n\n    A, B_gate_kv, C_kv, V = self._compute_base_projections(x)\n\n    if self.kv_repeat.enabled:\n        B_gate = self.kv_repeat.expand_and_reshape(B_gate_kv, B, self.head_dim, L)\n        B_gate = B_gate.reshape(B, self.heads * self.head_dim, L)\n        C = self.kv_repeat.expand_and_reshape(C_kv, B, self.head_dim, L)\n        C = C.reshape(B, self.heads * self.head_dim, L)\n    else:\n        B_gate = B_gate_kv\n        C = C_kv\n\n    out = torch.cat([B_gate, V, C, A], dim=1)  # [B, 3*H*D + H, L]\n\n    return out\n</code></pre>"},{"location":"dist/spear/docs/api/nn/#spear.nn.phalanx.SigmoidA.forward_axcv","title":"forward_axcv","text":"<pre><code>forward_axcv(\n    x: Tensor,\n) -&gt; tuple[Tensor, Tensor, Tensor, Tensor]\n</code></pre> <p>Forward for default/pytorch kernels - returns (A, X, C, V) tensors.</p> <p>This method is used by both the default BTP kernel and pure PyTorch implementation since they both need the same tensor format: separate A, X, C, V tensors.</p> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward_axcv(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Forward for default/pytorch kernels - returns (A, X, C, V) tensors.\n\n    This method is used by both the default BTP kernel and pure PyTorch implementation\n    since they both need the same tensor format: separate A, X, C, V tensors.\n    \"\"\"\n    B, L, D = x.shape\n    H, DH = self.heads, self.head_dim\n\n    A_logits, B_gate_kv_logits, C_kv, V_flat = self._compute_base_projections(x)\n\n    V = V_flat.view(B, H, DH, L)  # [B, H, DH, L]\n\n    if self.kv_repeat.enabled:\n        B_gate_logits = self.kv_repeat.expand_and_reshape(B_gate_kv_logits, B, DH, L)\n        C = self.kv_repeat.expand_and_reshape(C_kv, B, DH, L)\n    else:\n        B_gate_logits = B_gate_kv_logits.view(B, H, DH, L)\n        C = C_kv.view(B, H, DH, L)\n\n    A = torch.sigmoid(A_logits)  # [B, H, L]\n    B_gate = torch.sigmoid(B_gate_logits)  # [B, H, DH, L]\n\n    X = B_gate * V  # [B, H, DH, L]\n\n    return A, X, C, V\n</code></pre>"},{"location":"dist/spear/docs/api/nn/#kvrepeat","title":"KVRepeat","text":""},{"location":"dist/spear/docs/api/nn/#spear.nn.phalanx.swr.KVRepeat","title":"KVRepeat","text":"<pre><code>KVRepeat(kv_heads: int, total_heads: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Modular KV group repeating module for efficient parameter sharing.</p> <p>This module handles the repeating of KV (B gate and C) tensors across groups, allowing for parameter-efficient attention mechanisms where multiple query heads share the same key-value parameters.</p> <pre><code>total_heads: Total number of heads (must be divisible by kv_heads)\n</code></pre> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Repeat tensor across KV groups if needed.</p> </li> <li> <code>reshape_for_kv</code>             \u2013              <p>Reshape flat tensor to KV head dimensions.</p> </li> <li> <code>expand_and_reshape</code>             \u2013              <p>Reshape and expand tensor from KV heads to all heads.</p> </li> <li> <code>expand_flat</code>             \u2013              <p>Expand flat KV tensor directly without intermediate 4D reshape.</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def __init__(self, kv_heads: int, total_heads: int):\n    \"\"\"\n    Args:\n        kv_heads: Number of key-value heads\n        total_heads: Total number of heads (must be divisible by kv_heads)\n    \"\"\"\n    super().__init__()\n\n    if total_heads % kv_heads != 0:\n        raise ValueError(f\"total_heads ({total_heads}) must be divisible by kv_heads ({kv_heads})\")\n\n    self.kv_heads = kv_heads\n    self.total_heads = total_heads\n    self.kv_groups = total_heads // kv_heads\n    self.enabled = kv_heads &lt; total_heads\n\n    # Pre-compute indices for KV expansion: [0,0,0,1,1,1,...]\n    # Register as buffer so it moves with model to correct device\n    if self.enabled:\n        indices = torch.arange(total_heads) // self.kv_groups\n        self.register_buffer('kv_indices', indices, persistent=False)\n</code></pre>"},{"location":"dist/spear/docs/api/nn/#spear.nn.phalanx.swr.KVRepeat.forward","title":"forward","text":"<pre><code>forward(tensor: Tensor, pattern: str = 'b h d l') -&gt; Tensor\n</code></pre> <p>Repeat tensor across KV groups if needed.</p> <p>Parameters:</p> <ul> <li> <code>tensor</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor with shape matching pattern</p> </li> <li> <code>pattern</code>               (<code>str</code>, default:                   <code>'b h d l'</code> )           \u2013            <p>Einops pattern for input tensor (default: 'b h d l')</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Tensor repeated across groups if kv_groups &gt; 1, otherwise unchanged</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward(self, tensor: torch.Tensor, pattern: str = \"b h d l\") -&gt; torch.Tensor:\n    \"\"\"\n    Repeat tensor across KV groups if needed.\n\n    Args:\n        tensor: Input tensor with shape matching pattern\n        pattern: Einops pattern for input tensor (default: 'b h d l')\n\n    Returns:\n        Tensor repeated across groups if kv_groups &gt; 1, otherwise unchanged\n    \"\"\"\n    if not self.enabled:\n        return tensor\n\n    # Use cached indices for efficient expansion\n    return tensor.index_select(1, self.kv_indices)\n</code></pre>"},{"location":"dist/spear/docs/api/nn/#spear.nn.phalanx.swr.KVRepeat.reshape_for_kv","title":"reshape_for_kv","text":"<pre><code>reshape_for_kv(\n    tensor: Tensor, batch: int, head_dim: int, length: int\n) -&gt; Tensor\n</code></pre> <p>Reshape flat tensor to KV head dimensions.</p> <p>Parameters:</p> <ul> <li> <code>tensor</code>               (<code>Tensor</code>)           \u2013            <p>Flat tensor of shape [B, kv_heads * head_dim, L]</p> </li> <li> <code>batch</code>               (<code>int</code>)           \u2013            <p>Batch size</p> </li> <li> <code>head_dim</code>               (<code>int</code>)           \u2013            <p>Dimension per head</p> </li> <li> <code>length</code>               (<code>int</code>)           \u2013            <p>Sequence length</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Reshaped tensor of shape [B, kv_heads, head_dim, L]</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def reshape_for_kv(self, tensor: torch.Tensor, batch: int, head_dim: int, length: int) -&gt; torch.Tensor:\n    \"\"\"\n    Reshape flat tensor to KV head dimensions.\n\n    Args:\n        tensor: Flat tensor of shape [B, kv_heads * head_dim, L]\n        batch: Batch size\n        head_dim: Dimension per head\n        length: Sequence length\n\n    Returns:\n        Reshaped tensor of shape [B, kv_heads, head_dim, L]\n    \"\"\"\n    return tensor.view(batch, self.kv_heads, head_dim, length)\n</code></pre>"},{"location":"dist/spear/docs/api/nn/#spear.nn.phalanx.swr.KVRepeat.expand_and_reshape","title":"expand_and_reshape","text":"<pre><code>expand_and_reshape(\n    tensor: Tensor, batch: int, head_dim: int, length: int\n) -&gt; Tensor\n</code></pre> <p>Reshape and expand tensor from KV heads to all heads.</p> <p>Parameters:</p> <ul> <li> <code>tensor</code>               (<code>Tensor</code>)           \u2013            <p>Flat tensor of shape [B, kv_heads * head_dim, L]</p> </li> <li> <code>batch</code>               (<code>int</code>)           \u2013            <p>Batch size</p> </li> <li> <code>head_dim</code>               (<code>int</code>)           \u2013            <p>Dimension per head</p> </li> <li> <code>length</code>               (<code>int</code>)           \u2013            <p>Sequence length</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Expanded tensor of shape [B, total_heads, head_dim, L]</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def expand_and_reshape(self, tensor: torch.Tensor, batch: int, head_dim: int, length: int) -&gt; torch.Tensor:\n    \"\"\"\n    Reshape and expand tensor from KV heads to all heads.\n\n    Args:\n        tensor: Flat tensor of shape [B, kv_heads * head_dim, L]\n        batch: Batch size\n        head_dim: Dimension per head\n        length: Sequence length\n\n    Returns:\n        Expanded tensor of shape [B, total_heads, head_dim, L]\n    \"\"\"\n    if not self.enabled:\n        return tensor.view(batch, self.kv_heads, head_dim, length)\n\n    # Optimized expansion using pre-computed indices\n    # Reshape flat to [B, kv_heads, head_dim, L]\n    tensor = tensor.view(batch, self.kv_heads, head_dim, length)\n    # Use cached indices: [0,0,0,1,1,1,...] for kv_groups repeats\n    return tensor.index_select(1, self.kv_indices)\n</code></pre>"},{"location":"dist/spear/docs/api/nn/#spear.nn.phalanx.swr.KVRepeat.expand_flat","title":"expand_flat","text":"<pre><code>expand_flat(tensor: Tensor, head_dim: int) -&gt; Tensor\n</code></pre> <p>Expand flat KV tensor directly without intermediate 4D reshape. More efficient for cases where we immediately flatten back.</p> <p>Parameters:</p> <ul> <li> <code>tensor</code>               (<code>Tensor</code>)           \u2013            <p>Flat tensor of shape [B, kv_heads * head_dim, L]</p> </li> <li> <code>head_dim</code>               (<code>int</code>)           \u2013            <p>Dimension per head</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Expanded flat tensor of shape [B, total_heads * head_dim, L]</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def expand_flat(self, tensor: torch.Tensor, head_dim: int) -&gt; torch.Tensor:\n    \"\"\"\n    Expand flat KV tensor directly without intermediate 4D reshape.\n    More efficient for cases where we immediately flatten back.\n\n    Args:\n        tensor: Flat tensor of shape [B, kv_heads * head_dim, L]\n        head_dim: Dimension per head\n\n    Returns:\n        Expanded flat tensor of shape [B, total_heads * head_dim, L]\n    \"\"\"\n    if not self.enabled:\n        return tensor\n\n    B, _, L = tensor.shape\n    # Reshape to separate heads: [B, kv_heads, head_dim, L]\n    tensor = tensor.view(B, self.kv_heads, head_dim, L)\n    # Expand using cached indices: [B, total_heads, head_dim, L]\n    tensor = tensor.index_select(1, self.kv_indices)\n    # Flatten back: [B, total_heads * head_dim, L]\n    return tensor.view(B, self.total_heads * head_dim, L)\n</code></pre>"},{"location":"dist/spear/docs/api/ops/","title":"Operations","text":"<p><code>spear.ops</code> provides the block two-pass primitive and its testing harness. Consult the BTP concept page for a walkthrough of the algorithm before diving into the low-level API.</p>"},{"location":"dist/spear/docs/api/ops/#btp-block-two-pass","title":"BTP (Block Two-Pass)","text":""},{"location":"dist/spear/docs/api/ops/#spear.ops.btp.btp","title":"btp","text":"<pre><code>btp(\n    coeff: Tensor,\n    x: Tensor,\n    k: int = 2,\n    wpb: int | None = None,\n    output_dtype: dtype | None = None,\n) -&gt; Tensor\n</code></pre> <p>Apply BTP (Block Two-Pass) operation.</p> <p>Parameters:</p> <ul> <li> <code>coeff</code>               (<code>Tensor</code>)           \u2013            <p>Coefficient tensor of shape [B, H, L]</p> </li> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape [B, H, DH, L]</p> </li> <li> <code>k</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Order parameter (1 or 2)</p> </li> <li> <code>wpb</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Warps per block (default: 32 for k=2, 8 for k=1)</p> </li> <li> <code>output_dtype</code>               (<code>dtype | None</code>, default:                   <code>None</code> )           \u2013            <p>Output dtype (default: float32)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Output tensor of shape [B, H, DH, L]</p> </li> </ul> Source code in <code>spear/ops/btp/interface.py</code> <pre><code>def btp(\n    coeff: torch.Tensor,\n    x: torch.Tensor,\n    k: int = 2,\n    wpb: int | None = None,\n    output_dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply BTP (Block Two-Pass) operation.\n\n    Args:\n        coeff: Coefficient tensor of shape [B, H, L]\n        x: Input tensor of shape [B, H, DH, L]\n        k: Order parameter (1 or 2)\n        wpb: Warps per block (default: 32 for k=2, 8 for k=1)\n        output_dtype: Output dtype (default: float32)\n\n    Returns:\n        Output tensor of shape [B, H, DH, L]\n    \"\"\"\n    out_dtype = output_dtype or torch.float32\n    wpb_val = wpb if wpb is not None else (WPB_K2_DEFAULT if k == 2 else WPB_K1_DEFAULT)\n    coeff = coeff.contiguous()\n    x = x.contiguous()\n    return _BTPFunction.apply(coeff, x, k, out_dtype, wpb_val)\n</code></pre>"},{"location":"dist/spear/docs/concepts/btp/","title":"Block Two-Pass (BTP)","text":"<p>Block Two-Pass (BTP) is the algorithmic core that realises sliding window recurrences on GPUs. It factors the recurrence into two sweeps that match the hardware: a local pass that consumes each tile entirely on chip, and a rank-1 global pass that stitches tiles together without serialising thread blocks.</p>"},{"location":"dist/spear/docs/concepts/btp/#first-pass-local-contractions","title":"First Pass: Local Contractions","text":"<p>Inputs are reshaped into blocks of length <code>BL</code>, aligned with warp scheduling. Within a block we form a lower-triangular transfer matrix <code>L</code> from the coefficients and apply it to the activations with high-throughput GEMMs. Several numerically equivalent constructions exist; the production kernel uses a stable log-space variant, while the reference paths expose masked cumulative products and explicit double-precision formulations for testing.</p>"},{"location":"dist/spear/docs/concepts/btp/#second-pass-carry-propagation","title":"Second Pass: Carry Propagation","text":"<p>Once each block is solved locally, only the final column of <code>L</code> is needed to communicate with the next block. BTP multiplies that column by the cumulative gate at the boundary and performs a device-wide parallel rank-1 update. Because the update is diagonal at the top level, every block can run in parallel except for a single synchronisation, eliminating the carry chains that slow traditional scan implementations.</p>"},{"location":"dist/spear/docs/concepts/btp/#autograd-and-compilation","title":"Autograd and Compilation","text":"<p><code>spear.ops.btp.interface</code> registers the compiled CUDA kernels as <code>torch.library</code> operators so that <code>torch.compile</code> can inline them. A custom autograd function wraps forward and backward, allocating checkpoints per block in float32 for numerical stability. The PyTorch-only references live alongside the CUDA binding and make it straightforward to validate gradients or experiment with alternative <code>L</code> constructions.</p>"},{"location":"dist/spear/docs/concepts/btp/#practical-advice","title":"Practical Advice","text":"<p>Choose the warps-per-block (<code>wpb</code>) parameter based on the recurrence order <code>k</code>: Hopper defaults to 32 warps for second-order recurrences and 8 for first-order. The kernel expects head width <code>DH=16</code> and sequence lengths that are multiples of the block size. When those assumptions are violated\u2014in particular the length multiple\u2014the Python wrappers pad internally so that downstream layers can keep simpler shapes.</p>"},{"location":"dist/spear/docs/concepts/phalanx/","title":"Phalanx Layer","text":"<p>Phalanx turns the BTP kernel into a drop-in PyTorch module. It couples the SigmoidA parametrisation with the sliding window recurrence to deliver a horizon-bounded mixer that works in hybrid transformer stacks.</p>"},{"location":"dist/spear/docs/concepts/phalanx/#parametrisation","title":"Parametrisation","text":"<p>Inputs pass through four projections: <code>proj_a</code> produces the per-head gates, <code>proj_b</code> and <code>proj_c</code> produce key and value gates for the recurrence, and <code>proj_v</code> yields the driving term. Gates are applied through elementwise sigmoids, and the key/value projections optionally share parameters through <code>KVRepeat</code>, which repeats key-value heads across query groups to reduce memory footprint without shrinking the attention space.</p>"},{"location":"dist/spear/docs/concepts/phalanx/#execution-paths","title":"Execution Paths","text":"<p>Phalanx exposes three forward paths: <code>method=\"default\"</code> uses the CUDA BTP kernel (<code>spear.ops.btp.btp</code>), <code>method=\"pytorch\"</code> runs the log-space reference for convenience or CPU execution, and <code>method=\"pytorch_linspace\"</code> uses a masked cumulative-product variant that is useful for debugging numerical issues. All paths compute the same tensors <code>A</code>, <code>X</code>, <code>C</code>, and <code>V</code> before invoking the recurrence. After the BTP call, the output is combined with <code>C</code> and <code>V</code>, permuted back to batch-first layout, and projected to the model dimension via <code>proj_out</code>.</p>"},{"location":"dist/spear/docs/concepts/phalanx/#streaming-inference","title":"Streaming Inference","text":"<p><code>spear.nn.phalanx.inference</code> implements jagged-band sequential and per-token decoding under the same assumptions as training. A <code>JagState</code> stores the local accumulator, cumulative gate, and previous block carry so that decode can step one token at a time while staying consistent with the block layout used in prefill.</p>"},{"location":"dist/spear/docs/concepts/phalanx/#constraints-and-tips","title":"Constraints and Tips","text":"<p>The layer fixes the head dimension to 16, so either supply <code>heads=dim//16</code> or leave <code>heads=None</code> and let Phalanx infer it. Sequence lengths are padded up to the next multiple of the kernel block size (default 16). Mixed-precision training works by keeping projections in the requested dtype while running the recurrence in <code>bfloat16</code> and accumulating in <code>float32</code> where needed. For hybrid models, pair Phalanx with attention layers that handle long-range routing; the SWR window deliberately truncates global dependencies so that the layer can run faster at high sequence lengths.</p>"},{"location":"dist/spear/docs/concepts/swr/","title":"Sliding Window Recurrences","text":"<p>Sliding window recurrences (SWRs) bound the communication horizon of linear recurrences so that they respect the memory hierarchy of modern accelerators. Instead of maintaining global state, an SWR tiles the sequence into warp-sized chunks, computes the dense recurrence locally, and exchanges only the rank-1 carry that the next chunk needs. The window is jagged: its shape mirrors how thread blocks advance across the sequence, giving every warp full bandwidth inside its tile while keeping inter-warp updates minimal.</p>"},{"location":"dist/spear/docs/concepts/swr/#why-locality-first","title":"Why Locality First?","text":"<p>Classical recurrent operators mix information across the entire prefix of a sequence. Their transfer operator has exponentially decaying off-diagonal bands, which means long paths contribute almost nothing in practice. On GPUs, computing those paths anyway wastes bandwidth because the data has to visit the most expensive memory levels. SWRs embrace that decay. By truncating the operator to a jagged band, they conserve the useful parts of the recurrence while avoiding work that will numerically vanish, even in high precision.</p>"},{"location":"dist/spear/docs/concepts/swr/#tile-level-semantics","title":"Tile-Level Semantics","text":"<p>Within each tile a recurrence is computed exactly, using dense tensor cores and a zero-initialised carry. When a tile finishes, it snapshots the terminal state, multiplies it by the cumulative gate for the border positions, and hands off a single rank-1 update to the next tile. This pattern yields depth-one synchronisation across thread blocks and keeps the critical path short enough for <code>torch.compile</code> and other graph compilers.</p>"},{"location":"dist/spear/docs/concepts/swr/#relationship-to-other-mixers","title":"Relationship to Other Mixers","text":"<p>SWRs sit between pure local operators (sliding-window attention, short convolutions) and global recurrences. They retain dense modelling inside each tile like a convolution, but unlike windowed attention they expose an explicit recurrence that can be truncated or extended depending on hardware limits. In practice SWRs pair well with sparse attention: attention handles long-range routing, SWRs provide efficient token mixing within the receptive field.</p>"},{"location":"dist/spear/docs/concepts/swr/#inference-modes","title":"Inference Modes","text":"<p>The same jagged structure works for streaming inference. During prefill, blocks run in the jagged layout described above. For decoding, a <code>JagState</code> tracks the local accumulator, the inclusive gate product, and the previous block's terminal state. Both paths share the same numerical assumptions, so training and inference stay aligned.</p> <p>Refer to the BTP and Phalanx pages for the concrete algorithm and its PyTorch integration.</p>"},{"location":"dist/spear/examples/test_stability_log_space/","title":"Test stability log space","text":"In\u00a0[11]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\nimport os\nimport sys\n\nfrom einops import rearrange, repeat\nfrom pathlib import Path\nimport tempfile\nimport subprocess\n\n# Reproducibility\ntorch.manual_seed(0)\n\n# Ensure local package is importable\nREPO_ROOT = Path.cwd().resolve().parents[1]\nif str(REPO_ROOT) not in sys.path:\n    sys.path.insert(0, str(REPO_ROOT))\n\nHAS_SPEAR = True\ntry:\n    from spear.ops import btp as btp_ops\n    from spear.ops.btp.interface import DH\nexcept Exception as e:\n    print(f\"[warn] spear import failed: {e}\")\n    btp_ops = None\n    DH = 16  # fallback value\n    HAS_SPEAR = False\n\nPALETTE = {\n    \"ref_log\": \"#ff7f0e\",          # orange\n    \"ref_two\": \"#1f77b4\",          # blue (two-pass reference)\n    \"ref_log_stable\": \"#9467bd\",   # purple\n    \"unfused\": \"#d62728\",          # red\n    \"unfused_stable\": \"#8c564b\",   # brown\n}\n\ndef _run_unfused_subproc(A_path: str, x_path: str, out_dir: str, variant: str, do_backward: bool):\n    code = f\"\"\"\nimport os, sys, torch\nsys.path.insert(0, r\"{str(REPO_ROOT)}\")\nfrom spear.ops import btp as btp_ops\nA = torch.load(r\"{A_path}\").contiguous()\nx = torch.load(r\"{x_path}\").contiguous()\nA.requires_grad_({str(do_backward)})\nx.requires_grad_({str(do_backward)})\ny = btp_ops(A, x, k=2, output_dtype=torch.float32)\ntorch.save(y.detach(), r\"{out_dir}/y.pt\")\nif {str(do_backward)}:\n    y.float().sum().backward()\n    torch.save(x.grad.detach().float(), r\"{out_dir}/dx.pt\")\n    torch.save(A.grad.detach().float(), r\"{out_dir}/dA.pt\")\n\"\"\"\n    script_path = Path(out_dir) / \"_runner.py\"\n    script_path.write_text(code)\n    env = os.environ.copy()\n    env[\"SPEAR_USE_UNFUSED_STABLE\"] = \"1\" if variant == \"unfused_stable\" else \"0\"\n    env[\"PYTHONPATH\"] = f\"{str(REPO_ROOT)}:{env.get('PYTHONPATH','')}\"\n    python = sys.executable\n    subprocess.run([python, str(script_path)], check=True, env=env)\n\n\ndef nb_unfused_forward(A: torch.Tensor, x: torch.Tensor, variant: str = \"unfused\", dtype: torch.dtype = torch.float16) -&gt; torch.Tensor:\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA required for unfused kernels.\")\n    with tempfile.TemporaryDirectory() as td:\n        td = Path(td)\n        A_path, x_path = str(td / \"A.pt\"), str(td / \"x.pt\")\n        torch.save(A.to(\"cuda\", dtype=dtype), A_path)\n        torch.save(x.to(\"cuda\", dtype=dtype), x_path)\n        _run_unfused_subproc(A_path, x_path, str(td), variant=variant, do_backward=False)\n        y = torch.load(td / \"y.pt\", map_location=\"cuda\")\n    return y\n\n\ndef nb_unfused_backward(A: torch.Tensor, x: torch.Tensor, variant: str = \"unfused\", dtype: torch.dtype = torch.float16):\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA required for unfused kernels.\")\n    with tempfile.TemporaryDirectory() as td:\n        td = Path(td)\n        A_path, x_path = str(td / \"A.pt\"), str(td / \"x.pt\")\n        torch.save(A.to(\"cuda\", dtype=dtype), A_path)\n        torch.save(x.to(\"cuda\", dtype=dtype), x_path)\n        _run_unfused_subproc(A_path, x_path, str(td), variant=variant, do_backward=True)\n        y = torch.load(td / \"y.pt\", map_location=\"cuda\")\n        dx = torch.load(td / \"dx.pt\", map_location=\"cuda\")\n        dA = torch.load(td / \"dA.pt\", map_location=\"cuda\")\n    return y, dx, dA\n\n##############################################################################\n########################## BLOCK TWO PASS ####################################\n##############################################################################\n\ndef block_two_pass(u, log_a, L_constructor, BL=16):\n    \"\"\"\n    Two-pass convolution algorithm optimized for torch.compile and Triton.\n    \n    Input:\n    - u: (B, H, DH, N) - input tensor\n    - log_a: (B, H, N) - tensor of log coefficients\n    - L_constructor: function that constructs the L matrices\n    - BL: block size\n    \n    Output:\n    - x: (B, H, DH, N) - output tensor\n    \"\"\"\n    BS, H, DH, N = u.shape\n    NBL = N // BL\n\n    u, log_a = [rearrange(x, \"... (c l) -&gt; ... c l\", l=BL) for x in (u, log_a)]\n\n    # Initialize output tensor\n    x = torch.zeros_like(u)\n\n    # First pass: Compute T matrices (diagonal blocks) for local convolutions\n    L = L_constructor(log_a)\n    v = torch.einsum(\"shbij,shdbj-&gt;shdbi\", L, u)\n\n    # Second pass: Compute rank-1 cross-chunk update using previous block's carry\n    x[:, :, :, 0, :] = v[:, :, :, 0, :]\n    scale_factors = torch.exp(log_a[:, :, 1:, 0])  # (B, H, NBL-1)\n    first_cols = L[:, :, 1:, :, 0]                 # (B, H, NBL-1, BL)\n\n    g = scale_factors[..., None] * first_cols      # (B, H, NBL-1, BL)\n    prev_carry = v[:, :, :, :-1, -1][..., None]    # (B, H, DH, NBL-1, 1)\n    carry_over = g[:, :, None] * prev_carry        # (B, H, DH, NBL-1, BL)\n\n    x[:, :, :, 1:, :] = carry_over + v[:, :, :, 1:, :]\n\n    x = rearrange(x, \"... c l -&gt; ... (c l)\")\n    return x\n\n\n########################## SEGSUM ############################################\n##############################################################################\n\ndef segsum(x):\n    \"\"\"Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix.\"\"\"\n    L = x.size(-1)\n    x_cumsum = torch.cumsum(x, dim=-1)\n    x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n    mask = torch.tril(torch.ones(L, L, device=x.device, dtype=bool), diagonal=0)\n    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n    return x_segsum\n\ndef construct_L_logsumexp(log_a):\n    \"\"\"Construct L matrices for logsumexp-based computation.\"\"\"\n    return torch.exp(segsum(log_a))\n\n\n\n##############################################################################\n#################### SEGSUM STABLE ###########################################\n##############################################################################\n\n\ndef segsum_stable(x):\n    \"\"\"More stable segment sum calculation.\"\"\"\n    T = x.size(-1)\n    x = repeat(x, \"... d -&gt; ... d e\", e=T)\n    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=-1)\n    x = x.masked_fill(~mask, 0)\n    x_segsum = torch.cumsum(x, dim=-2)\n    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n    return x_segsum\n\ndef construct_L_logsumexp_stable(log_a):\n    \"\"\"Construct L matrices for logsumexp-based computation.\"\"\"\n    return torch.exp(segsum_stable(log_a))\n\n##############################################################################\n######################### REF MATERIALIZATION ##############################\n##############################################################################\n\ndef construct_L_ref(log_a):\n    \"\"\"\n    Construct L matrices using explicit product in double precision. \n    L_ij = a_i * a_(i-1) * ... * a_(j+1) if i &gt; j, \n    L_ij = 1                             if i = j, \n    L_ij = 0                             if i &lt; j.\n\n    Input:\n    - log_a: (..., N) - tensor of log coefficients\n\n    Output:\n    - L: (..., N, N) - L matrices\n    \"\"\"\n    log_a_type = log_a.dtype\n    log_a = log_a.double()\n    a = torch.exp(log_a)\n    L = torch.zeros(a.shape[:-1] + (a.shape[-1], a.shape[-1]), device=a.device, dtype=a.dtype)\n    for i in range(a.shape[-1]):\n        for j in range(i):\n            if i &gt; j:\n                L[..., i, j] = torch.prod(a[..., j+1:i+1], dim=-1)\n        L[..., i, i] = 1\n    L = L.to(log_a_type)\n    return L\n\n##############################################################################\n######################### SEQUENTIAL SCAN ####################################\n##############################################################################\n\ndef sequential_scan(u, log_a):\n    \"\"\"\n    Sequential scan implementation for reference.\n\n    Input:\n    - u: (B, H, DH, N) - input tensor\n    - log_a: (B, H, N) - tensor of log coefficients\n    \n    Output:\n    - x: (B, H, DH, N) - output tensor\n    \"\"\"\n    BS, H, DH, N = u.shape\n    x = torch.zeros_like(u)\n    a = torch.exp(log_a)\n    x[:, :, :, 0] = u[:, :, :, 0]\n    for i in range(1, N):\n        x[:, :, :, i] = x[:, :, :, i-1] * a[:, :, i] + u[:, :, :, i]\n    return x\n</pre> import matplotlib.pyplot as plt import torch import os import sys  from einops import rearrange, repeat from pathlib import Path import tempfile import subprocess  # Reproducibility torch.manual_seed(0)  # Ensure local package is importable REPO_ROOT = Path.cwd().resolve().parents[1] if str(REPO_ROOT) not in sys.path:     sys.path.insert(0, str(REPO_ROOT))  HAS_SPEAR = True try:     from spear.ops import btp as btp_ops     from spear.ops.btp.interface import DH except Exception as e:     print(f\"[warn] spear import failed: {e}\")     btp_ops = None     DH = 16  # fallback value     HAS_SPEAR = False  PALETTE = {     \"ref_log\": \"#ff7f0e\",          # orange     \"ref_two\": \"#1f77b4\",          # blue (two-pass reference)     \"ref_log_stable\": \"#9467bd\",   # purple     \"unfused\": \"#d62728\",          # red     \"unfused_stable\": \"#8c564b\",   # brown }  def _run_unfused_subproc(A_path: str, x_path: str, out_dir: str, variant: str, do_backward: bool):     code = f\"\"\" import os, sys, torch sys.path.insert(0, r\"{str(REPO_ROOT)}\") from spear.ops import btp as btp_ops A = torch.load(r\"{A_path}\").contiguous() x = torch.load(r\"{x_path}\").contiguous() A.requires_grad_({str(do_backward)}) x.requires_grad_({str(do_backward)}) y = btp_ops(A, x, k=2, output_dtype=torch.float32) torch.save(y.detach(), r\"{out_dir}/y.pt\") if {str(do_backward)}:     y.float().sum().backward()     torch.save(x.grad.detach().float(), r\"{out_dir}/dx.pt\")     torch.save(A.grad.detach().float(), r\"{out_dir}/dA.pt\") \"\"\"     script_path = Path(out_dir) / \"_runner.py\"     script_path.write_text(code)     env = os.environ.copy()     env[\"SPEAR_USE_UNFUSED_STABLE\"] = \"1\" if variant == \"unfused_stable\" else \"0\"     env[\"PYTHONPATH\"] = f\"{str(REPO_ROOT)}:{env.get('PYTHONPATH','')}\"     python = sys.executable     subprocess.run([python, str(script_path)], check=True, env=env)   def nb_unfused_forward(A: torch.Tensor, x: torch.Tensor, variant: str = \"unfused\", dtype: torch.dtype = torch.float16) -&gt; torch.Tensor:     if not torch.cuda.is_available():         raise RuntimeError(\"CUDA required for unfused kernels.\")     with tempfile.TemporaryDirectory() as td:         td = Path(td)         A_path, x_path = str(td / \"A.pt\"), str(td / \"x.pt\")         torch.save(A.to(\"cuda\", dtype=dtype), A_path)         torch.save(x.to(\"cuda\", dtype=dtype), x_path)         _run_unfused_subproc(A_path, x_path, str(td), variant=variant, do_backward=False)         y = torch.load(td / \"y.pt\", map_location=\"cuda\")     return y   def nb_unfused_backward(A: torch.Tensor, x: torch.Tensor, variant: str = \"unfused\", dtype: torch.dtype = torch.float16):     if not torch.cuda.is_available():         raise RuntimeError(\"CUDA required for unfused kernels.\")     with tempfile.TemporaryDirectory() as td:         td = Path(td)         A_path, x_path = str(td / \"A.pt\"), str(td / \"x.pt\")         torch.save(A.to(\"cuda\", dtype=dtype), A_path)         torch.save(x.to(\"cuda\", dtype=dtype), x_path)         _run_unfused_subproc(A_path, x_path, str(td), variant=variant, do_backward=True)         y = torch.load(td / \"y.pt\", map_location=\"cuda\")         dx = torch.load(td / \"dx.pt\", map_location=\"cuda\")         dA = torch.load(td / \"dA.pt\", map_location=\"cuda\")     return y, dx, dA  ############################################################################## ########################## BLOCK TWO PASS #################################### ##############################################################################  def block_two_pass(u, log_a, L_constructor, BL=16):     \"\"\"     Two-pass convolution algorithm optimized for torch.compile and Triton.          Input:     - u: (B, H, DH, N) - input tensor     - log_a: (B, H, N) - tensor of log coefficients     - L_constructor: function that constructs the L matrices     - BL: block size          Output:     - x: (B, H, DH, N) - output tensor     \"\"\"     BS, H, DH, N = u.shape     NBL = N // BL      u, log_a = [rearrange(x, \"... (c l) -&gt; ... c l\", l=BL) for x in (u, log_a)]      # Initialize output tensor     x = torch.zeros_like(u)      # First pass: Compute T matrices (diagonal blocks) for local convolutions     L = L_constructor(log_a)     v = torch.einsum(\"shbij,shdbj-&gt;shdbi\", L, u)      # Second pass: Compute rank-1 cross-chunk update using previous block's carry     x[:, :, :, 0, :] = v[:, :, :, 0, :]     scale_factors = torch.exp(log_a[:, :, 1:, 0])  # (B, H, NBL-1)     first_cols = L[:, :, 1:, :, 0]                 # (B, H, NBL-1, BL)      g = scale_factors[..., None] * first_cols      # (B, H, NBL-1, BL)     prev_carry = v[:, :, :, :-1, -1][..., None]    # (B, H, DH, NBL-1, 1)     carry_over = g[:, :, None] * prev_carry        # (B, H, DH, NBL-1, BL)      x[:, :, :, 1:, :] = carry_over + v[:, :, :, 1:, :]      x = rearrange(x, \"... c l -&gt; ... (c l)\")     return x   ########################## SEGSUM ############################################ ##############################################################################  def segsum(x):     \"\"\"Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix.\"\"\"     L = x.size(-1)     x_cumsum = torch.cumsum(x, dim=-1)     x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]     mask = torch.tril(torch.ones(L, L, device=x.device, dtype=bool), diagonal=0)     x_segsum = x_segsum.masked_fill(~mask, -torch.inf)     return x_segsum  def construct_L_logsumexp(log_a):     \"\"\"Construct L matrices for logsumexp-based computation.\"\"\"     return torch.exp(segsum(log_a))    ############################################################################## #################### SEGSUM STABLE ########################################### ##############################################################################   def segsum_stable(x):     \"\"\"More stable segment sum calculation.\"\"\"     T = x.size(-1)     x = repeat(x, \"... d -&gt; ... d e\", e=T)     mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=-1)     x = x.masked_fill(~mask, 0)     x_segsum = torch.cumsum(x, dim=-2)     mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)     x_segsum = x_segsum.masked_fill(~mask, -torch.inf)     return x_segsum  def construct_L_logsumexp_stable(log_a):     \"\"\"Construct L matrices for logsumexp-based computation.\"\"\"     return torch.exp(segsum_stable(log_a))  ############################################################################## ######################### REF MATERIALIZATION ############################## ##############################################################################  def construct_L_ref(log_a):     \"\"\"     Construct L matrices using explicit product in double precision.      L_ij = a_i * a_(i-1) * ... * a_(j+1) if i &gt; j,      L_ij = 1                             if i = j,      L_ij = 0                             if i &lt; j.      Input:     - log_a: (..., N) - tensor of log coefficients      Output:     - L: (..., N, N) - L matrices     \"\"\"     log_a_type = log_a.dtype     log_a = log_a.double()     a = torch.exp(log_a)     L = torch.zeros(a.shape[:-1] + (a.shape[-1], a.shape[-1]), device=a.device, dtype=a.dtype)     for i in range(a.shape[-1]):         for j in range(i):             if i &gt; j:                 L[..., i, j] = torch.prod(a[..., j+1:i+1], dim=-1)         L[..., i, i] = 1     L = L.to(log_a_type)     return L  ############################################################################## ######################### SEQUENTIAL SCAN #################################### ##############################################################################  def sequential_scan(u, log_a):     \"\"\"     Sequential scan implementation for reference.      Input:     - u: (B, H, DH, N) - input tensor     - log_a: (B, H, N) - tensor of log coefficients          Output:     - x: (B, H, DH, N) - output tensor     \"\"\"     BS, H, DH, N = u.shape     x = torch.zeros_like(u)     a = torch.exp(log_a)     x[:, :, :, 0] = u[:, :, :, 0]     for i in range(1, N):         x[:, :, :, i] = x[:, :, :, i-1] * a[:, :, i] + u[:, :, :, i]     return x In\u00a0[12]: Copied! <pre>BS = 16\nH = 16\nDH = 16  # BTP function requires DH=16\nN = 1024\na_min = .01\na_max = .9\n</pre> BS = 16 H = 16 DH = 16  # BTP function requires DH=16 N = 1024 a_min = .01 a_max = .9 In\u00a0[13]: Copied! <pre># Compare segsum methods standalone\nBL = 16\nlog_a = torch.log(torch.rand(BS, H, N) * (a_max - a_min) + a_min)\nlog_a = rearrange(log_a, \"... (c l) -&gt; ... c l\", l=BL)\n\nL_segsum = construct_L_logsumexp(log_a)\nL_segsum_stable = construct_L_logsumexp_stable(log_a)\nL_segsum_ref = construct_L_ref(log_a)\n\nerr_segsum_vs_ref = (L_segsum - L_segsum_ref).abs().max()\nerr_segsum_stable_vs_ref = (L_segsum_stable - L_segsum_ref).abs().max()\n\nprint(f\"Error between segsum and naive: {err_segsum_vs_ref}\")\nprint(f\"Error between segsum stable and naive: {err_segsum_stable_vs_ref}\")\n</pre> # Compare segsum methods standalone BL = 16 log_a = torch.log(torch.rand(BS, H, N) * (a_max - a_min) + a_min) log_a = rearrange(log_a, \"... (c l) -&gt; ... c l\", l=BL)  L_segsum = construct_L_logsumexp(log_a) L_segsum_stable = construct_L_logsumexp_stable(log_a) L_segsum_ref = construct_L_ref(log_a)  err_segsum_vs_ref = (L_segsum - L_segsum_ref).abs().max() err_segsum_stable_vs_ref = (L_segsum_stable - L_segsum_ref).abs().max()  print(f\"Error between segsum and naive: {err_segsum_vs_ref}\") print(f\"Error between segsum stable and naive: {err_segsum_stable_vs_ref}\") <pre>Error between segsum and naive: 1.6689300537109375e-06\nError between segsum stable and naive: 5.960464477539063e-08\n</pre> In\u00a0[14]: Copied! <pre>a = torch.rand(BS, H, N) * (a_max - a_min) + a_min\nlog_a = torch.log(a)\nu = torch.ones(BS, H, DH, N)\n\nwith torch.no_grad():\n    x_btp_bl_16 = block_two_pass(u, log_a, construct_L_logsumexp, BL=16)\n    x_btp_bl_32 = block_two_pass(u, log_a, construct_L_logsumexp, BL=32)\n    x_btp_bl_64 = block_two_pass(u, log_a, construct_L_logsumexp, BL=64)\n    x_btp_bl_128 = block_two_pass(u, log_a, construct_L_logsumexp, BL=128)\n    x_btp_bl_16_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=16)\n    x_btp_bl_32_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=32)\n    x_btp_bl_64_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=64)\n    x_btp_bl_128_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=128)\n    x_ref_bl_16 = block_two_pass(u, log_a, construct_L_ref, BL=16)\n    x_ref_bl_32 = block_two_pass(u, log_a, construct_L_ref, BL=32)\n    x_ref_bl_64 = block_two_pass(u, log_a, construct_L_ref, BL=64)\n    x_ref_bl_128 = block_two_pass(u, log_a, construct_L_ref, BL=128)\n\n# Compute CUDA unfused kernels (if CUDA is available and spear loaded)\ny_unf_stable = None\ny_unf_default_cpu = None\nif torch.cuda.is_available() and HAS_SPEAR:\n    device = \"cuda\"\n    # Kernel expects DH=DH and half/bfloat16 input\n    x_k = torch.ones(BS, H, DH, N, device=device, dtype=torch.float16)\n    A_k = a.to(device=device, dtype=torch.float16)\n    y_unf_stable = btp_ops(A_k, x_k, k=2, output_dtype=torch.float32)\n    y_unf_stable_cpu = y_unf_stable.float().mean(dim=(0,1,2)).detach().cpu().numpy()\n    try:\n        y_unf_default = nb_unfused_forward(A_k, x_k, variant=\"unfused\")\n        y_unf_default_cpu = y_unf_default.float().mean(dim=(0,1,2)).detach().cpu().numpy()\n    except Exception as e:\n        print(f\"[warn] CUDA unfused default failed: {e}\")\n\nfig, axs = plt.subplots(3, 1, figsize=(10, 10))\naxs[0].set_title(\"Block Two-Pass Stability Comparison (float32)\")\naxs[0].plot(x_btp_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (current)\")\naxs[0].plot(x_btp_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (current)\")\naxs[0].plot(x_btp_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (current)\")\naxs[0].plot(x_btp_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (current)\")\naxs[0].plot(x_btp_bl_16_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable)\")\naxs[0].plot(x_btp_bl_32_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable)\")\naxs[0].plot(x_btp_bl_64_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable)\")\naxs[0].plot(x_btp_bl_128_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable)\")\naxs[0].plot(x_ref_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (ref)\")\naxs[0].plot(x_ref_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (ref)\")\naxs[0].plot(x_ref_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (ref)\")\naxs[0].plot(x_ref_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (ref)\")\nif y_unf_stable is not None:\n    axs[0].plot(y_unf_stable_cpu, label=\"CUDA unfused_stable\", color=PALETTE[\"unfused_stable\"])\nif y_unf_default_cpu is not None:\n    axs[0].plot(y_unf_default_cpu, label=\"CUDA unfused\", color=PALETTE[\"unfused\"])\naxs[0].legend()\naxs[1].plot((x_btp_bl_16 - x_ref_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 - Ref\")\naxs[1].plot((x_btp_bl_32 - x_ref_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 - Ref\")\naxs[1].plot((x_btp_bl_64 - x_ref_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 - Ref\")\naxs[1].plot((x_btp_bl_128 - x_ref_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 - Ref\")\naxs[1].legend()\naxs[2].plot((x_btp_bl_16_stable - x_ref_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable) - Ref\")\naxs[2].plot((x_btp_bl_32_stable - x_ref_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable) - Ref\")\naxs[2].plot((x_btp_bl_64_stable - x_ref_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable) - Ref\")\naxs[2].plot((x_btp_bl_128_stable - x_ref_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable) - Ref\")\nif y_unf_stable is not None:\n    ref16_mean = x_ref_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()\n    axs[2].plot(y_unf_stable_cpu - ref16_mean, label=\"CUDA unfused_stable - Ref\", color=PALETTE[\"unfused_stable\"])\nif y_unf_default_cpu is not None:\n    ref16_mean = x_ref_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()\n    axs[2].plot(y_unf_default_cpu - ref16_mean, label=\"CUDA unfused - Ref\", color=PALETTE[\"unfused\"])\naxs[2].legend()\nplt.show()\nfig.tight_layout()\n</pre> a = torch.rand(BS, H, N) * (a_max - a_min) + a_min log_a = torch.log(a) u = torch.ones(BS, H, DH, N)  with torch.no_grad():     x_btp_bl_16 = block_two_pass(u, log_a, construct_L_logsumexp, BL=16)     x_btp_bl_32 = block_two_pass(u, log_a, construct_L_logsumexp, BL=32)     x_btp_bl_64 = block_two_pass(u, log_a, construct_L_logsumexp, BL=64)     x_btp_bl_128 = block_two_pass(u, log_a, construct_L_logsumexp, BL=128)     x_btp_bl_16_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=16)     x_btp_bl_32_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=32)     x_btp_bl_64_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=64)     x_btp_bl_128_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=128)     x_ref_bl_16 = block_two_pass(u, log_a, construct_L_ref, BL=16)     x_ref_bl_32 = block_two_pass(u, log_a, construct_L_ref, BL=32)     x_ref_bl_64 = block_two_pass(u, log_a, construct_L_ref, BL=64)     x_ref_bl_128 = block_two_pass(u, log_a, construct_L_ref, BL=128)  # Compute CUDA unfused kernels (if CUDA is available and spear loaded) y_unf_stable = None y_unf_default_cpu = None if torch.cuda.is_available() and HAS_SPEAR:     device = \"cuda\"     # Kernel expects DH=DH and half/bfloat16 input     x_k = torch.ones(BS, H, DH, N, device=device, dtype=torch.float16)     A_k = a.to(device=device, dtype=torch.float16)     y_unf_stable = btp_ops(A_k, x_k, k=2, output_dtype=torch.float32)     y_unf_stable_cpu = y_unf_stable.float().mean(dim=(0,1,2)).detach().cpu().numpy()     try:         y_unf_default = nb_unfused_forward(A_k, x_k, variant=\"unfused\")         y_unf_default_cpu = y_unf_default.float().mean(dim=(0,1,2)).detach().cpu().numpy()     except Exception as e:         print(f\"[warn] CUDA unfused default failed: {e}\")  fig, axs = plt.subplots(3, 1, figsize=(10, 10)) axs[0].set_title(\"Block Two-Pass Stability Comparison (float32)\") axs[0].plot(x_btp_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (current)\") axs[0].plot(x_btp_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (current)\") axs[0].plot(x_btp_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (current)\") axs[0].plot(x_btp_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (current)\") axs[0].plot(x_btp_bl_16_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable)\") axs[0].plot(x_btp_bl_32_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable)\") axs[0].plot(x_btp_bl_64_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable)\") axs[0].plot(x_btp_bl_128_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable)\") axs[0].plot(x_ref_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (ref)\") axs[0].plot(x_ref_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (ref)\") axs[0].plot(x_ref_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (ref)\") axs[0].plot(x_ref_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (ref)\") if y_unf_stable is not None:     axs[0].plot(y_unf_stable_cpu, label=\"CUDA unfused_stable\", color=PALETTE[\"unfused_stable\"]) if y_unf_default_cpu is not None:     axs[0].plot(y_unf_default_cpu, label=\"CUDA unfused\", color=PALETTE[\"unfused\"]) axs[0].legend() axs[1].plot((x_btp_bl_16 - x_ref_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 - Ref\") axs[1].plot((x_btp_bl_32 - x_ref_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 - Ref\") axs[1].plot((x_btp_bl_64 - x_ref_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 - Ref\") axs[1].plot((x_btp_bl_128 - x_ref_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 - Ref\") axs[1].legend() axs[2].plot((x_btp_bl_16_stable - x_ref_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable) - Ref\") axs[2].plot((x_btp_bl_32_stable - x_ref_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable) - Ref\") axs[2].plot((x_btp_bl_64_stable - x_ref_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable) - Ref\") axs[2].plot((x_btp_bl_128_stable - x_ref_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable) - Ref\") if y_unf_stable is not None:     ref16_mean = x_ref_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()     axs[2].plot(y_unf_stable_cpu - ref16_mean, label=\"CUDA unfused_stable - Ref\", color=PALETTE[\"unfused_stable\"]) if y_unf_default_cpu is not None:     ref16_mean = x_ref_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()     axs[2].plot(y_unf_default_cpu - ref16_mean, label=\"CUDA unfused - Ref\", color=PALETTE[\"unfused\"]) axs[2].legend() plt.show() fig.tight_layout()   In\u00a0[15]: Copied! <pre>a = torch.rand(BS, H, N, dtype=torch.float16) * (a_max - a_min) + a_min\nlog_a = torch.log(a)\nu = torch.ones(BS, H, DH, N, dtype=torch.float16)\n\nwith torch.no_grad():\n    x_btp_bl_16 = block_two_pass(u, log_a, construct_L_logsumexp, BL=16)\n    x_btp_bl_32 = block_two_pass(u, log_a, construct_L_logsumexp, BL=32)\n    x_btp_bl_64 = block_two_pass(u, log_a, construct_L_logsumexp, BL=64)\n    x_btp_bl_128 = block_two_pass(u, log_a, construct_L_logsumexp, BL=128)\n    x_btp_bl_16_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=16)\n    x_btp_bl_32_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=32)\n    x_btp_bl_64_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=64)\n    x_btp_bl_128_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=128)\n    x_seq_bl_16 = block_two_pass(u, log_a, construct_L_ref, BL=16)\n    x_seq_bl_32 = block_two_pass(u, log_a, construct_L_ref, BL=32)\n    x_seq_bl_64 = block_two_pass(u, log_a, construct_L_ref, BL=64)\n    x_seq_bl_128 = block_two_pass(u, log_a, construct_L_ref, BL=128)\n\n# Compute CUDA unfused kernels (if CUDA is available and spear loaded)\ny_unf_stable = None\ny_unf_default_cpu = None\nif torch.cuda.is_available() and HAS_SPEAR:\n    device = \"cuda\"\n    x_k = torch.ones(BS, H, DH, N, device=device, dtype=torch.float16)\n    A_k = a.to(device=device, dtype=torch.float16)\n    y_unf_stable = btp_ops(A_k, x_k, k=2, output_dtype=torch.float32)\n    y_unf_stable_cpu = y_unf_stable.float().mean(dim=(0,1,2)).detach().cpu().numpy()\n    try:\n        y_unf_default = nb_unfused_forward(A_k, x_k, variant=\"unfused\")\n        y_unf_default_cpu = y_unf_default.float().mean(dim=(0,1,2)).detach().cpu().numpy()\n    except Exception as e:\n        print(f\"[warn] CUDA unfused default failed: {e}\")\n\nfig, axs = plt.subplots(3, 1, figsize=(10, 10))\naxs[0].set_title(\"Block Two-Pass Stability Comparison (float16)\")\naxs[0].plot(x_btp_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (current)\")\naxs[0].plot(x_btp_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (current)\")\naxs[0].plot(x_btp_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (current)\")\naxs[0].plot(x_btp_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (current)\")\naxs[0].plot(x_btp_bl_16_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable)\")\naxs[0].plot(x_btp_bl_32_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable)\")\naxs[0].plot(x_btp_bl_64_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable)\")\naxs[0].plot(x_btp_bl_128_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable)\")\naxs[0].plot(x_seq_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (ref)\")\naxs[0].plot(x_seq_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (ref)\")\naxs[0].plot(x_seq_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (ref)\")\naxs[0].plot(x_seq_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (ref)\")\nif y_unf_stable is not None:\n    axs[0].plot(y_unf_stable_cpu, label=\"CUDA unfused_stable\", color=PALETTE[\"unfused_stable\"]) \nif y_unf_default_cpu is not None:\n    axs[0].plot(y_unf_default_cpu, label=\"CUDA unfused\", color=PALETTE[\"unfused\"])\naxs[0].legend()\naxs[1].plot((x_btp_bl_16 - x_seq_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 - Ref\")\naxs[1].plot((x_btp_bl_32 - x_seq_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 - Ref\")\naxs[1].plot((x_btp_bl_64 - x_seq_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 - Ref\")\naxs[1].plot((x_btp_bl_128 - x_seq_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 - Ref\")\naxs[1].legend()\naxs[2].plot((x_btp_bl_16_stable - x_seq_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable) - Ref\")\naxs[2].plot((x_btp_bl_32_stable - x_seq_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable) - Ref\")\naxs[2].plot((x_btp_bl_64_stable - x_seq_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable) - Ref\")\naxs[2].plot((x_btp_bl_128_stable - x_seq_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable) - Ref\")\nif y_unf_stable is not None:\n    ref16_mean = x_seq_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()\n    axs[2].plot(y_unf_stable_cpu - ref16_mean, label=\"CUDA unfused_stable - Ref\", color=PALETTE[\"unfused_stable\"]) \nif y_unf_default_cpu is not None:\n    ref16_mean = x_seq_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()\n    axs[2].plot(y_unf_default_cpu - ref16_mean, label=\"CUDA unfused - Ref\", color=PALETTE[\"unfused\"])\naxs[2].legend()\nplt.show()\nfig.tight_layout()\n</pre> a = torch.rand(BS, H, N, dtype=torch.float16) * (a_max - a_min) + a_min log_a = torch.log(a) u = torch.ones(BS, H, DH, N, dtype=torch.float16)  with torch.no_grad():     x_btp_bl_16 = block_two_pass(u, log_a, construct_L_logsumexp, BL=16)     x_btp_bl_32 = block_two_pass(u, log_a, construct_L_logsumexp, BL=32)     x_btp_bl_64 = block_two_pass(u, log_a, construct_L_logsumexp, BL=64)     x_btp_bl_128 = block_two_pass(u, log_a, construct_L_logsumexp, BL=128)     x_btp_bl_16_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=16)     x_btp_bl_32_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=32)     x_btp_bl_64_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=64)     x_btp_bl_128_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=128)     x_seq_bl_16 = block_two_pass(u, log_a, construct_L_ref, BL=16)     x_seq_bl_32 = block_two_pass(u, log_a, construct_L_ref, BL=32)     x_seq_bl_64 = block_two_pass(u, log_a, construct_L_ref, BL=64)     x_seq_bl_128 = block_two_pass(u, log_a, construct_L_ref, BL=128)  # Compute CUDA unfused kernels (if CUDA is available and spear loaded) y_unf_stable = None y_unf_default_cpu = None if torch.cuda.is_available() and HAS_SPEAR:     device = \"cuda\"     x_k = torch.ones(BS, H, DH, N, device=device, dtype=torch.float16)     A_k = a.to(device=device, dtype=torch.float16)     y_unf_stable = btp_ops(A_k, x_k, k=2, output_dtype=torch.float32)     y_unf_stable_cpu = y_unf_stable.float().mean(dim=(0,1,2)).detach().cpu().numpy()     try:         y_unf_default = nb_unfused_forward(A_k, x_k, variant=\"unfused\")         y_unf_default_cpu = y_unf_default.float().mean(dim=(0,1,2)).detach().cpu().numpy()     except Exception as e:         print(f\"[warn] CUDA unfused default failed: {e}\")  fig, axs = plt.subplots(3, 1, figsize=(10, 10)) axs[0].set_title(\"Block Two-Pass Stability Comparison (float16)\") axs[0].plot(x_btp_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (current)\") axs[0].plot(x_btp_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (current)\") axs[0].plot(x_btp_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (current)\") axs[0].plot(x_btp_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (current)\") axs[0].plot(x_btp_bl_16_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable)\") axs[0].plot(x_btp_bl_32_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable)\") axs[0].plot(x_btp_bl_64_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable)\") axs[0].plot(x_btp_bl_128_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable)\") axs[0].plot(x_seq_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (ref)\") axs[0].plot(x_seq_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (ref)\") axs[0].plot(x_seq_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (ref)\") axs[0].plot(x_seq_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (ref)\") if y_unf_stable is not None:     axs[0].plot(y_unf_stable_cpu, label=\"CUDA unfused_stable\", color=PALETTE[\"unfused_stable\"])  if y_unf_default_cpu is not None:     axs[0].plot(y_unf_default_cpu, label=\"CUDA unfused\", color=PALETTE[\"unfused\"]) axs[0].legend() axs[1].plot((x_btp_bl_16 - x_seq_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 - Ref\") axs[1].plot((x_btp_bl_32 - x_seq_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 - Ref\") axs[1].plot((x_btp_bl_64 - x_seq_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 - Ref\") axs[1].plot((x_btp_bl_128 - x_seq_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 - Ref\") axs[1].legend() axs[2].plot((x_btp_bl_16_stable - x_seq_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable) - Ref\") axs[2].plot((x_btp_bl_32_stable - x_seq_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable) - Ref\") axs[2].plot((x_btp_bl_64_stable - x_seq_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable) - Ref\") axs[2].plot((x_btp_bl_128_stable - x_seq_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable) - Ref\") if y_unf_stable is not None:     ref16_mean = x_seq_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()     axs[2].plot(y_unf_stable_cpu - ref16_mean, label=\"CUDA unfused_stable - Ref\", color=PALETTE[\"unfused_stable\"])  if y_unf_default_cpu is not None:     ref16_mean = x_seq_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()     axs[2].plot(y_unf_default_cpu - ref16_mean, label=\"CUDA unfused - Ref\", color=PALETTE[\"unfused\"]) axs[2].legend() plt.show() fig.tight_layout()   In\u00a0[16]: Copied! <pre># Backward dX comparison (reference vs CUDA unfused_stable)\n# Uses float16 inputs for the CUDA kernel; reference computed in FP32.\n\n# Prepare inputs\nuse_cuda_kernel = torch.cuda.is_available() and HAS_SPEAR\ndevice_k = \"cuda\" if use_cuda_kernel else \"cpu\"\n\n# Random inputs (match earlier BS,H,N) and enforce DH for kernel\ntorch.manual_seed(0)\nDH_k = DH if use_cuda_kernel else DH\nA_base = torch.rand(H, N, device=device_k) * (a_max - a_min) + a_min\nx_base = torch.randn(BS, H, DH_k, N, device=device_k)\n\n# --- CUDA unfused_stable backward (if available) ---\ny_unf = None\nif use_cuda_kernel:\n    A_k = A_base.detach().clone().to(dtype=torch.float16).requires_grad_(True)\n    x_k = x_base.detach().clone().to(dtype=torch.float16).requires_grad_(True)\n    y_unf = btp_ops(A_k.unsqueeze(0).expand(BS, -1, -1).contiguous(), x_k, k=2, output_dtype=torch.float32)\n    upstream_unf = torch.ones_like(y_unf, dtype=torch.float32)\n    y_unf.backward(upstream_unf)\n    dx_unf = x_k.grad.detach().float()\n    dA_unf = A_k.grad.detach().float()  # [H,L]\n    # Default unfused via subprocess to avoid extension clashes\n    try:\n        _ydef, dx_unf_def, dA_unf_def = nb_unfused_backward(A_k.unsqueeze(0).expand(BS, -1, -1).contiguous(), x_k, variant=\"unfused\")\n    except Exception as e:\n        dx_unf_def = None\n        dA_unf_def = None\n        print(f\"[warn] CUDA unfused default backward failed: {e}\")\n\n# --- Reference backward in FP32 ---\nA_r = A_base.detach().clone().to(device_k).requires_grad_(True)\nx_r = x_base.detach().clone().to(device_k).requires_grad_(True)\nA_r32 = A_r.float()\nx_r32 = x_r.float()\nA_single_r32 = A_r32.unsqueeze(0).expand(BS, -1, -1)\nlog_a_r32 = torch.log(A_single_r32)\ny_ref = block_two_pass(x_r32, log_a_r32, construct_L_logsumexp_stable, BL=16)\nupstream_ref = torch.ones_like(y_ref, dtype=torch.float32)\ny_ref.backward(upstream_ref)\n\ndx_ref = x_r.grad.detach().float()\ndA_ref = A_r.grad.detach().float()  # [H,L]\n\n# Reduce to 1D curves for visualization\nref_dx_curve = dx_ref.mean(dim=(0,1,2)).cpu().numpy()\nif y_unf is not None:\n    unf_dx_curve = dx_unf.mean(dim=(0,1,2)).cpu().numpy()\n\nplt.figure(figsize=(10,4), constrained_layout=True)\nplt.title(\"Backward dX comparison (mean over B,H,D)\")\nplt.plot(ref_dx_curve, label=\"reference (stable FP32)\")\nif y_unf is not None:\n    plt.plot(unf_dx_curve, label=\"CUDA unfused_stable\", color=PALETTE[\"unfused_stable\"]) \nif use_cuda_kernel and (dx_unf_def is not None):\n    plt.plot(dx_unf_def.mean(dim=(0,1,2)).cpu().numpy(), label=\"CUDA unfused\", color=PALETTE[\"unfused\"]) \nplt.legend()\n\n# Error (mean abs residual) on log-scale for readability\nif y_unf is not None:\n    err_dx_curve = (dx_unf - dx_ref).abs().mean(dim=(0,1,2)).cpu().numpy()\n    max_e = float(err_dx_curve.max())\n    mean_e = float(err_dx_curve.mean())\n    plt.figure(figsize=(10,4), constrained_layout=True)\n    plt.title(f\"Backward dX error |CUDA - ref| (mean) [log] | max={max_e:.2e}, mean={mean_e:.2e}\")\n    plt.semilogy(err_dx_curve + 1e-16, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"]) \n    if dx_unf_def is not None:\n        err_dx_curve_def = (dx_unf_def - dx_ref).abs().mean(dim=(0,1,2)).cpu().numpy()\n        plt.semilogy(err_dx_curve_def + 1e-16, label=\"unfused\", color=PALETTE[\"unfused\"]) \n    plt.grid(True, which=\"both\", ls=\":\", alpha=0.4)\n    plt.legend()\n</pre> # Backward dX comparison (reference vs CUDA unfused_stable) # Uses float16 inputs for the CUDA kernel; reference computed in FP32.  # Prepare inputs use_cuda_kernel = torch.cuda.is_available() and HAS_SPEAR device_k = \"cuda\" if use_cuda_kernel else \"cpu\"  # Random inputs (match earlier BS,H,N) and enforce DH for kernel torch.manual_seed(0) DH_k = DH if use_cuda_kernel else DH A_base = torch.rand(H, N, device=device_k) * (a_max - a_min) + a_min x_base = torch.randn(BS, H, DH_k, N, device=device_k)  # --- CUDA unfused_stable backward (if available) --- y_unf = None if use_cuda_kernel:     A_k = A_base.detach().clone().to(dtype=torch.float16).requires_grad_(True)     x_k = x_base.detach().clone().to(dtype=torch.float16).requires_grad_(True)     y_unf = btp_ops(A_k.unsqueeze(0).expand(BS, -1, -1).contiguous(), x_k, k=2, output_dtype=torch.float32)     upstream_unf = torch.ones_like(y_unf, dtype=torch.float32)     y_unf.backward(upstream_unf)     dx_unf = x_k.grad.detach().float()     dA_unf = A_k.grad.detach().float()  # [H,L]     # Default unfused via subprocess to avoid extension clashes     try:         _ydef, dx_unf_def, dA_unf_def = nb_unfused_backward(A_k.unsqueeze(0).expand(BS, -1, -1).contiguous(), x_k, variant=\"unfused\")     except Exception as e:         dx_unf_def = None         dA_unf_def = None         print(f\"[warn] CUDA unfused default backward failed: {e}\")  # --- Reference backward in FP32 --- A_r = A_base.detach().clone().to(device_k).requires_grad_(True) x_r = x_base.detach().clone().to(device_k).requires_grad_(True) A_r32 = A_r.float() x_r32 = x_r.float() A_single_r32 = A_r32.unsqueeze(0).expand(BS, -1, -1) log_a_r32 = torch.log(A_single_r32) y_ref = block_two_pass(x_r32, log_a_r32, construct_L_logsumexp_stable, BL=16) upstream_ref = torch.ones_like(y_ref, dtype=torch.float32) y_ref.backward(upstream_ref)  dx_ref = x_r.grad.detach().float() dA_ref = A_r.grad.detach().float()  # [H,L]  # Reduce to 1D curves for visualization ref_dx_curve = dx_ref.mean(dim=(0,1,2)).cpu().numpy() if y_unf is not None:     unf_dx_curve = dx_unf.mean(dim=(0,1,2)).cpu().numpy()  plt.figure(figsize=(10,4), constrained_layout=True) plt.title(\"Backward dX comparison (mean over B,H,D)\") plt.plot(ref_dx_curve, label=\"reference (stable FP32)\") if y_unf is not None:     plt.plot(unf_dx_curve, label=\"CUDA unfused_stable\", color=PALETTE[\"unfused_stable\"])  if use_cuda_kernel and (dx_unf_def is not None):     plt.plot(dx_unf_def.mean(dim=(0,1,2)).cpu().numpy(), label=\"CUDA unfused\", color=PALETTE[\"unfused\"])  plt.legend()  # Error (mean abs residual) on log-scale for readability if y_unf is not None:     err_dx_curve = (dx_unf - dx_ref).abs().mean(dim=(0,1,2)).cpu().numpy()     max_e = float(err_dx_curve.max())     mean_e = float(err_dx_curve.mean())     plt.figure(figsize=(10,4), constrained_layout=True)     plt.title(f\"Backward dX error |CUDA - ref| (mean) [log] | max={max_e:.2e}, mean={mean_e:.2e}\")     plt.semilogy(err_dx_curve + 1e-16, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"])      if dx_unf_def is not None:         err_dx_curve_def = (dx_unf_def - dx_ref).abs().mean(dim=(0,1,2)).cpu().numpy()         plt.semilogy(err_dx_curve_def + 1e-16, label=\"unfused\", color=PALETTE[\"unfused\"])      plt.grid(True, which=\"both\", ls=\":\", alpha=0.4)     plt.legend()    In\u00a0[17]: Copied! <pre># Final bar graph: mean abs error vs stable log reference for unfused kernels\n# Configs: (B,H) in [(64,1), (8,8), (1,64)], L=512\n\nif not (torch.cuda.is_available() and HAS_SPEAR):\n    print(\"[warn] CUDA or spear extension unavailable; skipping bar graph.\")\nelse:\n    from spear.ops.btp.reference import block_two_pass_log as ref_fn\n\n    configs = [(64, 1), (8, 8), (1, 64)]\n    L = 512\n    DH_k = DH\n\n    labels = [f\"{B}x{H}\" for (B, H) in configs]\n    errs_unf = []\n    errs_unf_st = []\n\n    for (B, H) in configs:\n        # Inputs: random in (0,1] for A, normal for x; half for kernels\n        A = torch.sigmoid(torch.randn(B, H, L, device=\"cuda\", dtype=torch.float16))\n        x = torch.randn(B, H, DH_k, L, device=\"cuda\", dtype=torch.float16)\n\n        # Reference in FP32\n        y_ref = ref_fn(A.float(), x.float(), BL=16).float()\n\n        # CUDA kernels via helpers\n        y_unf = nb_unfused_forward(A, x, variant=\"unfused\").to(y_ref.dtype)\n        y_unf_st = nb_unfused_forward(A, x, variant=\"unfused_stable\").to(y_ref.dtype)\n\n        # Mean absolute error over all dims\n        errs_unf.append((y_unf - y_ref).abs().mean().item())\n        errs_unf_st.append((y_unf_st - y_ref).abs().mean().item())\n\n    import numpy as np\n    x_pos = np.arange(len(labels))\n    width = 0.35\n\n    fig, ax = plt.subplots(1, 1, figsize=(9, 4), constrained_layout=True)\n    ax.bar(x_pos - width/2, errs_unf, width, label=\"unfused\", color=PALETTE[\"unfused\"]) \n    ax.bar(x_pos + width/2, errs_unf_st, width, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"]) \n\n    # Annotate bars with scientific notation\n    for xpos, val in zip(x_pos - width/2, errs_unf):\n        ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n    for xpos, val in zip(x_pos + width/2, errs_unf_st):\n        ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels(labels)\n    ax.set_ylabel(\"mean |y - y_ref|\")\n    ax.set_title(\"Forward mean abs error vs stable log reference (L=512)\")\n    ax.legend()\n    plt.show()\n</pre> # Final bar graph: mean abs error vs stable log reference for unfused kernels # Configs: (B,H) in [(64,1), (8,8), (1,64)], L=512  if not (torch.cuda.is_available() and HAS_SPEAR):     print(\"[warn] CUDA or spear extension unavailable; skipping bar graph.\") else:     from spear.ops.btp.reference import block_two_pass_log as ref_fn      configs = [(64, 1), (8, 8), (1, 64)]     L = 512     DH_k = DH      labels = [f\"{B}x{H}\" for (B, H) in configs]     errs_unf = []     errs_unf_st = []      for (B, H) in configs:         # Inputs: random in (0,1] for A, normal for x; half for kernels         A = torch.sigmoid(torch.randn(B, H, L, device=\"cuda\", dtype=torch.float16))         x = torch.randn(B, H, DH_k, L, device=\"cuda\", dtype=torch.float16)          # Reference in FP32         y_ref = ref_fn(A.float(), x.float(), BL=16).float()          # CUDA kernels via helpers         y_unf = nb_unfused_forward(A, x, variant=\"unfused\").to(y_ref.dtype)         y_unf_st = nb_unfused_forward(A, x, variant=\"unfused_stable\").to(y_ref.dtype)          # Mean absolute error over all dims         errs_unf.append((y_unf - y_ref).abs().mean().item())         errs_unf_st.append((y_unf_st - y_ref).abs().mean().item())      import numpy as np     x_pos = np.arange(len(labels))     width = 0.35      fig, ax = plt.subplots(1, 1, figsize=(9, 4), constrained_layout=True)     ax.bar(x_pos - width/2, errs_unf, width, label=\"unfused\", color=PALETTE[\"unfused\"])      ax.bar(x_pos + width/2, errs_unf_st, width, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"])       # Annotate bars with scientific notation     for xpos, val in zip(x_pos - width/2, errs_unf):         ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)     for xpos, val in zip(x_pos + width/2, errs_unf_st):         ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)      ax.set_xticks(x_pos)     ax.set_xticklabels(labels)     ax.set_ylabel(\"mean |y - y_ref|\")     ax.set_title(\"Forward mean abs error vs stable log reference (L=512)\")     ax.legend()     plt.show()  In\u00a0[18]: Copied! <pre># Backward bar graphs: mean abs gradient error vs stable log reference\n# Configs: (B,H) in [(64,1), (8,8), (1,64)], L=512\n\nif not (torch.cuda.is_available() and HAS_SPEAR):\n    print(\"[warn] CUDA or spear extension unavailable; skipping backward bar graphs.\")\nelse:\n    from spear.ops.btp.reference import block_two_pass_log as _stable_ref\n    ref_fn = getattr(_stable_ref, \"__wrapped__\", _stable_ref)\n\n    configs = [(64, 1), (8, 8), (1, 64)]\n    L = 512\n    DH_k = DH\n\n    labels = [f\"{B}x{H}\" for (B, H) in configs]\n    # dx errors (mean over B,H,D,L)\n    dx_err_unf = []\n    dx_err_unf_st = []\n    # dA errors (mean over B,H,L)\n    dA_err_unf = []\n    dA_err_unf_st = []\n\n    for (B, H) in configs:\n        # Inputs on CUDA (half) for kernels\n        A = torch.sigmoid(torch.randn(B, H, L, device=\"cuda\", dtype=torch.float16))\n        x = torch.randn(B, H, DH_k, L, device=\"cuda\", dtype=torch.float16)\n\n        # Reference grads in FP32\n        A_r = A.float().detach().clone().requires_grad_(True)\n        x_r = x.float().detach().clone().requires_grad_(True)\n        y_ref = ref_fn(A_r, x_r, BL=16)\n        upstream = torch.ones_like(y_ref, dtype=torch.float32)\n        y_ref.backward(upstream)\n        dx_ref = x_r.grad.detach().float()\n        dA_ref = A_r.grad.detach().float()\n\n        # CUDA kernels via helpers\n        _y, dx_unf, dA_unf = nb_unfused_backward(A, x, variant=\"unfused\")\n        _y, dx_unf_st, dA_unf_st = nb_unfused_backward(A, x, variant=\"unfused_stable\")\n\n        # Mean absolute errors\n        dx_err_unf.append((dx_unf.detach().float() - dx_ref).abs().mean().item())\n        dx_err_unf_st.append((dx_unf_st.detach().float() - dx_ref).abs().mean().item())\n        dA_err_unf.append((dA_unf.detach().float() - dA_ref).abs().mean().item())\n        dA_err_unf_st.append((dA_unf_st.detach().float() - dA_ref).abs().mean().item())\n\n    import numpy as np\n    x_pos = np.arange(len(labels))\n    width = 0.35\n\n    # dX bar graph\n    fig, ax = plt.subplots(1, 1, figsize=(9, 4), constrained_layout=True)\n    ax.bar(x_pos - width/2, dx_err_unf, width, label=\"unfused\", color=PALETTE[\"unfused\"]) \n    ax.bar(x_pos + width/2, dx_err_unf_st, width, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"]) \n    for xpos, val in zip(x_pos - width/2, dx_err_unf):\n        ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n    for xpos, val in zip(x_pos + width/2, dx_err_unf_st):\n        ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels(labels)\n    ax.set_ylabel(\"mean |dx - dx_ref|\")\n    ax.set_title(\"Backward dX mean abs error vs stable log reference (L=512)\")\n    ax.legend()\n    plt.show()\n\n    # dA bar graph\n    fig, ax = plt.subplots(1, 1, figsize=(9, 4), constrained_layout=True)\n    ax.bar(x_pos - width/2, dA_err_unf, width, label=\"unfused\", color=PALETTE[\"unfused\"]) \n    ax.bar(x_pos + width/2, dA_err_unf_st, width, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"]) \n    for xpos, val in zip(x_pos - width/2, dA_err_unf):\n        ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n    for xpos, val in zip(x_pos + width/2, dA_err_unf_st):\n        ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels(labels)\n    ax.set_ylabel(\"mean |dA - dA_ref|\")\n    ax.set_title(\"Backward dA mean abs error vs stable log reference (L=512)\")\n    ax.legend()\n    plt.show()\n</pre> # Backward bar graphs: mean abs gradient error vs stable log reference # Configs: (B,H) in [(64,1), (8,8), (1,64)], L=512  if not (torch.cuda.is_available() and HAS_SPEAR):     print(\"[warn] CUDA or spear extension unavailable; skipping backward bar graphs.\") else:     from spear.ops.btp.reference import block_two_pass_log as _stable_ref     ref_fn = getattr(_stable_ref, \"__wrapped__\", _stable_ref)      configs = [(64, 1), (8, 8), (1, 64)]     L = 512     DH_k = DH      labels = [f\"{B}x{H}\" for (B, H) in configs]     # dx errors (mean over B,H,D,L)     dx_err_unf = []     dx_err_unf_st = []     # dA errors (mean over B,H,L)     dA_err_unf = []     dA_err_unf_st = []      for (B, H) in configs:         # Inputs on CUDA (half) for kernels         A = torch.sigmoid(torch.randn(B, H, L, device=\"cuda\", dtype=torch.float16))         x = torch.randn(B, H, DH_k, L, device=\"cuda\", dtype=torch.float16)          # Reference grads in FP32         A_r = A.float().detach().clone().requires_grad_(True)         x_r = x.float().detach().clone().requires_grad_(True)         y_ref = ref_fn(A_r, x_r, BL=16)         upstream = torch.ones_like(y_ref, dtype=torch.float32)         y_ref.backward(upstream)         dx_ref = x_r.grad.detach().float()         dA_ref = A_r.grad.detach().float()          # CUDA kernels via helpers         _y, dx_unf, dA_unf = nb_unfused_backward(A, x, variant=\"unfused\")         _y, dx_unf_st, dA_unf_st = nb_unfused_backward(A, x, variant=\"unfused_stable\")          # Mean absolute errors         dx_err_unf.append((dx_unf.detach().float() - dx_ref).abs().mean().item())         dx_err_unf_st.append((dx_unf_st.detach().float() - dx_ref).abs().mean().item())         dA_err_unf.append((dA_unf.detach().float() - dA_ref).abs().mean().item())         dA_err_unf_st.append((dA_unf_st.detach().float() - dA_ref).abs().mean().item())      import numpy as np     x_pos = np.arange(len(labels))     width = 0.35      # dX bar graph     fig, ax = plt.subplots(1, 1, figsize=(9, 4), constrained_layout=True)     ax.bar(x_pos - width/2, dx_err_unf, width, label=\"unfused\", color=PALETTE[\"unfused\"])      ax.bar(x_pos + width/2, dx_err_unf_st, width, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"])      for xpos, val in zip(x_pos - width/2, dx_err_unf):         ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)     for xpos, val in zip(x_pos + width/2, dx_err_unf_st):         ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)     ax.set_xticks(x_pos)     ax.set_xticklabels(labels)     ax.set_ylabel(\"mean |dx - dx_ref|\")     ax.set_title(\"Backward dX mean abs error vs stable log reference (L=512)\")     ax.legend()     plt.show()      # dA bar graph     fig, ax = plt.subplots(1, 1, figsize=(9, 4), constrained_layout=True)     ax.bar(x_pos - width/2, dA_err_unf, width, label=\"unfused\", color=PALETTE[\"unfused\"])      ax.bar(x_pos + width/2, dA_err_unf_st, width, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"])      for xpos, val in zip(x_pos - width/2, dA_err_unf):         ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)     for xpos, val in zip(x_pos + width/2, dA_err_unf_st):         ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)     ax.set_xticks(x_pos)     ax.set_xticklabels(labels)     ax.set_ylabel(\"mean |dA - dA_ref|\")     ax.set_title(\"Backward dA mean abs error vs stable log reference (L=512)\")     ax.legend()     plt.show()  In\u00a0[19]: Copied! <pre># Bar graph: forward mean abs error (Python ref log vs stable log vs two_pass_ref)\nconfigs = [(64, 1), (8, 8), (1, 64)]\nL = 512\nDH = 16\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nlabels = [f\"{B}x{H}\" for (B, H) in configs]\nerrs_ref_log = []   # |y_log - y_stable|\nerrs_ref_two = []   # |y_two - y_stable|\nerrs_ref_st = []    # zeros (baseline)\n\nwith torch.no_grad():\n    for (B, H) in configs:\n        a = torch.sigmoid(torch.randn(B, H, L, device=device, dtype=torch.float32))\n        x = torch.randn(B, H, DH, L, device=device, dtype=torch.float32)\n        loga = torch.log(a)\n        y_log = block_two_pass(x, loga, construct_L_logsumexp, BL=16).float()\n        y_st  = block_two_pass(x, loga, construct_L_logsumexp_stable, BL=16).float()\n        y_two = block_two_pass(x, loga, construct_L_ref, BL=16).float()\n        errs_ref_log.append((y_log - y_st).abs().mean().item())\n        errs_ref_two.append((y_two - y_st).abs().mean().item())\n        errs_ref_st.append(0.0)\n\nimport numpy as np\nx_pos = np.arange(len(labels))\nwidth = 0.28\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 4), constrained_layout=True)\nax.bar(x_pos - width, errs_ref_log, width, label=\"ref_log\", color=PALETTE[\"ref_log\"]) \nax.bar(x_pos,         errs_ref_two, width, label=\"ref_two\", color=PALETTE[\"ref_two\"]) \nax.bar(x_pos + width, errs_ref_st,  width, label=\"ref_log_stable\", color=PALETTE[\"ref_log_stable\"]) \n\nfor xpos, val in zip(x_pos - width, errs_ref_log):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nfor xpos, val in zip(x_pos, errs_ref_two):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nfor xpos, val in zip(x_pos + width, errs_ref_st):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n\nax.set_xticks(x_pos)\nax.set_xticklabels(labels)\nax.set_ylabel(\"mean |y - y_stable|\")\nax.set_title(\"Forward mean abs error: log vs two_pass_ref vs stable (L=512)\")\nax.legend()\nplt.show()\n</pre> # Bar graph: forward mean abs error (Python ref log vs stable log vs two_pass_ref) configs = [(64, 1), (8, 8), (1, 64)] L = 512 DH = 16  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  labels = [f\"{B}x{H}\" for (B, H) in configs] errs_ref_log = []   # |y_log - y_stable| errs_ref_two = []   # |y_two - y_stable| errs_ref_st = []    # zeros (baseline)  with torch.no_grad():     for (B, H) in configs:         a = torch.sigmoid(torch.randn(B, H, L, device=device, dtype=torch.float32))         x = torch.randn(B, H, DH, L, device=device, dtype=torch.float32)         loga = torch.log(a)         y_log = block_two_pass(x, loga, construct_L_logsumexp, BL=16).float()         y_st  = block_two_pass(x, loga, construct_L_logsumexp_stable, BL=16).float()         y_two = block_two_pass(x, loga, construct_L_ref, BL=16).float()         errs_ref_log.append((y_log - y_st).abs().mean().item())         errs_ref_two.append((y_two - y_st).abs().mean().item())         errs_ref_st.append(0.0)  import numpy as np x_pos = np.arange(len(labels)) width = 0.28  fig, ax = plt.subplots(1, 1, figsize=(10, 4), constrained_layout=True) ax.bar(x_pos - width, errs_ref_log, width, label=\"ref_log\", color=PALETTE[\"ref_log\"])  ax.bar(x_pos,         errs_ref_two, width, label=\"ref_two\", color=PALETTE[\"ref_two\"])  ax.bar(x_pos + width, errs_ref_st,  width, label=\"ref_log_stable\", color=PALETTE[\"ref_log_stable\"])   for xpos, val in zip(x_pos - width, errs_ref_log):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) for xpos, val in zip(x_pos, errs_ref_two):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) for xpos, val in zip(x_pos + width, errs_ref_st):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)  ax.set_xticks(x_pos) ax.set_xticklabels(labels) ax.set_ylabel(\"mean |y - y_stable|\") ax.set_title(\"Forward mean abs error: log vs two_pass_ref vs stable (L=512)\") ax.legend() plt.show()  In\u00a0[20]: Copied! <pre># Bar graphs: backward mean abs errors (Python ref log vs two_pass_ref vs stable log)\nconfigs = [(64, 1), (8, 8), (1, 64)]\nL = 512\nDH = 16\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nlabels = [f\"{B}x{H}\" for (B, H) in configs]\n# dx errors (mean over B,H,D,L)\ndx_err_log = []\ndx_err_two = []\ndx_err_st  = []  # zeros baseline\n# dA errors (mean over B,H,L)\ndA_err_log = []\ndA_err_two = []\ndA_err_st  = []  # zeros baseline\n\nfor (B, H) in configs:\n    # Inputs\n    a_log = torch.sigmoid(torch.randn(B, H, L, device=device, dtype=torch.float32)).detach()\n    x_log = torch.randn(B, H, DH, L, device=device, dtype=torch.float32).detach()\n\n    # Graph A: logsumexp\n    a_r = a_log.clone().requires_grad_(True)\n    x_r = x_log.clone().requires_grad_(True)\n    y_log = block_two_pass(x_r, torch.log(a_r), construct_L_logsumexp, BL=16)\n    upstream = torch.ones_like(y_log, dtype=torch.float32) * (1.0 / float(B))\n    y_log.backward(upstream)\n    dx_log_val = x_r.grad.detach().float(); dA_log_val = a_r.grad.detach().float()\n\n    # Graph B: stable logsumexp\n    a_s = a_log.clone().requires_grad_(True)\n    x_s = x_log.clone().requires_grad_(True)\n    y_st = block_two_pass(x_s, torch.log(a_s), construct_L_logsumexp_stable, BL=16)\n    upstream = torch.ones_like(y_st, dtype=torch.float32) * (1.0 / float(B))\n    y_st.backward(upstream)\n    dx_st_val = x_s.grad.detach().float(); dA_st_val = a_s.grad.detach().float()\n\n    # Graph C: two_pass_ref (explicit product)\n    a_t = a_log.clone().requires_grad_(True)\n    x_t = x_log.clone().requires_grad_(True)\n    y_two = block_two_pass(x_t, torch.log(a_t), construct_L_ref, BL=16)\n    upstream = torch.ones_like(y_two, dtype=torch.float32) * (1.0 / float(B))\n    y_two.backward(upstream)\n    dx_two_val = x_t.grad.detach().float(); dA_two_val = a_t.grad.detach().float()\n\n    # Errors vs stable baseline\n    dx_err_log.append((dx_log_val - dx_st_val).abs().mean().item())\n    dx_err_two.append((dx_two_val - dx_st_val).abs().mean().item())\n    dx_err_st.append(0.0)\n    dA_err_log.append((dA_log_val - dA_st_val).abs().mean().item())\n    dA_err_two.append((dA_two_val - dA_st_val).abs().mean().item())\n    dA_err_st.append(0.0)\n\nimport numpy as np\nx_pos = np.arange(len(labels))\nwidth = 0.28\n\n# dX bar graph\nfig, ax = plt.subplots(1, 1, figsize=(10, 4), constrained_layout=True)\nax.bar(x_pos - width, dx_err_log, width, label=\"ref_log\", color=PALETTE[\"ref_log\"]) \nax.bar(x_pos,         dx_err_two, width, label=\"ref_two\", color=PALETTE[\"ref_two\"]) \nax.bar(x_pos + width, dx_err_st,  width, label=\"ref_log_stable\", color=PALETTE[\"ref_log_stable\"]) \nfor xpos, val in zip(x_pos - width, dx_err_log):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nfor xpos, val in zip(x_pos, dx_err_two):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nfor xpos, val in zip(x_pos + width, dx_err_st):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nax.set_xticks(x_pos)\nax.set_xticklabels(labels)\nax.set_ylabel(\"mean |dx - dx_stable|\")\nax.set_title(\"Backward dX mean abs error: log vs two_pass_ref vs stable (L=512)\")\nax.legend()\nplt.show()\n\n# dA bar graph\nfig, ax = plt.subplots(1, 1, figsize=(10, 4), constrained_layout=True)\nax.bar(x_pos - width, dA_err_log, width, label=\"ref_log\", color=PALETTE[\"ref_log\"]) \nax.bar(x_pos,         dA_err_two, width, label=\"ref_two\", color=PALETTE[\"ref_two\"]) \nax.bar(x_pos + width, dA_err_st,  width, label=\"ref_log_stable\", color=PALETTE[\"ref_log_stable\"]) \nfor xpos, val in zip(x_pos - width, dA_err_log):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nfor xpos, val in zip(x_pos, dA_err_two):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nfor xpos, val in zip(x_pos + width, dA_err_st):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nax.set_xticks(x_pos)\nax.set_xticklabels(labels)\nax.set_ylabel(\"mean |dA - dA_stable|\")\nax.set_title(\"Backward dA mean abs error: log vs two_pass_ref vs stable (L=512)\")\nax.legend()\nplt.show()\n</pre> # Bar graphs: backward mean abs errors (Python ref log vs two_pass_ref vs stable log) configs = [(64, 1), (8, 8), (1, 64)] L = 512 DH = 16  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  labels = [f\"{B}x{H}\" for (B, H) in configs] # dx errors (mean over B,H,D,L) dx_err_log = [] dx_err_two = [] dx_err_st  = []  # zeros baseline # dA errors (mean over B,H,L) dA_err_log = [] dA_err_two = [] dA_err_st  = []  # zeros baseline  for (B, H) in configs:     # Inputs     a_log = torch.sigmoid(torch.randn(B, H, L, device=device, dtype=torch.float32)).detach()     x_log = torch.randn(B, H, DH, L, device=device, dtype=torch.float32).detach()      # Graph A: logsumexp     a_r = a_log.clone().requires_grad_(True)     x_r = x_log.clone().requires_grad_(True)     y_log = block_two_pass(x_r, torch.log(a_r), construct_L_logsumexp, BL=16)     upstream = torch.ones_like(y_log, dtype=torch.float32) * (1.0 / float(B))     y_log.backward(upstream)     dx_log_val = x_r.grad.detach().float(); dA_log_val = a_r.grad.detach().float()      # Graph B: stable logsumexp     a_s = a_log.clone().requires_grad_(True)     x_s = x_log.clone().requires_grad_(True)     y_st = block_two_pass(x_s, torch.log(a_s), construct_L_logsumexp_stable, BL=16)     upstream = torch.ones_like(y_st, dtype=torch.float32) * (1.0 / float(B))     y_st.backward(upstream)     dx_st_val = x_s.grad.detach().float(); dA_st_val = a_s.grad.detach().float()      # Graph C: two_pass_ref (explicit product)     a_t = a_log.clone().requires_grad_(True)     x_t = x_log.clone().requires_grad_(True)     y_two = block_two_pass(x_t, torch.log(a_t), construct_L_ref, BL=16)     upstream = torch.ones_like(y_two, dtype=torch.float32) * (1.0 / float(B))     y_two.backward(upstream)     dx_two_val = x_t.grad.detach().float(); dA_two_val = a_t.grad.detach().float()      # Errors vs stable baseline     dx_err_log.append((dx_log_val - dx_st_val).abs().mean().item())     dx_err_two.append((dx_two_val - dx_st_val).abs().mean().item())     dx_err_st.append(0.0)     dA_err_log.append((dA_log_val - dA_st_val).abs().mean().item())     dA_err_two.append((dA_two_val - dA_st_val).abs().mean().item())     dA_err_st.append(0.0)  import numpy as np x_pos = np.arange(len(labels)) width = 0.28  # dX bar graph fig, ax = plt.subplots(1, 1, figsize=(10, 4), constrained_layout=True) ax.bar(x_pos - width, dx_err_log, width, label=\"ref_log\", color=PALETTE[\"ref_log\"])  ax.bar(x_pos,         dx_err_two, width, label=\"ref_two\", color=PALETTE[\"ref_two\"])  ax.bar(x_pos + width, dx_err_st,  width, label=\"ref_log_stable\", color=PALETTE[\"ref_log_stable\"])  for xpos, val in zip(x_pos - width, dx_err_log):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) for xpos, val in zip(x_pos, dx_err_two):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) for xpos, val in zip(x_pos + width, dx_err_st):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) ax.set_xticks(x_pos) ax.set_xticklabels(labels) ax.set_ylabel(\"mean |dx - dx_stable|\") ax.set_title(\"Backward dX mean abs error: log vs two_pass_ref vs stable (L=512)\") ax.legend() plt.show()  # dA bar graph fig, ax = plt.subplots(1, 1, figsize=(10, 4), constrained_layout=True) ax.bar(x_pos - width, dA_err_log, width, label=\"ref_log\", color=PALETTE[\"ref_log\"])  ax.bar(x_pos,         dA_err_two, width, label=\"ref_two\", color=PALETTE[\"ref_two\"])  ax.bar(x_pos + width, dA_err_st,  width, label=\"ref_log_stable\", color=PALETTE[\"ref_log_stable\"])  for xpos, val in zip(x_pos - width, dA_err_log):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) for xpos, val in zip(x_pos, dA_err_two):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) for xpos, val in zip(x_pos + width, dA_err_st):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) ax.set_xticks(x_pos) ax.set_xticklabels(labels) ax.set_ylabel(\"mean |dA - dA_stable|\") ax.set_title(\"Backward dA mean abs error: log vs two_pass_ref vs stable (L=512)\") ax.legend() plt.show()"},{"location":"docs/","title":"Home","text":"<p>SPEAR is a collection of kernels for AI model architectures developed by Radical Numerics.</p>"},{"location":"docs/#installation","title":"Installation","text":"<p>You may use PyPI to install SPEAR:</p> <pre><code>pip install spear-python\n</code></pre> <p>Note that it will take few minutes to compile kernels for your specific GPU architecture.</p> <p>You may also install it locally using the following method to install the package in development mode:</p> <pre><code>git clone https://github.com/radicalnumerics/spear.git &amp;&amp; cd spear # clone the repository\nuv venv &amp;&amp; source .venv/bin/activate # virtual env with uv (recommended)\nuv pip install -e '.[dev]' # install in development mode\n</code></pre>"},{"location":"docs/#caching","title":"Caching","text":"<p>We use <code>ccache</code> by default. To use it and enable faster compilation (see explanation on the vLLM docs), run: <pre><code>CCACHE_NOHASHDIR=\"true\" uv pip install --no-build-isolation -e '.[dev]'\n</code></pre></p>"},{"location":"docs/#quick-start","title":"Quick Start","text":"<pre><code>import torch\nfrom spear.nn.phalanx import Phalanx\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.bfloat16\n\ndim = 512  # Must be divisible by 16 (head_dim is fixed at 16)\nlength = 128\nbatch_size = 1024\nlayer = Phalanx(dim=dim, length=length, dtype=dtype).to(device)\n\nx = torch.randn(batch_size, length, dim, dtype=dtype, device=device)\ny = layer(x)\nprint(f\"Input: {x.shape} -&gt; Output: {y.shape}\")\n</code></pre>"},{"location":"docs/#development","title":"Development","text":"<p>We include pre-commit hooks for linting and formatting (Python, C++, CUDA). To install:</p> <pre><code>uv run pre-commit install\n</code></pre> <p>To run (note they will be run automatically on commit, so not necessary to run manually):</p> <pre><code>uv run pre-commit run --all-files\n</code></pre> <p>To run tests</p> <pre><code>uv run pytest\n</code></pre>"},{"location":"docs/#structure","title":"Structure","text":"<pre><code>csrc/        # kernels: CUDA/C++ or other DSLs\nspear/\n\u251c\u2500 ops/      # low-level wrappers per op family\n\u2502  \u2514\u2500 &lt;op&gt;/\n\u2514\u2500 nn/       # layers built from ops (parametrized)\n   \u2514\u2500 &lt;layer&gt;/\n</code></pre>"},{"location":"docs/#target-architectures","title":"Target Architectures","text":"<p>Currently supported hardware includes compute capabilities 9.0 (Hopper) and 10.0 (Blackwell).</p> Kernel Name (NVIDIA) sm9.0 (NVIDIA) sm10.0 (NVIDIA) sm10.3 <code>swr.btp.fwd.bf16.bdl.hd16-bl16.sm90</code> \u2714\ufe0e ~ \u26d4 <code>swr.btp.bwd.bf16.bdl.hd16-bl16.sm90</code> \u2714\ufe0e ~ \u26d4 <ul> <li>\u2714\ufe0e: optimized</li> <li>~: working but not fully optimized</li> <li>\u26d4: not available</li> </ul> <p> </p>"},{"location":"docs/api/","title":"API Reference","text":"<p>The API mirrors the three layers of the project. Conceptual overviews live in <code>docs/concepts</code>; this section links directly to the Python surface area so you can drop kernels and modules into your own models.</p>"},{"location":"docs/api/#spearnn","title":"<code>spear.nn</code>","text":"<p>High-level modules built on sliding window recurrences. The <code>Phalanx</code> layer wraps the BTP kernel with SigmoidA parametrisation, and <code>SigmoidA</code> is exposed separately when you only need the projections.</p> <p>See Phalanx for the design rationale.</p>"},{"location":"docs/api/#spear.nn.phalanx.Phalanx","title":"Phalanx","text":"<pre><code>Phalanx(\n    dim: int,\n    heads: int | None = None,\n    length: int = 2048,\n    method: str = \"default\",\n    dtype: dtype = float32,\n    k: int = 2,\n    wpb: int = 32,\n    output_dtype: dtype | None = None,\n    block_size: int = 16,\n    kv_heads: int | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Main forward pass - delegates to method-specific implementation.</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    heads: int | None = None,\n    length: int = 2048,\n    method: str = \"default\",\n    dtype: torch.dtype = torch.float32,\n    k: int = 2,\n    wpb: int = 32,\n    output_dtype: torch.dtype | None = None,\n    block_size: int = 16,\n    kv_heads: int | None = None,\n):\n    super().__init__()\n\n    # Phalanx requires head_dim = 16, so heads = dim // 16\n    if heads is None:\n        if dim % 16 != 0:\n            raise ValueError(f\"dim ({dim}) must be divisible by 16\")\n        heads = dim // 16\n    else:\n        if dim % heads != 0:\n            raise ValueError(f\"dim ({dim}) must be divisible by heads ({heads})\")\n        head_dim_check = dim // heads\n        if head_dim_check != 16:\n            raise ValueError(f\"head_dim must be 16, got {head_dim_check}. Use heads=dim//16 or leave heads=None\")\n\n    if method not in (\"default\", \"pytorch\", \"pytorch_linspace\"):\n        raise ValueError(f\"method must be one of 'default', 'pytorch', 'pytorch_linspace', got {method}\")\n\n    self.dim = dim\n    self.heads = heads\n    self.head_dim = 16  # Always 16 for Phalanx\n    self.length = (length + 15) // 16 * 16\n    self.method = method\n    self.dtype = dtype\n    self.compute_dtype = torch.bfloat16\n    self.k = k\n    self.wpb = wpb\n    self.output_dtype = output_dtype if output_dtype is not None else dtype\n    self.block_size = block_size\n    self.kv_heads = kv_heads if kv_heads is not None else heads\n    mixed_precision = True\n\n    self.param = SigmoidA(dim, heads, self.head_dim, dtype=dtype, kv_heads=kv_heads)\n\n    self.proj_out = nn.Linear(dim, dim, bias=False, dtype=dtype)\n\n    if method == \"default\":\n        from spear.ops.btp import btp\n\n        self.btp_module = btp\n        self._forward_fn = self._forward_default\n\n    elif method == \"pytorch\":\n        from spear.ops.btp.reference import block_two_pass_log\n\n        self.pytorch_block_two_pass = block_two_pass_log\n        self._forward_fn = self._forward_pytorch\n\n    elif method == \"pytorch_linspace\":\n        from spear.ops.btp.reference import block_two_pass_linspace\n\n        self.pytorch_block_two_pass = block_two_pass_linspace\n        self._forward_fn = self._forward_pytorch\n\n    else:\n        raise ValueError(f\"Invalid method: {method}. Supported methods: 'default', 'pytorch', 'pytorch_linspace'. \")\n</code></pre>"},{"location":"docs/api/#spear.nn.phalanx.Phalanx.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Main forward pass - delegates to method-specific implementation.</p> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Main forward pass - delegates to method-specific implementation.\"\"\"\n    return self._forward_fn(x)\n</code></pre>"},{"location":"docs/api/#spear.nn.phalanx.SigmoidA","title":"SigmoidA","text":"<pre><code>SigmoidA(\n    dim: int,\n    heads: int,\n    head_dim: int,\n    dtype: dtype = bfloat16,\n    mixed_precision: bool = True,\n    kv_heads: int | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Sigmoid-gated attention parametrization for SSM with optional KV groups.</p> <p>Methods:</p> <ul> <li> <code>forward_fused_gates</code>             \u2013              <p>Forward for fused gates kernel - returns packed tensor [B, 3HD + H, L].</p> </li> <li> <code>forward_axcv</code>             \u2013              <p>Forward for default/pytorch kernels - returns (A, X, C, V) tensors.</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    heads: int,\n    head_dim: int,\n    dtype: torch.dtype = torch.bfloat16,\n    mixed_precision: bool = True,\n    kv_heads: int | None = None,\n):\n    super().__init__()\n    self.heads = heads\n    self.head_dim = head_dim\n    self.dim = dim\n    self.dtype = dtype\n    self.compute_dtype = torch.bfloat16\n\n    self.kv_heads = kv_heads if kv_heads is not None else heads\n    if heads % self.kv_heads != 0:\n        raise ValueError(f\"heads ({heads}) must be divisible by kv_heads ({self.kv_heads})\")\n\n    self.kv_repeat = KVRepeat(self.kv_heads, heads)\n\n    self.dim_bvc = 3 * heads * head_dim\n    self.dim_a = heads\n\n    self.proj_a = nn.Linear(dim, heads, bias=True, dtype=dtype)\n\n    kv_gate_dim = self.kv_heads * head_dim\n    self.proj_b = nn.Linear(dim, kv_gate_dim, bias=True, dtype=dtype)\n    self.proj_c = nn.Linear(dim, kv_gate_dim, bias=True, dtype=dtype)\n\n    gate_dim = heads * head_dim\n    self.proj_v = nn.Linear(dim, gate_dim, bias=False, dtype=dtype)\n</code></pre>"},{"location":"docs/api/#spear.nn.phalanx.SigmoidA.forward_fused_gates","title":"forward_fused_gates","text":"<pre><code>forward_fused_gates(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward for fused gates kernel - returns packed tensor [B, 3HD + H, L].</p> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward_fused_gates(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward for fused gates kernel - returns packed tensor [B, 3*H*D + H, L].\"\"\"\n    B, L, D = x.shape\n\n    A, B_gate_kv, C_kv, V = self._compute_base_projections(x)\n\n    if self.kv_repeat.enabled:\n        B_gate = self.kv_repeat.expand_and_reshape(B_gate_kv, B, self.head_dim, L)\n        B_gate = B_gate.reshape(B, self.heads * self.head_dim, L)\n        C = self.kv_repeat.expand_and_reshape(C_kv, B, self.head_dim, L)\n        C = C.reshape(B, self.heads * self.head_dim, L)\n    else:\n        B_gate = B_gate_kv\n        C = C_kv\n\n    out = torch.cat([B_gate, V, C, A], dim=1)  # [B, 3*H*D + H, L]\n\n    return out\n</code></pre>"},{"location":"docs/api/#spear.nn.phalanx.SigmoidA.forward_axcv","title":"forward_axcv","text":"<pre><code>forward_axcv(\n    x: Tensor,\n) -&gt; tuple[Tensor, Tensor, Tensor, Tensor]\n</code></pre> <p>Forward for default/pytorch kernels - returns (A, X, C, V) tensors.</p> <p>This method is used by both the default BTP kernel and pure PyTorch implementation since they both need the same tensor format: separate A, X, C, V tensors.</p> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward_axcv(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Forward for default/pytorch kernels - returns (A, X, C, V) tensors.\n\n    This method is used by both the default BTP kernel and pure PyTorch implementation\n    since they both need the same tensor format: separate A, X, C, V tensors.\n    \"\"\"\n    B, L, D = x.shape\n    H, DH = self.heads, self.head_dim\n\n    A_logits, B_gate_kv_logits, C_kv, V_flat = self._compute_base_projections(x)\n\n    V = V_flat.view(B, H, DH, L)  # [B, H, DH, L]\n\n    if self.kv_repeat.enabled:\n        B_gate_logits = self.kv_repeat.expand_and_reshape(B_gate_kv_logits, B, DH, L)\n        C = self.kv_repeat.expand_and_reshape(C_kv, B, DH, L)\n    else:\n        B_gate_logits = B_gate_kv_logits.view(B, H, DH, L)\n        C = C_kv.view(B, H, DH, L)\n\n    A = torch.sigmoid(A_logits)  # [B, H, L]\n    B_gate = torch.sigmoid(B_gate_logits)  # [B, H, DH, L]\n\n    X = B_gate * V  # [B, H, DH, L]\n\n    return A, X, C, V\n</code></pre>"},{"location":"docs/api/#spearops","title":"<code>spear.ops</code>","text":"<p>Low-level access to the BTP kernels and reference implementations. Use these when you need fine-grained control over tiling, dtype, or integration with custom autograd.</p> <p>Start with Block Two-Pass for an end-to-end explanation.</p>"},{"location":"docs/api/#spear.ops.btp.btp","title":"btp","text":"<pre><code>btp(\n    coeff: Tensor,\n    x: Tensor,\n    k: int = 2,\n    wpb: int | None = None,\n    output_dtype: dtype | None = None,\n) -&gt; Tensor\n</code></pre> <p>Apply BTP (Block Two-Pass) operation.</p> <p>Parameters:</p> <ul> <li> <code>coeff</code>               (<code>Tensor</code>)           \u2013            <p>Coefficient tensor of shape [B, H, L]</p> </li> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape [B, H, DH, L]</p> </li> <li> <code>k</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Order parameter (1 or 2)</p> </li> <li> <code>wpb</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Warps per block (default: 32 for k=2, 8 for k=1)</p> </li> <li> <code>output_dtype</code>               (<code>dtype | None</code>, default:                   <code>None</code> )           \u2013            <p>Output dtype (default: float32)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Output tensor of shape [B, H, DH, L]</p> </li> </ul> Source code in <code>spear/ops/btp/interface.py</code> <pre><code>def btp(\n    coeff: torch.Tensor,\n    x: torch.Tensor,\n    k: int = 2,\n    wpb: int | None = None,\n    output_dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply BTP (Block Two-Pass) operation.\n\n    Args:\n        coeff: Coefficient tensor of shape [B, H, L]\n        x: Input tensor of shape [B, H, DH, L]\n        k: Order parameter (1 or 2)\n        wpb: Warps per block (default: 32 for k=2, 8 for k=1)\n        output_dtype: Output dtype (default: float32)\n\n    Returns:\n        Output tensor of shape [B, H, DH, L]\n    \"\"\"\n    out_dtype = output_dtype or torch.float32\n    wpb_val = wpb if wpb is not None else (WPB_K2_DEFAULT if k == 2 else WPB_K1_DEFAULT)\n    coeff = coeff.contiguous()\n    x = x.contiguous()\n    return _BTPFunction.apply(coeff, x, k, out_dtype, wpb_val)\n</code></pre>"},{"location":"docs/api/nn/","title":"Neural Network Modules","text":"<p><code>spear.nn</code> provides the trainable layers that sit on top of the BTP kernel. Each entry here is documented in depth through mkdocstrings; read the Phalanx concept to understand the surrounding design decisions.</p>"},{"location":"docs/api/nn/#phalanx","title":"Phalanx","text":""},{"location":"docs/api/nn/#spear.nn.phalanx.Phalanx","title":"Phalanx","text":"<pre><code>Phalanx(\n    dim: int,\n    heads: int | None = None,\n    length: int = 2048,\n    method: str = \"default\",\n    dtype: dtype = float32,\n    k: int = 2,\n    wpb: int = 32,\n    output_dtype: dtype | None = None,\n    block_size: int = 16,\n    kv_heads: int | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Main forward pass - delegates to method-specific implementation.</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    heads: int | None = None,\n    length: int = 2048,\n    method: str = \"default\",\n    dtype: torch.dtype = torch.float32,\n    k: int = 2,\n    wpb: int = 32,\n    output_dtype: torch.dtype | None = None,\n    block_size: int = 16,\n    kv_heads: int | None = None,\n):\n    super().__init__()\n\n    # Phalanx requires head_dim = 16, so heads = dim // 16\n    if heads is None:\n        if dim % 16 != 0:\n            raise ValueError(f\"dim ({dim}) must be divisible by 16\")\n        heads = dim // 16\n    else:\n        if dim % heads != 0:\n            raise ValueError(f\"dim ({dim}) must be divisible by heads ({heads})\")\n        head_dim_check = dim // heads\n        if head_dim_check != 16:\n            raise ValueError(f\"head_dim must be 16, got {head_dim_check}. Use heads=dim//16 or leave heads=None\")\n\n    if method not in (\"default\", \"pytorch\", \"pytorch_linspace\"):\n        raise ValueError(f\"method must be one of 'default', 'pytorch', 'pytorch_linspace', got {method}\")\n\n    self.dim = dim\n    self.heads = heads\n    self.head_dim = 16  # Always 16 for Phalanx\n    self.length = (length + 15) // 16 * 16\n    self.method = method\n    self.dtype = dtype\n    self.compute_dtype = torch.bfloat16\n    self.k = k\n    self.wpb = wpb\n    self.output_dtype = output_dtype if output_dtype is not None else dtype\n    self.block_size = block_size\n    self.kv_heads = kv_heads if kv_heads is not None else heads\n    mixed_precision = True\n\n    self.param = SigmoidA(dim, heads, self.head_dim, dtype=dtype, kv_heads=kv_heads)\n\n    self.proj_out = nn.Linear(dim, dim, bias=False, dtype=dtype)\n\n    if method == \"default\":\n        from spear.ops.btp import btp\n\n        self.btp_module = btp\n        self._forward_fn = self._forward_default\n\n    elif method == \"pytorch\":\n        from spear.ops.btp.reference import block_two_pass_log\n\n        self.pytorch_block_two_pass = block_two_pass_log\n        self._forward_fn = self._forward_pytorch\n\n    elif method == \"pytorch_linspace\":\n        from spear.ops.btp.reference import block_two_pass_linspace\n\n        self.pytorch_block_two_pass = block_two_pass_linspace\n        self._forward_fn = self._forward_pytorch\n\n    else:\n        raise ValueError(f\"Invalid method: {method}. Supported methods: 'default', 'pytorch', 'pytorch_linspace'. \")\n</code></pre>"},{"location":"docs/api/nn/#spear.nn.phalanx.Phalanx.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Main forward pass - delegates to method-specific implementation.</p> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Main forward pass - delegates to method-specific implementation.\"\"\"\n    return self._forward_fn(x)\n</code></pre>"},{"location":"docs/api/nn/#sigmoida","title":"SigmoidA","text":""},{"location":"docs/api/nn/#spear.nn.phalanx.SigmoidA","title":"SigmoidA","text":"<pre><code>SigmoidA(\n    dim: int,\n    heads: int,\n    head_dim: int,\n    dtype: dtype = bfloat16,\n    mixed_precision: bool = True,\n    kv_heads: int | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Sigmoid-gated attention parametrization for SSM with optional KV groups.</p> <p>Methods:</p> <ul> <li> <code>forward_fused_gates</code>             \u2013              <p>Forward for fused gates kernel - returns packed tensor [B, 3HD + H, L].</p> </li> <li> <code>forward_axcv</code>             \u2013              <p>Forward for default/pytorch kernels - returns (A, X, C, V) tensors.</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    heads: int,\n    head_dim: int,\n    dtype: torch.dtype = torch.bfloat16,\n    mixed_precision: bool = True,\n    kv_heads: int | None = None,\n):\n    super().__init__()\n    self.heads = heads\n    self.head_dim = head_dim\n    self.dim = dim\n    self.dtype = dtype\n    self.compute_dtype = torch.bfloat16\n\n    self.kv_heads = kv_heads if kv_heads is not None else heads\n    if heads % self.kv_heads != 0:\n        raise ValueError(f\"heads ({heads}) must be divisible by kv_heads ({self.kv_heads})\")\n\n    self.kv_repeat = KVRepeat(self.kv_heads, heads)\n\n    self.dim_bvc = 3 * heads * head_dim\n    self.dim_a = heads\n\n    self.proj_a = nn.Linear(dim, heads, bias=True, dtype=dtype)\n\n    kv_gate_dim = self.kv_heads * head_dim\n    self.proj_b = nn.Linear(dim, kv_gate_dim, bias=True, dtype=dtype)\n    self.proj_c = nn.Linear(dim, kv_gate_dim, bias=True, dtype=dtype)\n\n    gate_dim = heads * head_dim\n    self.proj_v = nn.Linear(dim, gate_dim, bias=False, dtype=dtype)\n</code></pre>"},{"location":"docs/api/nn/#spear.nn.phalanx.SigmoidA.forward_fused_gates","title":"forward_fused_gates","text":"<pre><code>forward_fused_gates(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward for fused gates kernel - returns packed tensor [B, 3HD + H, L].</p> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward_fused_gates(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward for fused gates kernel - returns packed tensor [B, 3*H*D + H, L].\"\"\"\n    B, L, D = x.shape\n\n    A, B_gate_kv, C_kv, V = self._compute_base_projections(x)\n\n    if self.kv_repeat.enabled:\n        B_gate = self.kv_repeat.expand_and_reshape(B_gate_kv, B, self.head_dim, L)\n        B_gate = B_gate.reshape(B, self.heads * self.head_dim, L)\n        C = self.kv_repeat.expand_and_reshape(C_kv, B, self.head_dim, L)\n        C = C.reshape(B, self.heads * self.head_dim, L)\n    else:\n        B_gate = B_gate_kv\n        C = C_kv\n\n    out = torch.cat([B_gate, V, C, A], dim=1)  # [B, 3*H*D + H, L]\n\n    return out\n</code></pre>"},{"location":"docs/api/nn/#spear.nn.phalanx.SigmoidA.forward_axcv","title":"forward_axcv","text":"<pre><code>forward_axcv(\n    x: Tensor,\n) -&gt; tuple[Tensor, Tensor, Tensor, Tensor]\n</code></pre> <p>Forward for default/pytorch kernels - returns (A, X, C, V) tensors.</p> <p>This method is used by both the default BTP kernel and pure PyTorch implementation since they both need the same tensor format: separate A, X, C, V tensors.</p> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward_axcv(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Forward for default/pytorch kernels - returns (A, X, C, V) tensors.\n\n    This method is used by both the default BTP kernel and pure PyTorch implementation\n    since they both need the same tensor format: separate A, X, C, V tensors.\n    \"\"\"\n    B, L, D = x.shape\n    H, DH = self.heads, self.head_dim\n\n    A_logits, B_gate_kv_logits, C_kv, V_flat = self._compute_base_projections(x)\n\n    V = V_flat.view(B, H, DH, L)  # [B, H, DH, L]\n\n    if self.kv_repeat.enabled:\n        B_gate_logits = self.kv_repeat.expand_and_reshape(B_gate_kv_logits, B, DH, L)\n        C = self.kv_repeat.expand_and_reshape(C_kv, B, DH, L)\n    else:\n        B_gate_logits = B_gate_kv_logits.view(B, H, DH, L)\n        C = C_kv.view(B, H, DH, L)\n\n    A = torch.sigmoid(A_logits)  # [B, H, L]\n    B_gate = torch.sigmoid(B_gate_logits)  # [B, H, DH, L]\n\n    X = B_gate * V  # [B, H, DH, L]\n\n    return A, X, C, V\n</code></pre>"},{"location":"docs/api/nn/#kvrepeat","title":"KVRepeat","text":""},{"location":"docs/api/nn/#spear.nn.phalanx.swr.KVRepeat","title":"KVRepeat","text":"<pre><code>KVRepeat(kv_heads: int, total_heads: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Modular KV group repeating module for efficient parameter sharing.</p> <p>This module handles the repeating of KV (B gate and C) tensors across groups, allowing for parameter-efficient attention mechanisms where multiple query heads share the same key-value parameters.</p> <pre><code>total_heads: Total number of heads (must be divisible by kv_heads)\n</code></pre> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Repeat tensor across KV groups if needed.</p> </li> <li> <code>reshape_for_kv</code>             \u2013              <p>Reshape flat tensor to KV head dimensions.</p> </li> <li> <code>expand_and_reshape</code>             \u2013              <p>Reshape and expand tensor from KV heads to all heads.</p> </li> <li> <code>expand_flat</code>             \u2013              <p>Expand flat KV tensor directly without intermediate 4D reshape.</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def __init__(self, kv_heads: int, total_heads: int):\n    \"\"\"\n    Args:\n        kv_heads: Number of key-value heads\n        total_heads: Total number of heads (must be divisible by kv_heads)\n    \"\"\"\n    super().__init__()\n\n    if total_heads % kv_heads != 0:\n        raise ValueError(f\"total_heads ({total_heads}) must be divisible by kv_heads ({kv_heads})\")\n\n    self.kv_heads = kv_heads\n    self.total_heads = total_heads\n    self.kv_groups = total_heads // kv_heads\n    self.enabled = kv_heads &lt; total_heads\n\n    # Pre-compute indices for KV expansion: [0,0,0,1,1,1,...]\n    # Register as buffer so it moves with model to correct device\n    if self.enabled:\n        indices = torch.arange(total_heads) // self.kv_groups\n        self.register_buffer('kv_indices', indices, persistent=False)\n</code></pre>"},{"location":"docs/api/nn/#spear.nn.phalanx.swr.KVRepeat.forward","title":"forward","text":"<pre><code>forward(tensor: Tensor, pattern: str = 'b h d l') -&gt; Tensor\n</code></pre> <p>Repeat tensor across KV groups if needed.</p> <p>Parameters:</p> <ul> <li> <code>tensor</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor with shape matching pattern</p> </li> <li> <code>pattern</code>               (<code>str</code>, default:                   <code>'b h d l'</code> )           \u2013            <p>Einops pattern for input tensor (default: 'b h d l')</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Tensor repeated across groups if kv_groups &gt; 1, otherwise unchanged</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def forward(self, tensor: torch.Tensor, pattern: str = \"b h d l\") -&gt; torch.Tensor:\n    \"\"\"\n    Repeat tensor across KV groups if needed.\n\n    Args:\n        tensor: Input tensor with shape matching pattern\n        pattern: Einops pattern for input tensor (default: 'b h d l')\n\n    Returns:\n        Tensor repeated across groups if kv_groups &gt; 1, otherwise unchanged\n    \"\"\"\n    if not self.enabled:\n        return tensor\n\n    # Use cached indices for efficient expansion\n    return tensor.index_select(1, self.kv_indices)\n</code></pre>"},{"location":"docs/api/nn/#spear.nn.phalanx.swr.KVRepeat.reshape_for_kv","title":"reshape_for_kv","text":"<pre><code>reshape_for_kv(\n    tensor: Tensor, batch: int, head_dim: int, length: int\n) -&gt; Tensor\n</code></pre> <p>Reshape flat tensor to KV head dimensions.</p> <p>Parameters:</p> <ul> <li> <code>tensor</code>               (<code>Tensor</code>)           \u2013            <p>Flat tensor of shape [B, kv_heads * head_dim, L]</p> </li> <li> <code>batch</code>               (<code>int</code>)           \u2013            <p>Batch size</p> </li> <li> <code>head_dim</code>               (<code>int</code>)           \u2013            <p>Dimension per head</p> </li> <li> <code>length</code>               (<code>int</code>)           \u2013            <p>Sequence length</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Reshaped tensor of shape [B, kv_heads, head_dim, L]</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def reshape_for_kv(self, tensor: torch.Tensor, batch: int, head_dim: int, length: int) -&gt; torch.Tensor:\n    \"\"\"\n    Reshape flat tensor to KV head dimensions.\n\n    Args:\n        tensor: Flat tensor of shape [B, kv_heads * head_dim, L]\n        batch: Batch size\n        head_dim: Dimension per head\n        length: Sequence length\n\n    Returns:\n        Reshaped tensor of shape [B, kv_heads, head_dim, L]\n    \"\"\"\n    return tensor.view(batch, self.kv_heads, head_dim, length)\n</code></pre>"},{"location":"docs/api/nn/#spear.nn.phalanx.swr.KVRepeat.expand_and_reshape","title":"expand_and_reshape","text":"<pre><code>expand_and_reshape(\n    tensor: Tensor, batch: int, head_dim: int, length: int\n) -&gt; Tensor\n</code></pre> <p>Reshape and expand tensor from KV heads to all heads.</p> <p>Parameters:</p> <ul> <li> <code>tensor</code>               (<code>Tensor</code>)           \u2013            <p>Flat tensor of shape [B, kv_heads * head_dim, L]</p> </li> <li> <code>batch</code>               (<code>int</code>)           \u2013            <p>Batch size</p> </li> <li> <code>head_dim</code>               (<code>int</code>)           \u2013            <p>Dimension per head</p> </li> <li> <code>length</code>               (<code>int</code>)           \u2013            <p>Sequence length</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Expanded tensor of shape [B, total_heads, head_dim, L]</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def expand_and_reshape(self, tensor: torch.Tensor, batch: int, head_dim: int, length: int) -&gt; torch.Tensor:\n    \"\"\"\n    Reshape and expand tensor from KV heads to all heads.\n\n    Args:\n        tensor: Flat tensor of shape [B, kv_heads * head_dim, L]\n        batch: Batch size\n        head_dim: Dimension per head\n        length: Sequence length\n\n    Returns:\n        Expanded tensor of shape [B, total_heads, head_dim, L]\n    \"\"\"\n    if not self.enabled:\n        return tensor.view(batch, self.kv_heads, head_dim, length)\n\n    # Optimized expansion using pre-computed indices\n    # Reshape flat to [B, kv_heads, head_dim, L]\n    tensor = tensor.view(batch, self.kv_heads, head_dim, length)\n    # Use cached indices: [0,0,0,1,1,1,...] for kv_groups repeats\n    return tensor.index_select(1, self.kv_indices)\n</code></pre>"},{"location":"docs/api/nn/#spear.nn.phalanx.swr.KVRepeat.expand_flat","title":"expand_flat","text":"<pre><code>expand_flat(tensor: Tensor, head_dim: int) -&gt; Tensor\n</code></pre> <p>Expand flat KV tensor directly without intermediate 4D reshape. More efficient for cases where we immediately flatten back.</p> <p>Parameters:</p> <ul> <li> <code>tensor</code>               (<code>Tensor</code>)           \u2013            <p>Flat tensor of shape [B, kv_heads * head_dim, L]</p> </li> <li> <code>head_dim</code>               (<code>int</code>)           \u2013            <p>Dimension per head</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Expanded flat tensor of shape [B, total_heads * head_dim, L]</p> </li> </ul> Source code in <code>spear/nn/phalanx/swr.py</code> <pre><code>def expand_flat(self, tensor: torch.Tensor, head_dim: int) -&gt; torch.Tensor:\n    \"\"\"\n    Expand flat KV tensor directly without intermediate 4D reshape.\n    More efficient for cases where we immediately flatten back.\n\n    Args:\n        tensor: Flat tensor of shape [B, kv_heads * head_dim, L]\n        head_dim: Dimension per head\n\n    Returns:\n        Expanded flat tensor of shape [B, total_heads * head_dim, L]\n    \"\"\"\n    if not self.enabled:\n        return tensor\n\n    B, _, L = tensor.shape\n    # Reshape to separate heads: [B, kv_heads, head_dim, L]\n    tensor = tensor.view(B, self.kv_heads, head_dim, L)\n    # Expand using cached indices: [B, total_heads, head_dim, L]\n    tensor = tensor.index_select(1, self.kv_indices)\n    # Flatten back: [B, total_heads * head_dim, L]\n    return tensor.view(B, self.total_heads * head_dim, L)\n</code></pre>"},{"location":"docs/api/ops/","title":"Operations","text":"<p><code>spear.ops</code> provides the block two-pass primitive and its testing harness. Consult the BTP concept page for a walkthrough of the algorithm before diving into the low-level API.</p>"},{"location":"docs/api/ops/#btp-block-two-pass","title":"BTP (Block Two-Pass)","text":""},{"location":"docs/api/ops/#spear.ops.btp.btp","title":"btp","text":"<pre><code>btp(\n    coeff: Tensor,\n    x: Tensor,\n    k: int = 2,\n    wpb: int | None = None,\n    output_dtype: dtype | None = None,\n) -&gt; Tensor\n</code></pre> <p>Apply BTP (Block Two-Pass) operation.</p> <p>Parameters:</p> <ul> <li> <code>coeff</code>               (<code>Tensor</code>)           \u2013            <p>Coefficient tensor of shape [B, H, L]</p> </li> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape [B, H, DH, L]</p> </li> <li> <code>k</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Order parameter (1 or 2)</p> </li> <li> <code>wpb</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Warps per block (default: 32 for k=2, 8 for k=1)</p> </li> <li> <code>output_dtype</code>               (<code>dtype | None</code>, default:                   <code>None</code> )           \u2013            <p>Output dtype (default: float32)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Output tensor of shape [B, H, DH, L]</p> </li> </ul> Source code in <code>spear/ops/btp/interface.py</code> <pre><code>def btp(\n    coeff: torch.Tensor,\n    x: torch.Tensor,\n    k: int = 2,\n    wpb: int | None = None,\n    output_dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply BTP (Block Two-Pass) operation.\n\n    Args:\n        coeff: Coefficient tensor of shape [B, H, L]\n        x: Input tensor of shape [B, H, DH, L]\n        k: Order parameter (1 or 2)\n        wpb: Warps per block (default: 32 for k=2, 8 for k=1)\n        output_dtype: Output dtype (default: float32)\n\n    Returns:\n        Output tensor of shape [B, H, DH, L]\n    \"\"\"\n    out_dtype = output_dtype or torch.float32\n    wpb_val = wpb if wpb is not None else (WPB_K2_DEFAULT if k == 2 else WPB_K1_DEFAULT)\n    coeff = coeff.contiguous()\n    x = x.contiguous()\n    return _BTPFunction.apply(coeff, x, k, out_dtype, wpb_val)\n</code></pre>"},{"location":"docs/concepts/btp/","title":"Block Two-Pass (BTP)","text":"<p>Block Two-Pass (BTP) is the algorithmic core that realises sliding window recurrences on GPUs. It factors the recurrence into two sweeps that match the hardware: a local pass that consumes each tile entirely on chip, and a rank-1 global pass that stitches tiles together without serialising thread blocks.</p>"},{"location":"docs/concepts/btp/#first-pass-local-contractions","title":"First Pass: Local Contractions","text":"<p>Inputs are reshaped into blocks of length <code>BL</code>, aligned with warp scheduling. Within a block we form a lower-triangular transfer matrix <code>L</code> from the coefficients and apply it to the activations with high-throughput GEMMs. Several numerically equivalent constructions exist; the production kernel uses a stable log-space variant, while the reference paths expose masked cumulative products and explicit double-precision formulations for testing.</p>"},{"location":"docs/concepts/btp/#second-pass-carry-propagation","title":"Second Pass: Carry Propagation","text":"<p>Once each block is solved locally, only the final column of <code>L</code> is needed to communicate with the next block. BTP multiplies that column by the cumulative gate at the boundary and performs a device-wide parallel rank-1 update. Because the update is diagonal at the top level, every block can run in parallel except for a single synchronisation, eliminating the carry chains that slow traditional scan implementations.</p>"},{"location":"docs/concepts/btp/#autograd-and-compilation","title":"Autograd and Compilation","text":"<p><code>spear.ops.btp.interface</code> registers the compiled CUDA kernels as <code>torch.library</code> operators so that <code>torch.compile</code> can inline them. A custom autograd function wraps forward and backward, allocating checkpoints per block in float32 for numerical stability. The PyTorch-only references live alongside the CUDA binding and make it straightforward to validate gradients or experiment with alternative <code>L</code> constructions.</p>"},{"location":"docs/concepts/btp/#practical-advice","title":"Practical Advice","text":"<p>Choose the warps-per-block (<code>wpb</code>) parameter based on the recurrence order <code>k</code>: Hopper defaults to 32 warps for second-order recurrences and 8 for first-order. The kernel expects head width <code>DH=16</code> and sequence lengths that are multiples of the block size. When those assumptions are violated\u2014in particular the length multiple\u2014the Python wrappers pad internally so that downstream layers can keep simpler shapes.</p>"},{"location":"docs/concepts/phalanx/","title":"Phalanx Layer","text":"<p>Phalanx turns the BTP kernel into a drop-in PyTorch module. It couples the SigmoidA parametrisation with the sliding window recurrence to deliver a horizon-bounded mixer that works in hybrid transformer stacks.</p>"},{"location":"docs/concepts/phalanx/#parametrisation","title":"Parametrisation","text":"<p>Inputs pass through four projections: <code>proj_a</code> produces the per-head gates, <code>proj_b</code> and <code>proj_c</code> produce key and value gates for the recurrence, and <code>proj_v</code> yields the driving term. Gates are applied through elementwise sigmoids, and the key/value projections optionally share parameters through <code>KVRepeat</code>, which repeats key-value heads across query groups to reduce memory footprint without shrinking the attention space.</p>"},{"location":"docs/concepts/phalanx/#execution-paths","title":"Execution Paths","text":"<p>Phalanx exposes three forward paths: <code>method=\"default\"</code> uses the CUDA BTP kernel (<code>spear.ops.btp.btp</code>), <code>method=\"pytorch\"</code> runs the log-space reference for convenience or CPU execution, and <code>method=\"pytorch_linspace\"</code> uses a masked cumulative-product variant that is useful for debugging numerical issues. All paths compute the same tensors <code>A</code>, <code>X</code>, <code>C</code>, and <code>V</code> before invoking the recurrence. After the BTP call, the output is combined with <code>C</code> and <code>V</code>, permuted back to batch-first layout, and projected to the model dimension via <code>proj_out</code>.</p>"},{"location":"docs/concepts/phalanx/#streaming-inference","title":"Streaming Inference","text":"<p><code>spear.nn.phalanx.inference</code> implements jagged-band sequential and per-token decoding under the same assumptions as training. A <code>JagState</code> stores the local accumulator, cumulative gate, and previous block carry so that decode can step one token at a time while staying consistent with the block layout used in prefill.</p>"},{"location":"docs/concepts/phalanx/#constraints-and-tips","title":"Constraints and Tips","text":"<p>The layer fixes the head dimension to 16, so either supply <code>heads=dim//16</code> or leave <code>heads=None</code> and let Phalanx infer it. Sequence lengths are padded up to the next multiple of the kernel block size (default 16). Mixed-precision training works by keeping projections in the requested dtype while running the recurrence in <code>bfloat16</code> and accumulating in <code>float32</code> where needed. For hybrid models, pair Phalanx with attention layers that handle long-range routing; the SWR window deliberately truncates global dependencies so that the layer can run faster at high sequence lengths.</p>"},{"location":"docs/concepts/swr/","title":"Sliding Window Recurrences","text":"<p>Sliding window recurrences (SWRs) bound the communication horizon of linear recurrences so that they respect the memory hierarchy of modern accelerators. Instead of maintaining global state, an SWR tiles the sequence into warp-sized chunks, computes the dense recurrence locally, and exchanges only the rank-1 carry that the next chunk needs. The window is jagged: its shape mirrors how thread blocks advance across the sequence, giving every warp full bandwidth inside its tile while keeping inter-warp updates minimal.</p>"},{"location":"docs/concepts/swr/#why-locality-first","title":"Why Locality First?","text":"<p>Classical recurrent operators mix information across the entire prefix of a sequence. Their transfer operator has exponentially decaying off-diagonal bands, which means long paths contribute almost nothing in practice. On GPUs, computing those paths anyway wastes bandwidth because the data has to visit the most expensive memory levels. SWRs embrace that decay. By truncating the operator to a jagged band, they conserve the useful parts of the recurrence while avoiding work that will numerically vanish, even in high precision.</p>"},{"location":"docs/concepts/swr/#tile-level-semantics","title":"Tile-Level Semantics","text":"<p>Within each tile a recurrence is computed exactly, using dense tensor cores and a zero-initialised carry. When a tile finishes, it snapshots the terminal state, multiplies it by the cumulative gate for the border positions, and hands off a single rank-1 update to the next tile. This pattern yields depth-one synchronisation across thread blocks and keeps the critical path short enough for <code>torch.compile</code> and other graph compilers.</p>"},{"location":"docs/concepts/swr/#relationship-to-other-mixers","title":"Relationship to Other Mixers","text":"<p>SWRs sit between pure local operators (sliding-window attention, short convolutions) and global recurrences. They retain dense modelling inside each tile like a convolution, but unlike windowed attention they expose an explicit recurrence that can be truncated or extended depending on hardware limits. In practice SWRs pair well with sparse attention: attention handles long-range routing, SWRs provide efficient token mixing within the receptive field.</p>"},{"location":"docs/concepts/swr/#inference-modes","title":"Inference Modes","text":"<p>The same jagged structure works for streaming inference. During prefill, blocks run in the jagged layout described above. For decoding, a <code>JagState</code> tracks the local accumulator, the inclusive gate product, and the previous block's terminal state. Both paths share the same numerical assumptions, so training and inference stay aligned.</p> <p>Refer to the BTP and Phalanx pages for the concrete algorithm and its PyTorch integration.</p>"},{"location":"examples/test_stability_log_space/","title":"Stability","text":"In\u00a0[11]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\nimport os\nimport sys\n\nfrom einops import rearrange, repeat\nfrom pathlib import Path\nimport tempfile\nimport subprocess\n\n# Reproducibility\ntorch.manual_seed(0)\n\n# Ensure local package is importable\nREPO_ROOT = Path.cwd().resolve().parents[1]\nif str(REPO_ROOT) not in sys.path:\n    sys.path.insert(0, str(REPO_ROOT))\n\nHAS_SPEAR = True\ntry:\n    from spear.ops import btp as btp_ops\n    from spear.ops.btp.interface import DH\nexcept Exception as e:\n    print(f\"[warn] spear import failed: {e}\")\n    btp_ops = None\n    DH = 16  # fallback value\n    HAS_SPEAR = False\n\nPALETTE = {\n    \"ref_log\": \"#ff7f0e\",          # orange\n    \"ref_two\": \"#1f77b4\",          # blue (two-pass reference)\n    \"ref_log_stable\": \"#9467bd\",   # purple\n    \"unfused\": \"#d62728\",          # red\n    \"unfused_stable\": \"#8c564b\",   # brown\n}\n\ndef _run_unfused_subproc(A_path: str, x_path: str, out_dir: str, variant: str, do_backward: bool):\n    code = f\"\"\"\nimport os, sys, torch\nsys.path.insert(0, r\"{str(REPO_ROOT)}\")\nfrom spear.ops import btp as btp_ops\nA = torch.load(r\"{A_path}\").contiguous()\nx = torch.load(r\"{x_path}\").contiguous()\nA.requires_grad_({str(do_backward)})\nx.requires_grad_({str(do_backward)})\ny = btp_ops(A, x, k=2, output_dtype=torch.float32)\ntorch.save(y.detach(), r\"{out_dir}/y.pt\")\nif {str(do_backward)}:\n    y.float().sum().backward()\n    torch.save(x.grad.detach().float(), r\"{out_dir}/dx.pt\")\n    torch.save(A.grad.detach().float(), r\"{out_dir}/dA.pt\")\n\"\"\"\n    script_path = Path(out_dir) / \"_runner.py\"\n    script_path.write_text(code)\n    env = os.environ.copy()\n    env[\"SPEAR_USE_UNFUSED_STABLE\"] = \"1\" if variant == \"unfused_stable\" else \"0\"\n    env[\"PYTHONPATH\"] = f\"{str(REPO_ROOT)}:{env.get('PYTHONPATH','')}\"\n    python = sys.executable\n    subprocess.run([python, str(script_path)], check=True, env=env)\n\n\ndef nb_unfused_forward(A: torch.Tensor, x: torch.Tensor, variant: str = \"unfused\", dtype: torch.dtype = torch.float16) -&gt; torch.Tensor:\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA required for unfused kernels.\")\n    with tempfile.TemporaryDirectory() as td:\n        td = Path(td)\n        A_path, x_path = str(td / \"A.pt\"), str(td / \"x.pt\")\n        torch.save(A.to(\"cuda\", dtype=dtype), A_path)\n        torch.save(x.to(\"cuda\", dtype=dtype), x_path)\n        _run_unfused_subproc(A_path, x_path, str(td), variant=variant, do_backward=False)\n        y = torch.load(td / \"y.pt\", map_location=\"cuda\")\n    return y\n\n\ndef nb_unfused_backward(A: torch.Tensor, x: torch.Tensor, variant: str = \"unfused\", dtype: torch.dtype = torch.float16):\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA required for unfused kernels.\")\n    with tempfile.TemporaryDirectory() as td:\n        td = Path(td)\n        A_path, x_path = str(td / \"A.pt\"), str(td / \"x.pt\")\n        torch.save(A.to(\"cuda\", dtype=dtype), A_path)\n        torch.save(x.to(\"cuda\", dtype=dtype), x_path)\n        _run_unfused_subproc(A_path, x_path, str(td), variant=variant, do_backward=True)\n        y = torch.load(td / \"y.pt\", map_location=\"cuda\")\n        dx = torch.load(td / \"dx.pt\", map_location=\"cuda\")\n        dA = torch.load(td / \"dA.pt\", map_location=\"cuda\")\n    return y, dx, dA\n\n##############################################################################\n########################## BLOCK TWO PASS ####################################\n##############################################################################\n\ndef block_two_pass(u, log_a, L_constructor, BL=16):\n    \"\"\"\n    Two-pass convolution algorithm optimized for torch.compile and Triton.\n    \n    Input:\n    - u: (B, H, DH, N) - input tensor\n    - log_a: (B, H, N) - tensor of log coefficients\n    - L_constructor: function that constructs the L matrices\n    - BL: block size\n    \n    Output:\n    - x: (B, H, DH, N) - output tensor\n    \"\"\"\n    BS, H, DH, N = u.shape\n    NBL = N // BL\n\n    u, log_a = [rearrange(x, \"... (c l) -&gt; ... c l\", l=BL) for x in (u, log_a)]\n\n    # Initialize output tensor\n    x = torch.zeros_like(u)\n\n    # First pass: Compute T matrices (diagonal blocks) for local convolutions\n    L = L_constructor(log_a)\n    v = torch.einsum(\"shbij,shdbj-&gt;shdbi\", L, u)\n\n    # Second pass: Compute rank-1 cross-chunk update using previous block's carry\n    x[:, :, :, 0, :] = v[:, :, :, 0, :]\n    scale_factors = torch.exp(log_a[:, :, 1:, 0])  # (B, H, NBL-1)\n    first_cols = L[:, :, 1:, :, 0]                 # (B, H, NBL-1, BL)\n\n    g = scale_factors[..., None] * first_cols      # (B, H, NBL-1, BL)\n    prev_carry = v[:, :, :, :-1, -1][..., None]    # (B, H, DH, NBL-1, 1)\n    carry_over = g[:, :, None] * prev_carry        # (B, H, DH, NBL-1, BL)\n\n    x[:, :, :, 1:, :] = carry_over + v[:, :, :, 1:, :]\n\n    x = rearrange(x, \"... c l -&gt; ... (c l)\")\n    return x\n\n\n########################## SEGSUM ############################################\n##############################################################################\n\ndef segsum(x):\n    \"\"\"Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix.\"\"\"\n    L = x.size(-1)\n    x_cumsum = torch.cumsum(x, dim=-1)\n    x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n    mask = torch.tril(torch.ones(L, L, device=x.device, dtype=bool), diagonal=0)\n    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n    return x_segsum\n\ndef construct_L_logsumexp(log_a):\n    \"\"\"Construct L matrices for logsumexp-based computation.\"\"\"\n    return torch.exp(segsum(log_a))\n\n\n\n##############################################################################\n#################### SEGSUM STABLE ###########################################\n##############################################################################\n\n\ndef segsum_stable(x):\n    \"\"\"More stable segment sum calculation.\"\"\"\n    T = x.size(-1)\n    x = repeat(x, \"... d -&gt; ... d e\", e=T)\n    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=-1)\n    x = x.masked_fill(~mask, 0)\n    x_segsum = torch.cumsum(x, dim=-2)\n    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n    return x_segsum\n\ndef construct_L_logsumexp_stable(log_a):\n    \"\"\"Construct L matrices for logsumexp-based computation.\"\"\"\n    return torch.exp(segsum_stable(log_a))\n\n##############################################################################\n######################### REF MATERIALIZATION ##############################\n##############################################################################\n\ndef construct_L_ref(log_a):\n    \"\"\"\n    Construct L matrices using explicit product in double precision. \n    L_ij = a_i * a_(i-1) * ... * a_(j+1) if i &gt; j, \n    L_ij = 1                             if i = j, \n    L_ij = 0                             if i &lt; j.\n\n    Input:\n    - log_a: (..., N) - tensor of log coefficients\n\n    Output:\n    - L: (..., N, N) - L matrices\n    \"\"\"\n    log_a_type = log_a.dtype\n    log_a = log_a.double()\n    a = torch.exp(log_a)\n    L = torch.zeros(a.shape[:-1] + (a.shape[-1], a.shape[-1]), device=a.device, dtype=a.dtype)\n    for i in range(a.shape[-1]):\n        for j in range(i):\n            if i &gt; j:\n                L[..., i, j] = torch.prod(a[..., j+1:i+1], dim=-1)\n        L[..., i, i] = 1\n    L = L.to(log_a_type)\n    return L\n\n##############################################################################\n######################### SEQUENTIAL SCAN ####################################\n##############################################################################\n\ndef sequential_scan(u, log_a):\n    \"\"\"\n    Sequential scan implementation for reference.\n\n    Input:\n    - u: (B, H, DH, N) - input tensor\n    - log_a: (B, H, N) - tensor of log coefficients\n    \n    Output:\n    - x: (B, H, DH, N) - output tensor\n    \"\"\"\n    BS, H, DH, N = u.shape\n    x = torch.zeros_like(u)\n    a = torch.exp(log_a)\n    x[:, :, :, 0] = u[:, :, :, 0]\n    for i in range(1, N):\n        x[:, :, :, i] = x[:, :, :, i-1] * a[:, :, i] + u[:, :, :, i]\n    return x\n</pre> import matplotlib.pyplot as plt import torch import os import sys  from einops import rearrange, repeat from pathlib import Path import tempfile import subprocess  # Reproducibility torch.manual_seed(0)  # Ensure local package is importable REPO_ROOT = Path.cwd().resolve().parents[1] if str(REPO_ROOT) not in sys.path:     sys.path.insert(0, str(REPO_ROOT))  HAS_SPEAR = True try:     from spear.ops import btp as btp_ops     from spear.ops.btp.interface import DH except Exception as e:     print(f\"[warn] spear import failed: {e}\")     btp_ops = None     DH = 16  # fallback value     HAS_SPEAR = False  PALETTE = {     \"ref_log\": \"#ff7f0e\",          # orange     \"ref_two\": \"#1f77b4\",          # blue (two-pass reference)     \"ref_log_stable\": \"#9467bd\",   # purple     \"unfused\": \"#d62728\",          # red     \"unfused_stable\": \"#8c564b\",   # brown }  def _run_unfused_subproc(A_path: str, x_path: str, out_dir: str, variant: str, do_backward: bool):     code = f\"\"\" import os, sys, torch sys.path.insert(0, r\"{str(REPO_ROOT)}\") from spear.ops import btp as btp_ops A = torch.load(r\"{A_path}\").contiguous() x = torch.load(r\"{x_path}\").contiguous() A.requires_grad_({str(do_backward)}) x.requires_grad_({str(do_backward)}) y = btp_ops(A, x, k=2, output_dtype=torch.float32) torch.save(y.detach(), r\"{out_dir}/y.pt\") if {str(do_backward)}:     y.float().sum().backward()     torch.save(x.grad.detach().float(), r\"{out_dir}/dx.pt\")     torch.save(A.grad.detach().float(), r\"{out_dir}/dA.pt\") \"\"\"     script_path = Path(out_dir) / \"_runner.py\"     script_path.write_text(code)     env = os.environ.copy()     env[\"SPEAR_USE_UNFUSED_STABLE\"] = \"1\" if variant == \"unfused_stable\" else \"0\"     env[\"PYTHONPATH\"] = f\"{str(REPO_ROOT)}:{env.get('PYTHONPATH','')}\"     python = sys.executable     subprocess.run([python, str(script_path)], check=True, env=env)   def nb_unfused_forward(A: torch.Tensor, x: torch.Tensor, variant: str = \"unfused\", dtype: torch.dtype = torch.float16) -&gt; torch.Tensor:     if not torch.cuda.is_available():         raise RuntimeError(\"CUDA required for unfused kernels.\")     with tempfile.TemporaryDirectory() as td:         td = Path(td)         A_path, x_path = str(td / \"A.pt\"), str(td / \"x.pt\")         torch.save(A.to(\"cuda\", dtype=dtype), A_path)         torch.save(x.to(\"cuda\", dtype=dtype), x_path)         _run_unfused_subproc(A_path, x_path, str(td), variant=variant, do_backward=False)         y = torch.load(td / \"y.pt\", map_location=\"cuda\")     return y   def nb_unfused_backward(A: torch.Tensor, x: torch.Tensor, variant: str = \"unfused\", dtype: torch.dtype = torch.float16):     if not torch.cuda.is_available():         raise RuntimeError(\"CUDA required for unfused kernels.\")     with tempfile.TemporaryDirectory() as td:         td = Path(td)         A_path, x_path = str(td / \"A.pt\"), str(td / \"x.pt\")         torch.save(A.to(\"cuda\", dtype=dtype), A_path)         torch.save(x.to(\"cuda\", dtype=dtype), x_path)         _run_unfused_subproc(A_path, x_path, str(td), variant=variant, do_backward=True)         y = torch.load(td / \"y.pt\", map_location=\"cuda\")         dx = torch.load(td / \"dx.pt\", map_location=\"cuda\")         dA = torch.load(td / \"dA.pt\", map_location=\"cuda\")     return y, dx, dA  ############################################################################## ########################## BLOCK TWO PASS #################################### ##############################################################################  def block_two_pass(u, log_a, L_constructor, BL=16):     \"\"\"     Two-pass convolution algorithm optimized for torch.compile and Triton.          Input:     - u: (B, H, DH, N) - input tensor     - log_a: (B, H, N) - tensor of log coefficients     - L_constructor: function that constructs the L matrices     - BL: block size          Output:     - x: (B, H, DH, N) - output tensor     \"\"\"     BS, H, DH, N = u.shape     NBL = N // BL      u, log_a = [rearrange(x, \"... (c l) -&gt; ... c l\", l=BL) for x in (u, log_a)]      # Initialize output tensor     x = torch.zeros_like(u)      # First pass: Compute T matrices (diagonal blocks) for local convolutions     L = L_constructor(log_a)     v = torch.einsum(\"shbij,shdbj-&gt;shdbi\", L, u)      # Second pass: Compute rank-1 cross-chunk update using previous block's carry     x[:, :, :, 0, :] = v[:, :, :, 0, :]     scale_factors = torch.exp(log_a[:, :, 1:, 0])  # (B, H, NBL-1)     first_cols = L[:, :, 1:, :, 0]                 # (B, H, NBL-1, BL)      g = scale_factors[..., None] * first_cols      # (B, H, NBL-1, BL)     prev_carry = v[:, :, :, :-1, -1][..., None]    # (B, H, DH, NBL-1, 1)     carry_over = g[:, :, None] * prev_carry        # (B, H, DH, NBL-1, BL)      x[:, :, :, 1:, :] = carry_over + v[:, :, :, 1:, :]      x = rearrange(x, \"... c l -&gt; ... (c l)\")     return x   ########################## SEGSUM ############################################ ##############################################################################  def segsum(x):     \"\"\"Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix.\"\"\"     L = x.size(-1)     x_cumsum = torch.cumsum(x, dim=-1)     x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]     mask = torch.tril(torch.ones(L, L, device=x.device, dtype=bool), diagonal=0)     x_segsum = x_segsum.masked_fill(~mask, -torch.inf)     return x_segsum  def construct_L_logsumexp(log_a):     \"\"\"Construct L matrices for logsumexp-based computation.\"\"\"     return torch.exp(segsum(log_a))    ############################################################################## #################### SEGSUM STABLE ########################################### ##############################################################################   def segsum_stable(x):     \"\"\"More stable segment sum calculation.\"\"\"     T = x.size(-1)     x = repeat(x, \"... d -&gt; ... d e\", e=T)     mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=-1)     x = x.masked_fill(~mask, 0)     x_segsum = torch.cumsum(x, dim=-2)     mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)     x_segsum = x_segsum.masked_fill(~mask, -torch.inf)     return x_segsum  def construct_L_logsumexp_stable(log_a):     \"\"\"Construct L matrices for logsumexp-based computation.\"\"\"     return torch.exp(segsum_stable(log_a))  ############################################################################## ######################### REF MATERIALIZATION ############################## ##############################################################################  def construct_L_ref(log_a):     \"\"\"     Construct L matrices using explicit product in double precision.      L_ij = a_i * a_(i-1) * ... * a_(j+1) if i &gt; j,      L_ij = 1                             if i = j,      L_ij = 0                             if i &lt; j.      Input:     - log_a: (..., N) - tensor of log coefficients      Output:     - L: (..., N, N) - L matrices     \"\"\"     log_a_type = log_a.dtype     log_a = log_a.double()     a = torch.exp(log_a)     L = torch.zeros(a.shape[:-1] + (a.shape[-1], a.shape[-1]), device=a.device, dtype=a.dtype)     for i in range(a.shape[-1]):         for j in range(i):             if i &gt; j:                 L[..., i, j] = torch.prod(a[..., j+1:i+1], dim=-1)         L[..., i, i] = 1     L = L.to(log_a_type)     return L  ############################################################################## ######################### SEQUENTIAL SCAN #################################### ##############################################################################  def sequential_scan(u, log_a):     \"\"\"     Sequential scan implementation for reference.      Input:     - u: (B, H, DH, N) - input tensor     - log_a: (B, H, N) - tensor of log coefficients          Output:     - x: (B, H, DH, N) - output tensor     \"\"\"     BS, H, DH, N = u.shape     x = torch.zeros_like(u)     a = torch.exp(log_a)     x[:, :, :, 0] = u[:, :, :, 0]     for i in range(1, N):         x[:, :, :, i] = x[:, :, :, i-1] * a[:, :, i] + u[:, :, :, i]     return x In\u00a0[12]: Copied! <pre>BS = 16\nH = 16\nDH = 16  # BTP function requires DH=16\nN = 1024\na_min = .01\na_max = .9\n</pre> BS = 16 H = 16 DH = 16  # BTP function requires DH=16 N = 1024 a_min = .01 a_max = .9 In\u00a0[13]: Copied! <pre># Compare segsum methods standalone\nBL = 16\nlog_a = torch.log(torch.rand(BS, H, N) * (a_max - a_min) + a_min)\nlog_a = rearrange(log_a, \"... (c l) -&gt; ... c l\", l=BL)\n\nL_segsum = construct_L_logsumexp(log_a)\nL_segsum_stable = construct_L_logsumexp_stable(log_a)\nL_segsum_ref = construct_L_ref(log_a)\n\nerr_segsum_vs_ref = (L_segsum - L_segsum_ref).abs().max()\nerr_segsum_stable_vs_ref = (L_segsum_stable - L_segsum_ref).abs().max()\n\nprint(f\"Error between segsum and naive: {err_segsum_vs_ref}\")\nprint(f\"Error between segsum stable and naive: {err_segsum_stable_vs_ref}\")\n</pre> # Compare segsum methods standalone BL = 16 log_a = torch.log(torch.rand(BS, H, N) * (a_max - a_min) + a_min) log_a = rearrange(log_a, \"... (c l) -&gt; ... c l\", l=BL)  L_segsum = construct_L_logsumexp(log_a) L_segsum_stable = construct_L_logsumexp_stable(log_a) L_segsum_ref = construct_L_ref(log_a)  err_segsum_vs_ref = (L_segsum - L_segsum_ref).abs().max() err_segsum_stable_vs_ref = (L_segsum_stable - L_segsum_ref).abs().max()  print(f\"Error between segsum and naive: {err_segsum_vs_ref}\") print(f\"Error between segsum stable and naive: {err_segsum_stable_vs_ref}\") <pre>Error between segsum and naive: 1.6689300537109375e-06\nError between segsum stable and naive: 5.960464477539063e-08\n</pre> In\u00a0[14]: Copied! <pre>a = torch.rand(BS, H, N) * (a_max - a_min) + a_min\nlog_a = torch.log(a)\nu = torch.ones(BS, H, DH, N)\n\nwith torch.no_grad():\n    x_btp_bl_16 = block_two_pass(u, log_a, construct_L_logsumexp, BL=16)\n    x_btp_bl_32 = block_two_pass(u, log_a, construct_L_logsumexp, BL=32)\n    x_btp_bl_64 = block_two_pass(u, log_a, construct_L_logsumexp, BL=64)\n    x_btp_bl_128 = block_two_pass(u, log_a, construct_L_logsumexp, BL=128)\n    x_btp_bl_16_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=16)\n    x_btp_bl_32_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=32)\n    x_btp_bl_64_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=64)\n    x_btp_bl_128_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=128)\n    x_ref_bl_16 = block_two_pass(u, log_a, construct_L_ref, BL=16)\n    x_ref_bl_32 = block_two_pass(u, log_a, construct_L_ref, BL=32)\n    x_ref_bl_64 = block_two_pass(u, log_a, construct_L_ref, BL=64)\n    x_ref_bl_128 = block_two_pass(u, log_a, construct_L_ref, BL=128)\n\n# Compute CUDA unfused kernels (if CUDA is available and spear loaded)\ny_unf_stable = None\ny_unf_default_cpu = None\nif torch.cuda.is_available() and HAS_SPEAR:\n    device = \"cuda\"\n    # Kernel expects DH=DH and half/bfloat16 input\n    x_k = torch.ones(BS, H, DH, N, device=device, dtype=torch.float16)\n    A_k = a.to(device=device, dtype=torch.float16)\n    y_unf_stable = btp_ops(A_k, x_k, k=2, output_dtype=torch.float32)\n    y_unf_stable_cpu = y_unf_stable.float().mean(dim=(0,1,2)).detach().cpu().numpy()\n    try:\n        y_unf_default = nb_unfused_forward(A_k, x_k, variant=\"unfused\")\n        y_unf_default_cpu = y_unf_default.float().mean(dim=(0,1,2)).detach().cpu().numpy()\n    except Exception as e:\n        print(f\"[warn] CUDA unfused default failed: {e}\")\n\nfig, axs = plt.subplots(3, 1, figsize=(10, 10))\naxs[0].set_title(\"Block Two-Pass Stability Comparison (float32)\")\naxs[0].plot(x_btp_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (current)\")\naxs[0].plot(x_btp_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (current)\")\naxs[0].plot(x_btp_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (current)\")\naxs[0].plot(x_btp_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (current)\")\naxs[0].plot(x_btp_bl_16_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable)\")\naxs[0].plot(x_btp_bl_32_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable)\")\naxs[0].plot(x_btp_bl_64_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable)\")\naxs[0].plot(x_btp_bl_128_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable)\")\naxs[0].plot(x_ref_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (ref)\")\naxs[0].plot(x_ref_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (ref)\")\naxs[0].plot(x_ref_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (ref)\")\naxs[0].plot(x_ref_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (ref)\")\nif y_unf_stable is not None:\n    axs[0].plot(y_unf_stable_cpu, label=\"CUDA unfused_stable\", color=PALETTE[\"unfused_stable\"])\nif y_unf_default_cpu is not None:\n    axs[0].plot(y_unf_default_cpu, label=\"CUDA unfused\", color=PALETTE[\"unfused\"])\naxs[0].legend()\naxs[1].plot((x_btp_bl_16 - x_ref_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 - Ref\")\naxs[1].plot((x_btp_bl_32 - x_ref_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 - Ref\")\naxs[1].plot((x_btp_bl_64 - x_ref_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 - Ref\")\naxs[1].plot((x_btp_bl_128 - x_ref_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 - Ref\")\naxs[1].legend()\naxs[2].plot((x_btp_bl_16_stable - x_ref_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable) - Ref\")\naxs[2].plot((x_btp_bl_32_stable - x_ref_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable) - Ref\")\naxs[2].plot((x_btp_bl_64_stable - x_ref_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable) - Ref\")\naxs[2].plot((x_btp_bl_128_stable - x_ref_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable) - Ref\")\nif y_unf_stable is not None:\n    ref16_mean = x_ref_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()\n    axs[2].plot(y_unf_stable_cpu - ref16_mean, label=\"CUDA unfused_stable - Ref\", color=PALETTE[\"unfused_stable\"])\nif y_unf_default_cpu is not None:\n    ref16_mean = x_ref_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()\n    axs[2].plot(y_unf_default_cpu - ref16_mean, label=\"CUDA unfused - Ref\", color=PALETTE[\"unfused\"])\naxs[2].legend()\nplt.show()\nfig.tight_layout()\n</pre> a = torch.rand(BS, H, N) * (a_max - a_min) + a_min log_a = torch.log(a) u = torch.ones(BS, H, DH, N)  with torch.no_grad():     x_btp_bl_16 = block_two_pass(u, log_a, construct_L_logsumexp, BL=16)     x_btp_bl_32 = block_two_pass(u, log_a, construct_L_logsumexp, BL=32)     x_btp_bl_64 = block_two_pass(u, log_a, construct_L_logsumexp, BL=64)     x_btp_bl_128 = block_two_pass(u, log_a, construct_L_logsumexp, BL=128)     x_btp_bl_16_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=16)     x_btp_bl_32_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=32)     x_btp_bl_64_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=64)     x_btp_bl_128_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=128)     x_ref_bl_16 = block_two_pass(u, log_a, construct_L_ref, BL=16)     x_ref_bl_32 = block_two_pass(u, log_a, construct_L_ref, BL=32)     x_ref_bl_64 = block_two_pass(u, log_a, construct_L_ref, BL=64)     x_ref_bl_128 = block_two_pass(u, log_a, construct_L_ref, BL=128)  # Compute CUDA unfused kernels (if CUDA is available and spear loaded) y_unf_stable = None y_unf_default_cpu = None if torch.cuda.is_available() and HAS_SPEAR:     device = \"cuda\"     # Kernel expects DH=DH and half/bfloat16 input     x_k = torch.ones(BS, H, DH, N, device=device, dtype=torch.float16)     A_k = a.to(device=device, dtype=torch.float16)     y_unf_stable = btp_ops(A_k, x_k, k=2, output_dtype=torch.float32)     y_unf_stable_cpu = y_unf_stable.float().mean(dim=(0,1,2)).detach().cpu().numpy()     try:         y_unf_default = nb_unfused_forward(A_k, x_k, variant=\"unfused\")         y_unf_default_cpu = y_unf_default.float().mean(dim=(0,1,2)).detach().cpu().numpy()     except Exception as e:         print(f\"[warn] CUDA unfused default failed: {e}\")  fig, axs = plt.subplots(3, 1, figsize=(10, 10)) axs[0].set_title(\"Block Two-Pass Stability Comparison (float32)\") axs[0].plot(x_btp_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (current)\") axs[0].plot(x_btp_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (current)\") axs[0].plot(x_btp_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (current)\") axs[0].plot(x_btp_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (current)\") axs[0].plot(x_btp_bl_16_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable)\") axs[0].plot(x_btp_bl_32_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable)\") axs[0].plot(x_btp_bl_64_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable)\") axs[0].plot(x_btp_bl_128_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable)\") axs[0].plot(x_ref_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (ref)\") axs[0].plot(x_ref_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (ref)\") axs[0].plot(x_ref_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (ref)\") axs[0].plot(x_ref_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (ref)\") if y_unf_stable is not None:     axs[0].plot(y_unf_stable_cpu, label=\"CUDA unfused_stable\", color=PALETTE[\"unfused_stable\"]) if y_unf_default_cpu is not None:     axs[0].plot(y_unf_default_cpu, label=\"CUDA unfused\", color=PALETTE[\"unfused\"]) axs[0].legend() axs[1].plot((x_btp_bl_16 - x_ref_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 - Ref\") axs[1].plot((x_btp_bl_32 - x_ref_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 - Ref\") axs[1].plot((x_btp_bl_64 - x_ref_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 - Ref\") axs[1].plot((x_btp_bl_128 - x_ref_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 - Ref\") axs[1].legend() axs[2].plot((x_btp_bl_16_stable - x_ref_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable) - Ref\") axs[2].plot((x_btp_bl_32_stable - x_ref_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable) - Ref\") axs[2].plot((x_btp_bl_64_stable - x_ref_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable) - Ref\") axs[2].plot((x_btp_bl_128_stable - x_ref_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable) - Ref\") if y_unf_stable is not None:     ref16_mean = x_ref_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()     axs[2].plot(y_unf_stable_cpu - ref16_mean, label=\"CUDA unfused_stable - Ref\", color=PALETTE[\"unfused_stable\"]) if y_unf_default_cpu is not None:     ref16_mean = x_ref_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()     axs[2].plot(y_unf_default_cpu - ref16_mean, label=\"CUDA unfused - Ref\", color=PALETTE[\"unfused\"]) axs[2].legend() plt.show() fig.tight_layout()   In\u00a0[15]: Copied! <pre>a = torch.rand(BS, H, N, dtype=torch.float16) * (a_max - a_min) + a_min\nlog_a = torch.log(a)\nu = torch.ones(BS, H, DH, N, dtype=torch.float16)\n\nwith torch.no_grad():\n    x_btp_bl_16 = block_two_pass(u, log_a, construct_L_logsumexp, BL=16)\n    x_btp_bl_32 = block_two_pass(u, log_a, construct_L_logsumexp, BL=32)\n    x_btp_bl_64 = block_two_pass(u, log_a, construct_L_logsumexp, BL=64)\n    x_btp_bl_128 = block_two_pass(u, log_a, construct_L_logsumexp, BL=128)\n    x_btp_bl_16_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=16)\n    x_btp_bl_32_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=32)\n    x_btp_bl_64_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=64)\n    x_btp_bl_128_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=128)\n    x_seq_bl_16 = block_two_pass(u, log_a, construct_L_ref, BL=16)\n    x_seq_bl_32 = block_two_pass(u, log_a, construct_L_ref, BL=32)\n    x_seq_bl_64 = block_two_pass(u, log_a, construct_L_ref, BL=64)\n    x_seq_bl_128 = block_two_pass(u, log_a, construct_L_ref, BL=128)\n\n# Compute CUDA unfused kernels (if CUDA is available and spear loaded)\ny_unf_stable = None\ny_unf_default_cpu = None\nif torch.cuda.is_available() and HAS_SPEAR:\n    device = \"cuda\"\n    x_k = torch.ones(BS, H, DH, N, device=device, dtype=torch.float16)\n    A_k = a.to(device=device, dtype=torch.float16)\n    y_unf_stable = btp_ops(A_k, x_k, k=2, output_dtype=torch.float32)\n    y_unf_stable_cpu = y_unf_stable.float().mean(dim=(0,1,2)).detach().cpu().numpy()\n    try:\n        y_unf_default = nb_unfused_forward(A_k, x_k, variant=\"unfused\")\n        y_unf_default_cpu = y_unf_default.float().mean(dim=(0,1,2)).detach().cpu().numpy()\n    except Exception as e:\n        print(f\"[warn] CUDA unfused default failed: {e}\")\n\nfig, axs = plt.subplots(3, 1, figsize=(10, 10))\naxs[0].set_title(\"Block Two-Pass Stability Comparison (float16)\")\naxs[0].plot(x_btp_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (current)\")\naxs[0].plot(x_btp_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (current)\")\naxs[0].plot(x_btp_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (current)\")\naxs[0].plot(x_btp_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (current)\")\naxs[0].plot(x_btp_bl_16_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable)\")\naxs[0].plot(x_btp_bl_32_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable)\")\naxs[0].plot(x_btp_bl_64_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable)\")\naxs[0].plot(x_btp_bl_128_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable)\")\naxs[0].plot(x_seq_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (ref)\")\naxs[0].plot(x_seq_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (ref)\")\naxs[0].plot(x_seq_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (ref)\")\naxs[0].plot(x_seq_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (ref)\")\nif y_unf_stable is not None:\n    axs[0].plot(y_unf_stable_cpu, label=\"CUDA unfused_stable\", color=PALETTE[\"unfused_stable\"]) \nif y_unf_default_cpu is not None:\n    axs[0].plot(y_unf_default_cpu, label=\"CUDA unfused\", color=PALETTE[\"unfused\"])\naxs[0].legend()\naxs[1].plot((x_btp_bl_16 - x_seq_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 - Ref\")\naxs[1].plot((x_btp_bl_32 - x_seq_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 - Ref\")\naxs[1].plot((x_btp_bl_64 - x_seq_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 - Ref\")\naxs[1].plot((x_btp_bl_128 - x_seq_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 - Ref\")\naxs[1].legend()\naxs[2].plot((x_btp_bl_16_stable - x_seq_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable) - Ref\")\naxs[2].plot((x_btp_bl_32_stable - x_seq_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable) - Ref\")\naxs[2].plot((x_btp_bl_64_stable - x_seq_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable) - Ref\")\naxs[2].plot((x_btp_bl_128_stable - x_seq_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable) - Ref\")\nif y_unf_stable is not None:\n    ref16_mean = x_seq_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()\n    axs[2].plot(y_unf_stable_cpu - ref16_mean, label=\"CUDA unfused_stable - Ref\", color=PALETTE[\"unfused_stable\"]) \nif y_unf_default_cpu is not None:\n    ref16_mean = x_seq_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()\n    axs[2].plot(y_unf_default_cpu - ref16_mean, label=\"CUDA unfused - Ref\", color=PALETTE[\"unfused\"])\naxs[2].legend()\nplt.show()\nfig.tight_layout()\n</pre> a = torch.rand(BS, H, N, dtype=torch.float16) * (a_max - a_min) + a_min log_a = torch.log(a) u = torch.ones(BS, H, DH, N, dtype=torch.float16)  with torch.no_grad():     x_btp_bl_16 = block_two_pass(u, log_a, construct_L_logsumexp, BL=16)     x_btp_bl_32 = block_two_pass(u, log_a, construct_L_logsumexp, BL=32)     x_btp_bl_64 = block_two_pass(u, log_a, construct_L_logsumexp, BL=64)     x_btp_bl_128 = block_two_pass(u, log_a, construct_L_logsumexp, BL=128)     x_btp_bl_16_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=16)     x_btp_bl_32_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=32)     x_btp_bl_64_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=64)     x_btp_bl_128_stable = block_two_pass(u, log_a, construct_L_logsumexp_stable, BL=128)     x_seq_bl_16 = block_two_pass(u, log_a, construct_L_ref, BL=16)     x_seq_bl_32 = block_two_pass(u, log_a, construct_L_ref, BL=32)     x_seq_bl_64 = block_two_pass(u, log_a, construct_L_ref, BL=64)     x_seq_bl_128 = block_two_pass(u, log_a, construct_L_ref, BL=128)  # Compute CUDA unfused kernels (if CUDA is available and spear loaded) y_unf_stable = None y_unf_default_cpu = None if torch.cuda.is_available() and HAS_SPEAR:     device = \"cuda\"     x_k = torch.ones(BS, H, DH, N, device=device, dtype=torch.float16)     A_k = a.to(device=device, dtype=torch.float16)     y_unf_stable = btp_ops(A_k, x_k, k=2, output_dtype=torch.float32)     y_unf_stable_cpu = y_unf_stable.float().mean(dim=(0,1,2)).detach().cpu().numpy()     try:         y_unf_default = nb_unfused_forward(A_k, x_k, variant=\"unfused\")         y_unf_default_cpu = y_unf_default.float().mean(dim=(0,1,2)).detach().cpu().numpy()     except Exception as e:         print(f\"[warn] CUDA unfused default failed: {e}\")  fig, axs = plt.subplots(3, 1, figsize=(10, 10)) axs[0].set_title(\"Block Two-Pass Stability Comparison (float16)\") axs[0].plot(x_btp_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (current)\") axs[0].plot(x_btp_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (current)\") axs[0].plot(x_btp_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (current)\") axs[0].plot(x_btp_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (current)\") axs[0].plot(x_btp_bl_16_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable)\") axs[0].plot(x_btp_bl_32_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable)\") axs[0].plot(x_btp_bl_64_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable)\") axs[0].plot(x_btp_bl_128_stable.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable)\") axs[0].plot(x_seq_bl_16.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (ref)\") axs[0].plot(x_seq_bl_32.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (ref)\") axs[0].plot(x_seq_bl_64.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (ref)\") axs[0].plot(x_seq_bl_128.float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (ref)\") if y_unf_stable is not None:     axs[0].plot(y_unf_stable_cpu, label=\"CUDA unfused_stable\", color=PALETTE[\"unfused_stable\"])  if y_unf_default_cpu is not None:     axs[0].plot(y_unf_default_cpu, label=\"CUDA unfused\", color=PALETTE[\"unfused\"]) axs[0].legend() axs[1].plot((x_btp_bl_16 - x_seq_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 - Ref\") axs[1].plot((x_btp_bl_32 - x_seq_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 - Ref\") axs[1].plot((x_btp_bl_64 - x_seq_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 - Ref\") axs[1].plot((x_btp_bl_128 - x_seq_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 - Ref\") axs[1].legend() axs[2].plot((x_btp_bl_16_stable - x_seq_bl_16).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=16 (stable) - Ref\") axs[2].plot((x_btp_bl_32_stable - x_seq_bl_32).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=32 (stable) - Ref\") axs[2].plot((x_btp_bl_64_stable - x_seq_bl_64).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=64 (stable) - Ref\") axs[2].plot((x_btp_bl_128_stable - x_seq_bl_128).float().mean(dim=(0,1,2)).cpu().numpy(), label=\"Two-pass BL=128 (stable) - Ref\") if y_unf_stable is not None:     ref16_mean = x_seq_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()     axs[2].plot(y_unf_stable_cpu - ref16_mean, label=\"CUDA unfused_stable - Ref\", color=PALETTE[\"unfused_stable\"])  if y_unf_default_cpu is not None:     ref16_mean = x_seq_bl_16.float().mean(dim=(0,1,2)).cpu().numpy()     axs[2].plot(y_unf_default_cpu - ref16_mean, label=\"CUDA unfused - Ref\", color=PALETTE[\"unfused\"]) axs[2].legend() plt.show() fig.tight_layout()   In\u00a0[16]: Copied! <pre># Backward dX comparison (reference vs CUDA unfused_stable)\n# Uses float16 inputs for the CUDA kernel; reference computed in FP32.\n\n# Prepare inputs\nuse_cuda_kernel = torch.cuda.is_available() and HAS_SPEAR\ndevice_k = \"cuda\" if use_cuda_kernel else \"cpu\"\n\n# Random inputs (match earlier BS,H,N) and enforce DH for kernel\ntorch.manual_seed(0)\nDH_k = DH if use_cuda_kernel else DH\nA_base = torch.rand(H, N, device=device_k) * (a_max - a_min) + a_min\nx_base = torch.randn(BS, H, DH_k, N, device=device_k)\n\n# --- CUDA unfused_stable backward (if available) ---\ny_unf = None\nif use_cuda_kernel:\n    A_k = A_base.detach().clone().to(dtype=torch.float16).requires_grad_(True)\n    x_k = x_base.detach().clone().to(dtype=torch.float16).requires_grad_(True)\n    y_unf = btp_ops(A_k.unsqueeze(0).expand(BS, -1, -1).contiguous(), x_k, k=2, output_dtype=torch.float32)\n    upstream_unf = torch.ones_like(y_unf, dtype=torch.float32)\n    y_unf.backward(upstream_unf)\n    dx_unf = x_k.grad.detach().float()\n    dA_unf = A_k.grad.detach().float()  # [H,L]\n    # Default unfused via subprocess to avoid extension clashes\n    try:\n        _ydef, dx_unf_def, dA_unf_def = nb_unfused_backward(A_k.unsqueeze(0).expand(BS, -1, -1).contiguous(), x_k, variant=\"unfused\")\n    except Exception as e:\n        dx_unf_def = None\n        dA_unf_def = None\n        print(f\"[warn] CUDA unfused default backward failed: {e}\")\n\n# --- Reference backward in FP32 ---\nA_r = A_base.detach().clone().to(device_k).requires_grad_(True)\nx_r = x_base.detach().clone().to(device_k).requires_grad_(True)\nA_r32 = A_r.float()\nx_r32 = x_r.float()\nA_single_r32 = A_r32.unsqueeze(0).expand(BS, -1, -1)\nlog_a_r32 = torch.log(A_single_r32)\ny_ref = block_two_pass(x_r32, log_a_r32, construct_L_logsumexp_stable, BL=16)\nupstream_ref = torch.ones_like(y_ref, dtype=torch.float32)\ny_ref.backward(upstream_ref)\n\ndx_ref = x_r.grad.detach().float()\ndA_ref = A_r.grad.detach().float()  # [H,L]\n\n# Reduce to 1D curves for visualization\nref_dx_curve = dx_ref.mean(dim=(0,1,2)).cpu().numpy()\nif y_unf is not None:\n    unf_dx_curve = dx_unf.mean(dim=(0,1,2)).cpu().numpy()\n\nplt.figure(figsize=(10,4), constrained_layout=True)\nplt.title(\"Backward dX comparison (mean over B,H,D)\")\nplt.plot(ref_dx_curve, label=\"reference (stable FP32)\")\nif y_unf is not None:\n    plt.plot(unf_dx_curve, label=\"CUDA unfused_stable\", color=PALETTE[\"unfused_stable\"]) \nif use_cuda_kernel and (dx_unf_def is not None):\n    plt.plot(dx_unf_def.mean(dim=(0,1,2)).cpu().numpy(), label=\"CUDA unfused\", color=PALETTE[\"unfused\"]) \nplt.legend()\n\n# Error (mean abs residual) on log-scale for readability\nif y_unf is not None:\n    err_dx_curve = (dx_unf - dx_ref).abs().mean(dim=(0,1,2)).cpu().numpy()\n    max_e = float(err_dx_curve.max())\n    mean_e = float(err_dx_curve.mean())\n    plt.figure(figsize=(10,4), constrained_layout=True)\n    plt.title(f\"Backward dX error |CUDA - ref| (mean) [log] | max={max_e:.2e}, mean={mean_e:.2e}\")\n    plt.semilogy(err_dx_curve + 1e-16, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"]) \n    if dx_unf_def is not None:\n        err_dx_curve_def = (dx_unf_def - dx_ref).abs().mean(dim=(0,1,2)).cpu().numpy()\n        plt.semilogy(err_dx_curve_def + 1e-16, label=\"unfused\", color=PALETTE[\"unfused\"]) \n    plt.grid(True, which=\"both\", ls=\":\", alpha=0.4)\n    plt.legend()\n</pre> # Backward dX comparison (reference vs CUDA unfused_stable) # Uses float16 inputs for the CUDA kernel; reference computed in FP32.  # Prepare inputs use_cuda_kernel = torch.cuda.is_available() and HAS_SPEAR device_k = \"cuda\" if use_cuda_kernel else \"cpu\"  # Random inputs (match earlier BS,H,N) and enforce DH for kernel torch.manual_seed(0) DH_k = DH if use_cuda_kernel else DH A_base = torch.rand(H, N, device=device_k) * (a_max - a_min) + a_min x_base = torch.randn(BS, H, DH_k, N, device=device_k)  # --- CUDA unfused_stable backward (if available) --- y_unf = None if use_cuda_kernel:     A_k = A_base.detach().clone().to(dtype=torch.float16).requires_grad_(True)     x_k = x_base.detach().clone().to(dtype=torch.float16).requires_grad_(True)     y_unf = btp_ops(A_k.unsqueeze(0).expand(BS, -1, -1).contiguous(), x_k, k=2, output_dtype=torch.float32)     upstream_unf = torch.ones_like(y_unf, dtype=torch.float32)     y_unf.backward(upstream_unf)     dx_unf = x_k.grad.detach().float()     dA_unf = A_k.grad.detach().float()  # [H,L]     # Default unfused via subprocess to avoid extension clashes     try:         _ydef, dx_unf_def, dA_unf_def = nb_unfused_backward(A_k.unsqueeze(0).expand(BS, -1, -1).contiguous(), x_k, variant=\"unfused\")     except Exception as e:         dx_unf_def = None         dA_unf_def = None         print(f\"[warn] CUDA unfused default backward failed: {e}\")  # --- Reference backward in FP32 --- A_r = A_base.detach().clone().to(device_k).requires_grad_(True) x_r = x_base.detach().clone().to(device_k).requires_grad_(True) A_r32 = A_r.float() x_r32 = x_r.float() A_single_r32 = A_r32.unsqueeze(0).expand(BS, -1, -1) log_a_r32 = torch.log(A_single_r32) y_ref = block_two_pass(x_r32, log_a_r32, construct_L_logsumexp_stable, BL=16) upstream_ref = torch.ones_like(y_ref, dtype=torch.float32) y_ref.backward(upstream_ref)  dx_ref = x_r.grad.detach().float() dA_ref = A_r.grad.detach().float()  # [H,L]  # Reduce to 1D curves for visualization ref_dx_curve = dx_ref.mean(dim=(0,1,2)).cpu().numpy() if y_unf is not None:     unf_dx_curve = dx_unf.mean(dim=(0,1,2)).cpu().numpy()  plt.figure(figsize=(10,4), constrained_layout=True) plt.title(\"Backward dX comparison (mean over B,H,D)\") plt.plot(ref_dx_curve, label=\"reference (stable FP32)\") if y_unf is not None:     plt.plot(unf_dx_curve, label=\"CUDA unfused_stable\", color=PALETTE[\"unfused_stable\"])  if use_cuda_kernel and (dx_unf_def is not None):     plt.plot(dx_unf_def.mean(dim=(0,1,2)).cpu().numpy(), label=\"CUDA unfused\", color=PALETTE[\"unfused\"])  plt.legend()  # Error (mean abs residual) on log-scale for readability if y_unf is not None:     err_dx_curve = (dx_unf - dx_ref).abs().mean(dim=(0,1,2)).cpu().numpy()     max_e = float(err_dx_curve.max())     mean_e = float(err_dx_curve.mean())     plt.figure(figsize=(10,4), constrained_layout=True)     plt.title(f\"Backward dX error |CUDA - ref| (mean) [log] | max={max_e:.2e}, mean={mean_e:.2e}\")     plt.semilogy(err_dx_curve + 1e-16, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"])      if dx_unf_def is not None:         err_dx_curve_def = (dx_unf_def - dx_ref).abs().mean(dim=(0,1,2)).cpu().numpy()         plt.semilogy(err_dx_curve_def + 1e-16, label=\"unfused\", color=PALETTE[\"unfused\"])      plt.grid(True, which=\"both\", ls=\":\", alpha=0.4)     plt.legend()    In\u00a0[17]: Copied! <pre># Final bar graph: mean abs error vs stable log reference for unfused kernels\n# Configs: (B,H) in [(64,1), (8,8), (1,64)], L=512\n\nif not (torch.cuda.is_available() and HAS_SPEAR):\n    print(\"[warn] CUDA or spear extension unavailable; skipping bar graph.\")\nelse:\n    from spear.ops.btp.reference import block_two_pass_log as ref_fn\n\n    configs = [(64, 1), (8, 8), (1, 64)]\n    L = 512\n    DH_k = DH\n\n    labels = [f\"{B}x{H}\" for (B, H) in configs]\n    errs_unf = []\n    errs_unf_st = []\n\n    for (B, H) in configs:\n        # Inputs: random in (0,1] for A, normal for x; half for kernels\n        A = torch.sigmoid(torch.randn(B, H, L, device=\"cuda\", dtype=torch.float16))\n        x = torch.randn(B, H, DH_k, L, device=\"cuda\", dtype=torch.float16)\n\n        # Reference in FP32\n        y_ref = ref_fn(A.float(), x.float(), BL=16).float()\n\n        # CUDA kernels via helpers\n        y_unf = nb_unfused_forward(A, x, variant=\"unfused\").to(y_ref.dtype)\n        y_unf_st = nb_unfused_forward(A, x, variant=\"unfused_stable\").to(y_ref.dtype)\n\n        # Mean absolute error over all dims\n        errs_unf.append((y_unf - y_ref).abs().mean().item())\n        errs_unf_st.append((y_unf_st - y_ref).abs().mean().item())\n\n    import numpy as np\n    x_pos = np.arange(len(labels))\n    width = 0.35\n\n    fig, ax = plt.subplots(1, 1, figsize=(9, 4), constrained_layout=True)\n    ax.bar(x_pos - width/2, errs_unf, width, label=\"unfused\", color=PALETTE[\"unfused\"]) \n    ax.bar(x_pos + width/2, errs_unf_st, width, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"]) \n\n    # Annotate bars with scientific notation\n    for xpos, val in zip(x_pos - width/2, errs_unf):\n        ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n    for xpos, val in zip(x_pos + width/2, errs_unf_st):\n        ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels(labels)\n    ax.set_ylabel(\"mean |y - y_ref|\")\n    ax.set_title(\"Forward mean abs error vs stable log reference (L=512)\")\n    ax.legend()\n    plt.show()\n</pre> # Final bar graph: mean abs error vs stable log reference for unfused kernels # Configs: (B,H) in [(64,1), (8,8), (1,64)], L=512  if not (torch.cuda.is_available() and HAS_SPEAR):     print(\"[warn] CUDA or spear extension unavailable; skipping bar graph.\") else:     from spear.ops.btp.reference import block_two_pass_log as ref_fn      configs = [(64, 1), (8, 8), (1, 64)]     L = 512     DH_k = DH      labels = [f\"{B}x{H}\" for (B, H) in configs]     errs_unf = []     errs_unf_st = []      for (B, H) in configs:         # Inputs: random in (0,1] for A, normal for x; half for kernels         A = torch.sigmoid(torch.randn(B, H, L, device=\"cuda\", dtype=torch.float16))         x = torch.randn(B, H, DH_k, L, device=\"cuda\", dtype=torch.float16)          # Reference in FP32         y_ref = ref_fn(A.float(), x.float(), BL=16).float()          # CUDA kernels via helpers         y_unf = nb_unfused_forward(A, x, variant=\"unfused\").to(y_ref.dtype)         y_unf_st = nb_unfused_forward(A, x, variant=\"unfused_stable\").to(y_ref.dtype)          # Mean absolute error over all dims         errs_unf.append((y_unf - y_ref).abs().mean().item())         errs_unf_st.append((y_unf_st - y_ref).abs().mean().item())      import numpy as np     x_pos = np.arange(len(labels))     width = 0.35      fig, ax = plt.subplots(1, 1, figsize=(9, 4), constrained_layout=True)     ax.bar(x_pos - width/2, errs_unf, width, label=\"unfused\", color=PALETTE[\"unfused\"])      ax.bar(x_pos + width/2, errs_unf_st, width, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"])       # Annotate bars with scientific notation     for xpos, val in zip(x_pos - width/2, errs_unf):         ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)     for xpos, val in zip(x_pos + width/2, errs_unf_st):         ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)      ax.set_xticks(x_pos)     ax.set_xticklabels(labels)     ax.set_ylabel(\"mean |y - y_ref|\")     ax.set_title(\"Forward mean abs error vs stable log reference (L=512)\")     ax.legend()     plt.show()  In\u00a0[18]: Copied! <pre># Backward bar graphs: mean abs gradient error vs stable log reference\n# Configs: (B,H) in [(64,1), (8,8), (1,64)], L=512\n\nif not (torch.cuda.is_available() and HAS_SPEAR):\n    print(\"[warn] CUDA or spear extension unavailable; skipping backward bar graphs.\")\nelse:\n    from spear.ops.btp.reference import block_two_pass_log as _stable_ref\n    ref_fn = getattr(_stable_ref, \"__wrapped__\", _stable_ref)\n\n    configs = [(64, 1), (8, 8), (1, 64)]\n    L = 512\n    DH_k = DH\n\n    labels = [f\"{B}x{H}\" for (B, H) in configs]\n    # dx errors (mean over B,H,D,L)\n    dx_err_unf = []\n    dx_err_unf_st = []\n    # dA errors (mean over B,H,L)\n    dA_err_unf = []\n    dA_err_unf_st = []\n\n    for (B, H) in configs:\n        # Inputs on CUDA (half) for kernels\n        A = torch.sigmoid(torch.randn(B, H, L, device=\"cuda\", dtype=torch.float16))\n        x = torch.randn(B, H, DH_k, L, device=\"cuda\", dtype=torch.float16)\n\n        # Reference grads in FP32\n        A_r = A.float().detach().clone().requires_grad_(True)\n        x_r = x.float().detach().clone().requires_grad_(True)\n        y_ref = ref_fn(A_r, x_r, BL=16)\n        upstream = torch.ones_like(y_ref, dtype=torch.float32)\n        y_ref.backward(upstream)\n        dx_ref = x_r.grad.detach().float()\n        dA_ref = A_r.grad.detach().float()\n\n        # CUDA kernels via helpers\n        _y, dx_unf, dA_unf = nb_unfused_backward(A, x, variant=\"unfused\")\n        _y, dx_unf_st, dA_unf_st = nb_unfused_backward(A, x, variant=\"unfused_stable\")\n\n        # Mean absolute errors\n        dx_err_unf.append((dx_unf.detach().float() - dx_ref).abs().mean().item())\n        dx_err_unf_st.append((dx_unf_st.detach().float() - dx_ref).abs().mean().item())\n        dA_err_unf.append((dA_unf.detach().float() - dA_ref).abs().mean().item())\n        dA_err_unf_st.append((dA_unf_st.detach().float() - dA_ref).abs().mean().item())\n\n    import numpy as np\n    x_pos = np.arange(len(labels))\n    width = 0.35\n\n    # dX bar graph\n    fig, ax = plt.subplots(1, 1, figsize=(9, 4), constrained_layout=True)\n    ax.bar(x_pos - width/2, dx_err_unf, width, label=\"unfused\", color=PALETTE[\"unfused\"]) \n    ax.bar(x_pos + width/2, dx_err_unf_st, width, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"]) \n    for xpos, val in zip(x_pos - width/2, dx_err_unf):\n        ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n    for xpos, val in zip(x_pos + width/2, dx_err_unf_st):\n        ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels(labels)\n    ax.set_ylabel(\"mean |dx - dx_ref|\")\n    ax.set_title(\"Backward dX mean abs error vs stable log reference (L=512)\")\n    ax.legend()\n    plt.show()\n\n    # dA bar graph\n    fig, ax = plt.subplots(1, 1, figsize=(9, 4), constrained_layout=True)\n    ax.bar(x_pos - width/2, dA_err_unf, width, label=\"unfused\", color=PALETTE[\"unfused\"]) \n    ax.bar(x_pos + width/2, dA_err_unf_st, width, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"]) \n    for xpos, val in zip(x_pos - width/2, dA_err_unf):\n        ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n    for xpos, val in zip(x_pos + width/2, dA_err_unf_st):\n        ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels(labels)\n    ax.set_ylabel(\"mean |dA - dA_ref|\")\n    ax.set_title(\"Backward dA mean abs error vs stable log reference (L=512)\")\n    ax.legend()\n    plt.show()\n</pre> # Backward bar graphs: mean abs gradient error vs stable log reference # Configs: (B,H) in [(64,1), (8,8), (1,64)], L=512  if not (torch.cuda.is_available() and HAS_SPEAR):     print(\"[warn] CUDA or spear extension unavailable; skipping backward bar graphs.\") else:     from spear.ops.btp.reference import block_two_pass_log as _stable_ref     ref_fn = getattr(_stable_ref, \"__wrapped__\", _stable_ref)      configs = [(64, 1), (8, 8), (1, 64)]     L = 512     DH_k = DH      labels = [f\"{B}x{H}\" for (B, H) in configs]     # dx errors (mean over B,H,D,L)     dx_err_unf = []     dx_err_unf_st = []     # dA errors (mean over B,H,L)     dA_err_unf = []     dA_err_unf_st = []      for (B, H) in configs:         # Inputs on CUDA (half) for kernels         A = torch.sigmoid(torch.randn(B, H, L, device=\"cuda\", dtype=torch.float16))         x = torch.randn(B, H, DH_k, L, device=\"cuda\", dtype=torch.float16)          # Reference grads in FP32         A_r = A.float().detach().clone().requires_grad_(True)         x_r = x.float().detach().clone().requires_grad_(True)         y_ref = ref_fn(A_r, x_r, BL=16)         upstream = torch.ones_like(y_ref, dtype=torch.float32)         y_ref.backward(upstream)         dx_ref = x_r.grad.detach().float()         dA_ref = A_r.grad.detach().float()          # CUDA kernels via helpers         _y, dx_unf, dA_unf = nb_unfused_backward(A, x, variant=\"unfused\")         _y, dx_unf_st, dA_unf_st = nb_unfused_backward(A, x, variant=\"unfused_stable\")          # Mean absolute errors         dx_err_unf.append((dx_unf.detach().float() - dx_ref).abs().mean().item())         dx_err_unf_st.append((dx_unf_st.detach().float() - dx_ref).abs().mean().item())         dA_err_unf.append((dA_unf.detach().float() - dA_ref).abs().mean().item())         dA_err_unf_st.append((dA_unf_st.detach().float() - dA_ref).abs().mean().item())      import numpy as np     x_pos = np.arange(len(labels))     width = 0.35      # dX bar graph     fig, ax = plt.subplots(1, 1, figsize=(9, 4), constrained_layout=True)     ax.bar(x_pos - width/2, dx_err_unf, width, label=\"unfused\", color=PALETTE[\"unfused\"])      ax.bar(x_pos + width/2, dx_err_unf_st, width, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"])      for xpos, val in zip(x_pos - width/2, dx_err_unf):         ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)     for xpos, val in zip(x_pos + width/2, dx_err_unf_st):         ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)     ax.set_xticks(x_pos)     ax.set_xticklabels(labels)     ax.set_ylabel(\"mean |dx - dx_ref|\")     ax.set_title(\"Backward dX mean abs error vs stable log reference (L=512)\")     ax.legend()     plt.show()      # dA bar graph     fig, ax = plt.subplots(1, 1, figsize=(9, 4), constrained_layout=True)     ax.bar(x_pos - width/2, dA_err_unf, width, label=\"unfused\", color=PALETTE[\"unfused\"])      ax.bar(x_pos + width/2, dA_err_unf_st, width, label=\"unfused_stable\", color=PALETTE[\"unfused_stable\"])      for xpos, val in zip(x_pos - width/2, dA_err_unf):         ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)     for xpos, val in zip(x_pos + width/2, dA_err_unf_st):         ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)     ax.set_xticks(x_pos)     ax.set_xticklabels(labels)     ax.set_ylabel(\"mean |dA - dA_ref|\")     ax.set_title(\"Backward dA mean abs error vs stable log reference (L=512)\")     ax.legend()     plt.show()  In\u00a0[19]: Copied! <pre># Bar graph: forward mean abs error (Python ref log vs stable log vs two_pass_ref)\nconfigs = [(64, 1), (8, 8), (1, 64)]\nL = 512\nDH = 16\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nlabels = [f\"{B}x{H}\" for (B, H) in configs]\nerrs_ref_log = []   # |y_log - y_stable|\nerrs_ref_two = []   # |y_two - y_stable|\nerrs_ref_st = []    # zeros (baseline)\n\nwith torch.no_grad():\n    for (B, H) in configs:\n        a = torch.sigmoid(torch.randn(B, H, L, device=device, dtype=torch.float32))\n        x = torch.randn(B, H, DH, L, device=device, dtype=torch.float32)\n        loga = torch.log(a)\n        y_log = block_two_pass(x, loga, construct_L_logsumexp, BL=16).float()\n        y_st  = block_two_pass(x, loga, construct_L_logsumexp_stable, BL=16).float()\n        y_two = block_two_pass(x, loga, construct_L_ref, BL=16).float()\n        errs_ref_log.append((y_log - y_st).abs().mean().item())\n        errs_ref_two.append((y_two - y_st).abs().mean().item())\n        errs_ref_st.append(0.0)\n\nimport numpy as np\nx_pos = np.arange(len(labels))\nwidth = 0.28\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 4), constrained_layout=True)\nax.bar(x_pos - width, errs_ref_log, width, label=\"ref_log\", color=PALETTE[\"ref_log\"]) \nax.bar(x_pos,         errs_ref_two, width, label=\"ref_two\", color=PALETTE[\"ref_two\"]) \nax.bar(x_pos + width, errs_ref_st,  width, label=\"ref_log_stable\", color=PALETTE[\"ref_log_stable\"]) \n\nfor xpos, val in zip(x_pos - width, errs_ref_log):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nfor xpos, val in zip(x_pos, errs_ref_two):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nfor xpos, val in zip(x_pos + width, errs_ref_st):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n\nax.set_xticks(x_pos)\nax.set_xticklabels(labels)\nax.set_ylabel(\"mean |y - y_stable|\")\nax.set_title(\"Forward mean abs error: log vs two_pass_ref vs stable (L=512)\")\nax.legend()\nplt.show()\n</pre> # Bar graph: forward mean abs error (Python ref log vs stable log vs two_pass_ref) configs = [(64, 1), (8, 8), (1, 64)] L = 512 DH = 16  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  labels = [f\"{B}x{H}\" for (B, H) in configs] errs_ref_log = []   # |y_log - y_stable| errs_ref_two = []   # |y_two - y_stable| errs_ref_st = []    # zeros (baseline)  with torch.no_grad():     for (B, H) in configs:         a = torch.sigmoid(torch.randn(B, H, L, device=device, dtype=torch.float32))         x = torch.randn(B, H, DH, L, device=device, dtype=torch.float32)         loga = torch.log(a)         y_log = block_two_pass(x, loga, construct_L_logsumexp, BL=16).float()         y_st  = block_two_pass(x, loga, construct_L_logsumexp_stable, BL=16).float()         y_two = block_two_pass(x, loga, construct_L_ref, BL=16).float()         errs_ref_log.append((y_log - y_st).abs().mean().item())         errs_ref_two.append((y_two - y_st).abs().mean().item())         errs_ref_st.append(0.0)  import numpy as np x_pos = np.arange(len(labels)) width = 0.28  fig, ax = plt.subplots(1, 1, figsize=(10, 4), constrained_layout=True) ax.bar(x_pos - width, errs_ref_log, width, label=\"ref_log\", color=PALETTE[\"ref_log\"])  ax.bar(x_pos,         errs_ref_two, width, label=\"ref_two\", color=PALETTE[\"ref_two\"])  ax.bar(x_pos + width, errs_ref_st,  width, label=\"ref_log_stable\", color=PALETTE[\"ref_log_stable\"])   for xpos, val in zip(x_pos - width, errs_ref_log):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) for xpos, val in zip(x_pos, errs_ref_two):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) for xpos, val in zip(x_pos + width, errs_ref_st):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)  ax.set_xticks(x_pos) ax.set_xticklabels(labels) ax.set_ylabel(\"mean |y - y_stable|\") ax.set_title(\"Forward mean abs error: log vs two_pass_ref vs stable (L=512)\") ax.legend() plt.show()  In\u00a0[20]: Copied! <pre># Bar graphs: backward mean abs errors (Python ref log vs two_pass_ref vs stable log)\nconfigs = [(64, 1), (8, 8), (1, 64)]\nL = 512\nDH = 16\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nlabels = [f\"{B}x{H}\" for (B, H) in configs]\n# dx errors (mean over B,H,D,L)\ndx_err_log = []\ndx_err_two = []\ndx_err_st  = []  # zeros baseline\n# dA errors (mean over B,H,L)\ndA_err_log = []\ndA_err_two = []\ndA_err_st  = []  # zeros baseline\n\nfor (B, H) in configs:\n    # Inputs\n    a_log = torch.sigmoid(torch.randn(B, H, L, device=device, dtype=torch.float32)).detach()\n    x_log = torch.randn(B, H, DH, L, device=device, dtype=torch.float32).detach()\n\n    # Graph A: logsumexp\n    a_r = a_log.clone().requires_grad_(True)\n    x_r = x_log.clone().requires_grad_(True)\n    y_log = block_two_pass(x_r, torch.log(a_r), construct_L_logsumexp, BL=16)\n    upstream = torch.ones_like(y_log, dtype=torch.float32) * (1.0 / float(B))\n    y_log.backward(upstream)\n    dx_log_val = x_r.grad.detach().float(); dA_log_val = a_r.grad.detach().float()\n\n    # Graph B: stable logsumexp\n    a_s = a_log.clone().requires_grad_(True)\n    x_s = x_log.clone().requires_grad_(True)\n    y_st = block_two_pass(x_s, torch.log(a_s), construct_L_logsumexp_stable, BL=16)\n    upstream = torch.ones_like(y_st, dtype=torch.float32) * (1.0 / float(B))\n    y_st.backward(upstream)\n    dx_st_val = x_s.grad.detach().float(); dA_st_val = a_s.grad.detach().float()\n\n    # Graph C: two_pass_ref (explicit product)\n    a_t = a_log.clone().requires_grad_(True)\n    x_t = x_log.clone().requires_grad_(True)\n    y_two = block_two_pass(x_t, torch.log(a_t), construct_L_ref, BL=16)\n    upstream = torch.ones_like(y_two, dtype=torch.float32) * (1.0 / float(B))\n    y_two.backward(upstream)\n    dx_two_val = x_t.grad.detach().float(); dA_two_val = a_t.grad.detach().float()\n\n    # Errors vs stable baseline\n    dx_err_log.append((dx_log_val - dx_st_val).abs().mean().item())\n    dx_err_two.append((dx_two_val - dx_st_val).abs().mean().item())\n    dx_err_st.append(0.0)\n    dA_err_log.append((dA_log_val - dA_st_val).abs().mean().item())\n    dA_err_two.append((dA_two_val - dA_st_val).abs().mean().item())\n    dA_err_st.append(0.0)\n\nimport numpy as np\nx_pos = np.arange(len(labels))\nwidth = 0.28\n\n# dX bar graph\nfig, ax = plt.subplots(1, 1, figsize=(10, 4), constrained_layout=True)\nax.bar(x_pos - width, dx_err_log, width, label=\"ref_log\", color=PALETTE[\"ref_log\"]) \nax.bar(x_pos,         dx_err_two, width, label=\"ref_two\", color=PALETTE[\"ref_two\"]) \nax.bar(x_pos + width, dx_err_st,  width, label=\"ref_log_stable\", color=PALETTE[\"ref_log_stable\"]) \nfor xpos, val in zip(x_pos - width, dx_err_log):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nfor xpos, val in zip(x_pos, dx_err_two):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nfor xpos, val in zip(x_pos + width, dx_err_st):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nax.set_xticks(x_pos)\nax.set_xticklabels(labels)\nax.set_ylabel(\"mean |dx - dx_stable|\")\nax.set_title(\"Backward dX mean abs error: log vs two_pass_ref vs stable (L=512)\")\nax.legend()\nplt.show()\n\n# dA bar graph\nfig, ax = plt.subplots(1, 1, figsize=(10, 4), constrained_layout=True)\nax.bar(x_pos - width, dA_err_log, width, label=\"ref_log\", color=PALETTE[\"ref_log\"]) \nax.bar(x_pos,         dA_err_two, width, label=\"ref_two\", color=PALETTE[\"ref_two\"]) \nax.bar(x_pos + width, dA_err_st,  width, label=\"ref_log_stable\", color=PALETTE[\"ref_log_stable\"]) \nfor xpos, val in zip(x_pos - width, dA_err_log):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nfor xpos, val in zip(x_pos, dA_err_two):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nfor xpos, val in zip(x_pos + width, dA_err_st):\n    ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\nax.set_xticks(x_pos)\nax.set_xticklabels(labels)\nax.set_ylabel(\"mean |dA - dA_stable|\")\nax.set_title(\"Backward dA mean abs error: log vs two_pass_ref vs stable (L=512)\")\nax.legend()\nplt.show()\n</pre> # Bar graphs: backward mean abs errors (Python ref log vs two_pass_ref vs stable log) configs = [(64, 1), (8, 8), (1, 64)] L = 512 DH = 16  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  labels = [f\"{B}x{H}\" for (B, H) in configs] # dx errors (mean over B,H,D,L) dx_err_log = [] dx_err_two = [] dx_err_st  = []  # zeros baseline # dA errors (mean over B,H,L) dA_err_log = [] dA_err_two = [] dA_err_st  = []  # zeros baseline  for (B, H) in configs:     # Inputs     a_log = torch.sigmoid(torch.randn(B, H, L, device=device, dtype=torch.float32)).detach()     x_log = torch.randn(B, H, DH, L, device=device, dtype=torch.float32).detach()      # Graph A: logsumexp     a_r = a_log.clone().requires_grad_(True)     x_r = x_log.clone().requires_grad_(True)     y_log = block_two_pass(x_r, torch.log(a_r), construct_L_logsumexp, BL=16)     upstream = torch.ones_like(y_log, dtype=torch.float32) * (1.0 / float(B))     y_log.backward(upstream)     dx_log_val = x_r.grad.detach().float(); dA_log_val = a_r.grad.detach().float()      # Graph B: stable logsumexp     a_s = a_log.clone().requires_grad_(True)     x_s = x_log.clone().requires_grad_(True)     y_st = block_two_pass(x_s, torch.log(a_s), construct_L_logsumexp_stable, BL=16)     upstream = torch.ones_like(y_st, dtype=torch.float32) * (1.0 / float(B))     y_st.backward(upstream)     dx_st_val = x_s.grad.detach().float(); dA_st_val = a_s.grad.detach().float()      # Graph C: two_pass_ref (explicit product)     a_t = a_log.clone().requires_grad_(True)     x_t = x_log.clone().requires_grad_(True)     y_two = block_two_pass(x_t, torch.log(a_t), construct_L_ref, BL=16)     upstream = torch.ones_like(y_two, dtype=torch.float32) * (1.0 / float(B))     y_two.backward(upstream)     dx_two_val = x_t.grad.detach().float(); dA_two_val = a_t.grad.detach().float()      # Errors vs stable baseline     dx_err_log.append((dx_log_val - dx_st_val).abs().mean().item())     dx_err_two.append((dx_two_val - dx_st_val).abs().mean().item())     dx_err_st.append(0.0)     dA_err_log.append((dA_log_val - dA_st_val).abs().mean().item())     dA_err_two.append((dA_two_val - dA_st_val).abs().mean().item())     dA_err_st.append(0.0)  import numpy as np x_pos = np.arange(len(labels)) width = 0.28  # dX bar graph fig, ax = plt.subplots(1, 1, figsize=(10, 4), constrained_layout=True) ax.bar(x_pos - width, dx_err_log, width, label=\"ref_log\", color=PALETTE[\"ref_log\"])  ax.bar(x_pos,         dx_err_two, width, label=\"ref_two\", color=PALETTE[\"ref_two\"])  ax.bar(x_pos + width, dx_err_st,  width, label=\"ref_log_stable\", color=PALETTE[\"ref_log_stable\"])  for xpos, val in zip(x_pos - width, dx_err_log):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) for xpos, val in zip(x_pos, dx_err_two):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) for xpos, val in zip(x_pos + width, dx_err_st):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) ax.set_xticks(x_pos) ax.set_xticklabels(labels) ax.set_ylabel(\"mean |dx - dx_stable|\") ax.set_title(\"Backward dX mean abs error: log vs two_pass_ref vs stable (L=512)\") ax.legend() plt.show()  # dA bar graph fig, ax = plt.subplots(1, 1, figsize=(10, 4), constrained_layout=True) ax.bar(x_pos - width, dA_err_log, width, label=\"ref_log\", color=PALETTE[\"ref_log\"])  ax.bar(x_pos,         dA_err_two, width, label=\"ref_two\", color=PALETTE[\"ref_two\"])  ax.bar(x_pos + width, dA_err_st,  width, label=\"ref_log_stable\", color=PALETTE[\"ref_log_stable\"])  for xpos, val in zip(x_pos - width, dA_err_log):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) for xpos, val in zip(x_pos, dA_err_two):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) for xpos, val in zip(x_pos + width, dA_err_st):     ax.text(xpos, val, f\"{val:.2e}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90) ax.set_xticks(x_pos) ax.set_xticklabels(labels) ax.set_ylabel(\"mean |dA - dA_stable|\") ax.set_title(\"Backward dA mean abs error: log vs two_pass_ref vs stable (L=512)\") ax.legend() plt.show()"}]}